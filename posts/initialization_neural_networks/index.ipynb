{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The importantance of propper initializtaion\"\n",
    "author: \"Alex Kelly\"\n",
    "format: \n",
    "  html: \n",
    "    code-fold: true\n",
    "    toc : true \n",
    "    numbered-sections : true \n",
    "jupyter: python3\n",
    "draft : False \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why initialization model weights before starting the optimization  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![](weight_initialization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all.  Its not just about the initialization of the 1st layer of weights, its about all the weights from layer 1 to the last to the outputs.  \n",
    "\n",
    "Here are a few key points on weight initialations \n",
    "\n",
    "1. The hardware has floating point limitations that mean it processes limited number of bits and stores in a limited amount of memory.   If the weights are too high or too low then it ends up calculating the results with too high or too low to store into memory specified which are called exploding or vanishing neurons (i.e.. dead neurons) at anypoint level in the nereual network.  This results in information lost, which are called dead neurons or neurons that dont contribute to the end result in a optimal way.  \n",
    "2.  The mean should be close to zero,  if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don't contribute to the end prediction or classification.  Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\n",
    "3. The standard deviation should be near 1 so that the values don't vary too far from the mean (i.e. 0 mentioned in point 2).  A standard deviation that's too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
