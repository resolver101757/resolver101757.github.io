<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Kelly">

<title>Alex Paul Kelly - The importantance of propper initializtaion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Alex Paul Kelly</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/resolver101757" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/alex_paul_kelly" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexpkelly/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The importantance of propper initializtaion</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Kelly </p>
            </div>
    </div>
      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="why-am-i-writing-about-lsuv" class="level1">
<h1>Why am I writing about LSUV?</h1>
<p>I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand inner workings of each aspect of a neural network. This will allow me to create new techniques and improve existing techniques and enable me to piece together the right neural network for the right task.</p>
<p>On top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.</p>
</section>
<section id="why-initialization-model-weights-before-starting-the-optimization" class="level1">
<h1>Why initialization model weights before starting the optimization</h1>
<p><img src="weight_initialization.png" class="img-fluid"></p>
<p>Proper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.</p>
<p>Here are a few key points on weight initializations:</p>
<ol type="1">
<li>The hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.<br>
</li>
<li>The mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.</li>
<li>The standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e.&nbsp;0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.</li>
</ol>
</section>
<section id="lsuv-vs-other-weight-optimization-techniques" class="level1">
<h1>LSUV vs other weight optimization techniques</h1>
<p>Each model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :</p>
<ul>
<li><p>LSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.</p></li>
<li><p>Zero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.</p></li>
<li><p>Random Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.</p></li>
<li><p>Xavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.</p></li>
<li><p>He Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.</p></li>
<li><p>LeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.</p></li>
</ul>
<p>LSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.</p>
</section>
<section id="the-following-sections-guide-you-through-the-code-along-with-comments-and-reflections-on-the-results" class="level1">
<h1>The following sections guide you through the code along with comments and reflections on the results</h1>
<p>The aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.</p>
<p>We will be covering :</p>
<ul>
<li>Setting up the environment, loading the data set</li>
<li>finding the learning rate</li>
<li>learner without LSUV or any other initialization techniques and exploring the results.</li>
<li>learner wtih Standardizing inputs with no weights optimization techniques</li>
<li>learner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.</li>
<li>LSUV training method</li>
</ul>
<p>each of the learner sections where we will be running the model will have the following charts :</p>
<p><br> <strong>loss and accuracy :</strong> learner loss and accuracy for the training and validation data sets <br> <strong>Color_dim :</strong> The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections. <br> <strong>Dead_chart :</strong> Shows how many inactive neurons there are, 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better. <br> <strong>Plot_stats :</strong> Means close to zero but standard deviations far off expected goal of 1, to far from 1 to train optimally. <br> <br> - and finally the conclusion of the results</p>
</section>
<section id="setup-environment-loading-the-dataset-transforming-the-data-for-training" class="level1">
<h1>Setup environment, loading the dataset, transforming the data for training</h1>
<p>This code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.</p>
<div class="cell" data-outputid="57f9d48a-b16e-426e-ff9f-2e9a6438567c">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install required libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install torcheval</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Python Standard Library imports</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections.abc <span class="im">import</span> Mapping</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> operator <span class="im">import</span> attrgetter</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> copy</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> contextmanager</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Third-party library imports</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> TF</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, load_dataset_builder</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastprogress <span class="im">import</span> progress_bar, master_bar</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.test <span class="im">import</span> test_close</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> init</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn,tensor</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torcheval.metrics <span class="im">import</span> MulticlassAccuracy, Mean</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom module imports</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> conv <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.datasets <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.conv <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.activations <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration settings</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>torch.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>, sci_mode<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>mpl.rcParams[<span class="st">'image.cmap'</span>] <span class="op">=</span> <span class="st">'viridis'</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>logging.disable(logging.WARNING)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># get labels</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>x,y <span class="op">=</span> <span class="st">'image'</span>,<span class="st">'label'</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co">#  Street View House Numbers dataset name</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>name <span class="op">=</span> (<span class="st">'svhn'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># fetch dataset from hugging face</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>dsd <span class="op">=</span> load_dataset(name, <span class="st">"cropped_digits"</span>,)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># remove extra (not required for initial run through)</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>dsd.pop(<span class="st">"extra"</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co"># convert images to greyscale</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_to_gray(batch):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> batch[<span class="st">'image'</span>]</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> image.mode <span class="op">!=</span> <span class="st">'L'</span>:  <span class="co"># Only convert if not already grayscale</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        gray_image <span class="op">=</span> image.convert(<span class="st">'L'</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">'image'</span>] <span class="op">=</span> gray_image</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to all datasets</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> dsd.keys():</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    dsd[key] <span class="op">=</span> dsd[key].<span class="bu">map</span>(convert_to_gray, batched<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co"># transform data</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [torch.flatten(TF.to_tensor(o)) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># extract data set</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(dd<span class="op">=</span>tds, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dt))</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>xb.shape,yb[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="find-the-optimal-learning-rate" class="level1">
<h1>Find the optimal learning rate</h1>
<p>The Learning Rate Finder is a tool designed to help find a good learning rate for training deep learning models. It increases the learning rate after each mini-batch and records the loss. As the learning rate increases, initially, the loss will decrease (as the model learns). But after a certain point, the learning rate might be too high causing the loss to increase due to overshooting the optimal weights. The usual method is to choose the best learning rate is to choose a figure just before the steep fall.</p>
<div class="cell" data-outputid="959aa7bf-512b-4968-8322-8c3d64764be8">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset and loader</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>lrfind <span class="op">=</span> LRFinderCB()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [TrainCB(), DeviceCB(), lrfind]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fits data</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, epochs<span class="op">=</span><span class="dv">1</span>, xtra_cbs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(model, dls, loss_func<span class="op">=</span>F.cross_entropy, lr<span class="op">=</span><span class="fl">0.0000001</span>, cbs<span class="op">=</span>cbs<span class="op">+</span>fc.L(xtra_cbs))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    learn.fit(epochs)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># along with ni (features) input channels and output channels (feature maps)</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cnn_layers():</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">1</span> ,<span class="dv">8</span>, ks<span class="op">=</span><span class="dv">5</span>),        <span class="co">#14x14</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">8</span> ,<span class="dv">16</span>),             <span class="co">#7x7</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">16</span>,<span class="dv">32</span>),             <span class="co">#4x4</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">32</span>,<span class="dv">64</span>),             <span class="co">#2x2</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">64</span>,<span class="dv">10</span>, act<span class="op">=</span><span class="va">False</span>),  <span class="co">#1x1</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        nn.Flatten()]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>fit(model)<span class="op">;</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.plot(lrfind.lrs, lrfind.losses)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-7-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Learning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="example-of-poorly-initialized-model-no-input-or-weight-initialization" class="level1">
<h1>Example of poorly initialized model (No input or weight initialization)</h1>
<p>It will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.</p>
<p>By paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements. <br> <br> <strong>Key technical information for this section :</strong> <br> <br> <strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> A Convolution neural network showing the number of filters in each layer and ending with 10 output units. <br> <strong>Activation is nn.ReLU :</strong> ReLU function introduces non-linearity to the model. <br> <strong>Data normalisation :</strong> None <br> <strong>weight normalization :</strong> None <br></p>
<div class="cell" data-outputid="99933d23-14cf-4409-f93b-174b17e1e5ca">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># improved function to include labelling for the stats</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ActivationStats(HooksCallback):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mod_filter<span class="op">=</span>fc.noop):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(append_stats, mod_filter)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> color_dim(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">11</span>,<span class="dv">5</span>)):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      fig, axes <span class="op">=</span> get_grid(<span class="bu">len</span>(<span class="va">self</span>), figsize<span class="op">=</span>figsize)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> ax, h <span class="kw">in</span> <span class="bu">zip</span>(axes.flat, <span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>          im <span class="op">=</span> ax.imshow(get_hist(h), origin<span class="op">=</span><span class="st">'lower'</span>)  <span class="co"># Using imshow directly</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Add labels, title, and colorbar for clarity</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>          ax.set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>          ax.set_ylabel(<span class="st">"Activation Value"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>          ax.set_title(<span class="st">"Layer "</span> <span class="op">+</span> <span class="st">"str(self.index(h))"</span> <span class="op">+</span> <span class="st">" Activations"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>          cbar <span class="op">=</span> plt.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>          cbar.set_label(<span class="st">"Frequency"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>      plt.tight_layout()  <span class="co"># Prevent overlap</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dead_chart(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">11</span>,<span class="dv">5</span>)):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> get_grid(<span class="bu">len</span>(<span class="va">self</span>), figsize<span class="op">=</span>figsize)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ax, h <span class="kw">in</span> <span class="bu">zip</span>(axes.flatten(), <span class="va">self</span>):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            ax.plot(get_min(h), linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            ax.set_ylim(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            ax.set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            ax.set_ylabel(<span class="st">"Activation Value"</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            ax.set_title(<span class="st">"Layer "</span> <span class="op">+</span> <span class="st">"str(self.index(h))"</span> <span class="op">+</span> <span class="st">" Dead Activations"</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()  <span class="co"># Prevent overlap</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_stats(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>)):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>figsize)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="dv">0</span>,<span class="dv">1</span>:</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                axs[i].plot(h.stats[i])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_title(<span class="st">'Means'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_title(<span class="st">'Stdevs'</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_ylabel(<span class="st">"Mean Activation Value"</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_ylabel(<span class="st">"Standard Deviation of Activation Value"</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        plt.legend(fc.L.<span class="bu">range</span>(<span class="va">self</span>))</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#plt.tight_layout()  # Prevent overlap</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset and loader</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co"># setup model for learning</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> MetricsCB(accuracy<span class="op">=</span>MulticlassAccuracy())</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [TrainCB(), DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="co"># fits dataset</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, epochs<span class="op">=</span><span class="dv">3</span>, xtra_cbs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(model, dls, loss_func<span class="op">=</span>F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs<span class="op">+</span>fc.L(xtra_cbs))</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    learn.fit(epochs)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co"># conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># along with ni (features) input channels and output channels (feature maps)</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cnn_layers():</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">1</span> ,<span class="dv">8</span>, ks<span class="op">=</span><span class="dv">5</span>),        <span class="co">#14x14</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">8</span> ,<span class="dv">16</span>),             <span class="co">#7x7</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">16</span>,<span class="dv">32</span>),             <span class="co">#4x4</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">32</span>,<span class="dv">64</span>),             <span class="co">#2x2</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">64</span>,<span class="dv">10</span>, act<span class="op">=</span><span class="va">False</span>),  <span class="co">#1x1</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        nn.Flatten()]</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="co">#astats = ActivationStats(fc.risinstance(GeneralRelu))</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance(nn.ReLU))</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>fit(model, xtra_cbs<span class="op">=</span>[astats])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Visualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval</p>
</div>
<div class="cell-output cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">accuracy</th>
<th style="text-align: left;">loss</th>
<th style="text-align: left;">epoch</th>
<th style="text-align: left;">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.188</td>
<td style="text-align: left;">2.247</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.225</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.189</td>
<td style="text-align: left;">2.237</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.225</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.189</td>
<td style="text-align: left;">2.237</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.224</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-markdown/cell-8-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="cf9bdf33-b93c-4dfb-f0e0-df6918894ff5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-9-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Colour chart to show inactive neurons</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="e2e1577f-20c6-4eb3-ae5e-579b729983d8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-10-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="a265280c-15d5-409b-ee9a-eb05c013064b">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-11-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="improve-the-model-by-input-normalization" class="level1">
<h1>Improve the model by input normalization</h1>
<p>To give the optimize algorithm every chance converge quicker, normalization the inputs to a mean of zero and standard deviation of 1 will help. This can be done alone or with normalization the weights too. The following section discusses normalization the inputs alone.</p>
<p>The key to this is to ensure that each feature contributes equally to the learning process, which is especially important when the features have different units or different scales.</p>
<p>It will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.</p>
<p>By paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input normalization (also know as feature scaling). We can then compare them against previously un-initialized inputs and see any improvements.</p>
<p><strong>Key Technical Information for This Section:</strong></p>
<p><strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> This denotes a Convolutional Neural Network with varying numbers of filters across different layers, culminating in 10 output units. <br> <strong>Activation is nn.ReLU :</strong> The model utilizes the ReLU (Rectified Linear Unit) activation function to introduce non-linearity, aiding in better approximations of complex functions.</p>
<div class="cell" data-outputid="40c18584-3461-4b71-d65b-8ae0f6af5327">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>xl,yl <span class="op">=</span> <span class="st">'image'</span>,<span class="st">'label'</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset and loader</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>xmeans <span class="op">=</span> []</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>xstds <span class="op">=</span> []</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> <span class="bu">iter</span>(dls.train):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    xmean, xstd <span class="op">=</span> xb.mean(), xb.std()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    xmeans.append(xmean.item())</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    xstds.append(xstd.item())</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>plt.plot(xmeans, label<span class="op">=</span><span class="st">'xmean'</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'X Mean over Iterations'</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean'</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>plt.plot(xstds, label<span class="op">=</span><span class="st">'xstd'</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'X Std Dev over Iterations'</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Std Dev'</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-12-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Chart showing mean and standard deviation (y axis) over batches (x axis)</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="f78e2509-130a-48d3-e369-63e5f9b0e718">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[xl] <span class="op">=</span> [(TF.to_tensor(o)<span class="op">-</span>xmean)<span class="op">/</span>xstd <span class="cf">for</span> o <span class="kw">in</span> b[xl]]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>xmeans <span class="op">=</span> []</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>xstds <span class="op">=</span> []</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> <span class="bu">iter</span>(dls.train):</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    xmean, xstd <span class="op">=</span> xb.mean(), xb.std()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    xmeans.append(xmean.item())</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    xstds.append(xstd.item())</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.plot(xmeans, label<span class="op">=</span><span class="st">'xmean'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'X Mean over Iterations'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.plot(xstds, label<span class="op">=</span><span class="st">'xstd'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'X Std Dev over Iterations'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Std Dev'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-13-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Chart showing mean and standard deviation (y axis) over batches (x axis)</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="03a6f18b-b4fe-40a8-cacd-f3d581526f78">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># slightly better than last time but definatly not perfect&gt;</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fit(model, xtra_cbs<span class="op">=</span>[astats])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Visualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval</p>
</div>
<div class="cell-output cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">accuracy</th>
<th style="text-align: left;">loss</th>
<th style="text-align: left;">epoch</th>
<th style="text-align: left;">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.185</td>
<td style="text-align: left;">2.248</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.225</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.189</td>
<td style="text-align: left;">2.237</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.224</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.189</td>
<td style="text-align: left;">2.237</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">2.224</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-markdown/cell-14-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="c39846c4-ef9a-46de-efb4-c8b62486d648">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-15-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Colour chart to show inactive neurons</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="ae9ef3a2-64ca-486f-93ff-a417b20f6ea4">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stanard deviations still away from one but mean looks reasonable</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-16-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="f3e93645-5b19-43f5-badb-5b6e20e5c6f0">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first layer quite bad and last layer is totally dead.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-17-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="batch-normalization-with-leaky-relu-activation-and-kaiming-normalization." class="level1">
<h1>Batch Normalization with Leaky ReLU activation and Kaiming normalization.</h1>
<p><br></p>
<p>We are now changing 3 things. First is batch normalisation to control the inputs between each of layers, Leaky ReLU to keep more neurons alive so they contribute to the end result and finally weight initialization.</p>
<p>It will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.</p>
<p>By paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.</p>
<p><strong>Key Technical Information for This Section:</strong></p>
<p><strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units. <br> <strong>Activation is Leaky ReLU :</strong> We will now use Leaky ReLU (instead of ReLu) as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e.&nbsp;it passes a positve number for each of the activations. <br> <strong>Data normalisation: is BatchNorm :</strong> This will help calculate the data after each activation layer??? <br> <strong>Learning Rate 0.2 :</strong> Initial training was conducted with a learning rate of 0.2. <br> Best Training So Far: This version of the model has shown the best training results compared to previous iterations.</p>
<p>What to Try Next : The section concludes with open questions and suggestions for future experiments to further enhance model performance. The use of batch normalization and Leaky ReLU has led to improved training dynamics. The next aim is to implement LSUV as call back</p>
<div class="cell" data-outputid="627aee1b-413c-4193-c817-f95be7eb6711">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset from source dsd</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| Avoiding inactive Neurons: Leaky ReLU helps to mitigate the problem of "inactive neurons" that can occur with ReLU units,</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># where neurons get stuck during training and always output a zero value. By allowing a small, non-zero output for negative inputs,</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Leaky ReLU ensures that gradients can still flow through the neuron, which can help to keep learning progressing.</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeneralRelu(nn.Module):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, leak<span class="op">=</span><span class="va">None</span>, sub<span class="op">=</span><span class="va">None</span>, maxv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.leak,<span class="va">self</span>.sub,<span class="va">self</span>.maxv <span class="op">=</span> leak,sub,maxv</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.leaky_relu(x,<span class="va">self</span>.leak) <span class="cf">if</span> <span class="va">self</span>.leak <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> F.relu(x)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sub <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x <span class="op">-=</span> <span class="va">self</span>.sub</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.maxv <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x.clamp_max_(<span class="va">self</span>.maxv)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_func(f, start<span class="op">=-</span><span class="fl">5.</span>, end<span class="op">=</span><span class="fl">5.</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.linspace(start, end, steps)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, f(x))</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, which<span class="op">=</span><span class="st">'both'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co"># visual representation of the new relu, left values Jeremeys example</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>plot_func(GeneralRelu(leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-18-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plot of leaky, always passes through a positive value</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="19574d8a-12be-4a23-ca8d-59bf49741cef">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, act<span class="op">=</span>nn.ReLU, norm<span class="op">=</span><span class="va">None</span>, bias<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bias <span class="kw">is</span> <span class="va">None</span>: bias <span class="op">=</span> <span class="kw">not</span> <span class="bu">isinstance</span>(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [nn.Conv2d(ni, nf, stride<span class="op">=</span>stride, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>, bias<span class="op">=</span>bias)]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm: layers.append(norm(nf))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: layers.append(act())</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| initializes weights based on kaiming_normal_</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(m, leaky<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># checks for a instance of layer and module of the neural network</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># checks for a instance of 1d, 2d, 3d neural network</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>      <span class="co"># creates the initialization of the weights, for a, anything that is not zero, standard relu is assumed.</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>      init.kaiming_normal_(m.weight, a<span class="op">=</span>leaky)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a function based on relu with the parameters already applied</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>act_gr <span class="op">=</span> partial(GeneralRelu, leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a function based on leaky being 0.1</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>iw <span class="op">=</span> partial(init_weights, leaky<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Returns a instance of a model</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model(act<span class="op">=</span>nn.ReLU, nfs<span class="op">=</span><span class="va">None</span>, norm<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># stores convolutions if not passed for later creation</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> nfs <span class="kw">is</span> <span class="va">None</span>: nfs <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">64</span>]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creates convolutions based on conv function for each of the layers in nfs</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [conv(nfs[i], nfs[i<span class="op">+</span><span class="dv">1</span>], act<span class="op">=</span>act, norm<span class="op">=</span>norm) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nfs)<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers, conv(nfs[<span class="op">-</span><span class="dv">1</span>],<span class="dv">10</span>, act<span class="op">=</span><span class="va">None</span>, norm<span class="op">=</span><span class="va">False</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>                         nn.Flatten()).to(def_device)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="co"># collects mean and standard deviations of of each layer thats a ReLu</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co"># astats = ActivationStats(fc.risinstance(nn.ReLU))</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance(GeneralRelu))</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="co"># addeds all call backs into a list for later use.</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>), astats]</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates instance of the model and then applys kaiming_normal to the weights</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(act_gr, norm<span class="op">=</span>nn.BatchNorm2d).<span class="bu">apply</span>(iw)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a instance of the learner function</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> MomentumLearner(model, dls, F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Visualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval</p>
</div>
<div class="cell-output cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">accuracy</th>
<th style="text-align: left;">loss</th>
<th style="text-align: left;">epoch</th>
<th style="text-align: left;">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.599</td>
<td style="text-align: left;">1.220</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.799</td>
<td style="text-align: left;">0.680</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.840</td>
<td style="text-align: left;">0.532</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.835</td>
<td style="text-align: left;">0.558</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.867</td>
<td style="text-align: left;">0.444</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.830</td>
<td style="text-align: left;">0.559</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-markdown/cell-19-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="68e63e43-f178-493c-f6dd-4cf2a09de1cd">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-21-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting asthe standard deviations are lower than 1 but the training went really well, would getting these closer to 1 help?????…. click to expand code</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="bd3ec706-ea4e-421f-e620-ecd98daac47b">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-22-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="implement-lsuv-initialization-as-a-class" class="level1">
<h1>implement LSUV initialization as a class</h1>
<p>This new model is more basic than the first model. Theres no data normalization but we’re keeping Leaky ReLU and changing the weight initialization to a custom LSUV callback.</p>
<p>It will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.</p>
<p>By paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.</p>
<p><strong>Key Technical Information for This Section:</strong></p>
<p><strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units. <br> <strong>Activation is Leaky ReLU :</strong> We will now use Leaky ReLU as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e.&nbsp;it passes a positve number for each of the activations. <br> <strong>Data normalisation :</strong> none <br> <strong>weight normalization :</strong> LSUV <br> <strong>Learning Rate 0.2 :</strong> Initial training was conducted with a learning rate of 0.2. <br> What to Try Next : Weight initilization without input (data) normalization gave good results. It would be a good experiment to add some input (data) normalization like batch normalization. However, the experiment has now finished and im happy with the results here.</p>
<div class="cell" data-outputid="82255664-21fc-4880-e5a8-99246e730165">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dt))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>xb.shape,yb[:<span class="dv">10</span>]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This class implements Layer-Sequential Unit-Variance Initialization (LSUV), a technique used to</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the weights and biases of neural networks. LSUV aims to set these parameters such that</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># during the forward pass, the variance of the activations remains close to 1. This avoids issues</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># commonly associated with poor initialization, such as vanishing or exploding gradients.</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># To achieve this, the class modifies the initial weights and biases in the context of a sample of input</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># data, targeting a specified range for hardware/software-specific floating-point representation. This approach</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># minimizes the risk of exceeding the numerical range, which can lead to unstable training dynamics, or</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># put anotherway reduces the number of neurons contributing (deactivate) and the weight into the final result.</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods within this class handle the adjustment of weights and biases, based on the calculated</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">#  variances and means of the activations. This is typically invoked at the beginning of the training</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co"># process, prior to the main training loop.</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Initial tests have shown effective results, although chart visualizations may</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co"># require further refinement.</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSUVStatsHook(Callback):</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># initialize and store all relevent details to object</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, modules_for_hooks, modules_for_weights, verbose<span class="op">=</span><span class="va">False</span>, debug<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.mean <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.std <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log <span class="op">=</span> fc.noop <span class="cf">if</span> <span class="kw">not</span> verbose <span class="cf">else</span> <span class="bu">print</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.debug <span class="op">=</span> debug</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">#fc.store_attr()</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.modules_for_hooks <span class="op">=</span> modules_for_hooks</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.modules_for_weights <span class="op">=</span> modules_for_weights</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># update hooks</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> hook(<span class="va">self</span>, module, <span class="bu">input</span>, output):</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">#import pdb;pdb.set_trace()</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    acts <span class="op">=</span> output.detach().cpu()</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.mean <span class="op">=</span> acts.mean()</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.std <span class="op">=</span> acts.std()</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># apply hooks to relus, update weights and bias to convs</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> calc_apply_LSUV_weights_bias(<span class="va">self</span>, learn, batch_of_data):</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get all of the modules that will be used for calculating the  lsuv</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"self.modules_for_hooks is type"</span>, <span class="va">self</span>.modules_for_hooks)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"GeneralRelu is type "</span> , GeneralRelu)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    modules_to_apply_hooks <span class="op">=</span> [o <span class="cf">for</span> o <span class="kw">in</span> learn.model.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(o, <span class="va">self</span>.modules_for_hooks)]</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"modules to apply hooks to: "</span>, modules_to_apply_hooks)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>    module_to_update_weights <span class="op">=</span> [o <span class="cf">for</span> o <span class="kw">in</span> learn.model.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(o, <span class="va">self</span>.modules_for_weights)]</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the weights and bias's util desired range is achieved</span></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>    no_of_layers <span class="op">=</span> <span class="bu">len</span>(modules_to_apply_hooks)</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">range</span>(no_of_layers):</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.log(<span class="st">"entering layer : "</span>, item)</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>      handle <span class="op">=</span> modules_to_apply_hooks[item].register_forward_hook(hook_LUSV.hook)</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> learn.model(batch_of_data) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> (<span class="bu">abs</span>(hook_LUSV.std<span class="op">-</span><span class="dv">1</span>)<span class="op">&gt;</span><span class="fl">1e-3</span> <span class="kw">or</span> <span class="bu">abs</span>(hook_LUSV.mean)<span class="op">&gt;</span><span class="fl">1e-3</span>):</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"update weights to modules: "</span>,  module_to_update_weights[item])</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>          module_to_update_weights[item].bias <span class="op">-=</span> hook_LUSV.mean</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>          module_to_update_weights[item].weight.data <span class="op">/=</span> hook_LUSV.std</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"standard deviation is :"</span>, hook_LUSV.std)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"mean is :              "</span>, hook_LUSV.mean)</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># deregister the hook</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>    handle.remove()</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calls calc_apply_LSUV_weights_bias to update weights and bias's</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> before_fit(<span class="va">self</span>, learn):</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    LSUVStatsHook.calc_apply_LSUV_weights_bias(<span class="va">self</span>, learn, batch_of_data<span class="op">=</span>xb)</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom callback with some debugging code commented out</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MetricsCB(Callback):</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>ms, <span class="op">**</span>metrics):</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>        <span class="co">#import pdb; pdb.set_trace()</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> o <span class="kw">in</span> ms: metrics[<span class="bu">type</span>(o).<span class="va">__name__</span>] <span class="op">=</span> o</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics <span class="op">=</span> metrics</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.all_metrics <span class="op">=</span> copy(metrics)</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.all_metrics[<span class="st">'loss'</span>] <span class="op">=</span> <span class="va">self</span>.loss <span class="op">=</span> Mean()</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log(<span class="va">self</span>, d): <span class="bu">print</span>(d)</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_fit(<span class="va">self</span>, learn): learn.metrics <span class="op">=</span> <span class="va">self</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_epoch(<span class="va">self</span>, learn): [o.reset() <span class="cf">for</span> o <span class="kw">in</span> <span class="va">self</span>.all_metrics.values()]</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_epoch(<span class="va">self</span>, learn):</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>        <span class="co">#import pdb; pdb.set_trace()</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>        log <span class="op">=</span> {k:<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">.</span>compute()<span class="sc">:.3f}</span><span class="ss">'</span> <span class="cf">for</span> k,v <span class="kw">in</span> <span class="va">self</span>.all_metrics.items()}</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>        log[<span class="st">'epoch'</span>] <span class="op">=</span> learn.epoch</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>        log[<span class="st">'train'</span>] <span class="op">=</span> <span class="st">'train'</span> <span class="cf">if</span> learn.model.training <span class="cf">else</span> <span class="st">'eval'</span></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log(log)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_batch(<span class="va">self</span>, learn):</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">#import pdb; pdb.set_trace()</span></span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>        x,y,<span class="op">*</span>_ <span class="op">=</span> to_cpu(learn.batch)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics.values(): m.update(to_cpu(learn.preds), y)</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss.update(to_cpu(learn.loss), weight<span class="op">=</span><span class="bu">len</span>(x))</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(f"loss : {learn.loss}, weight : {len(x)}")</span></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the metrics</span></span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> MetricsCB(accuracy<span class="op">=</span>MulticlassAccuracy())</span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a><span class="co"># module for a custom Relu</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeneralRelu(nn.Module):</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, leak<span class="op">=</span><span class="va">None</span>, sub<span class="op">=</span><span class="va">None</span>, maxv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.leak,<span class="va">self</span>.sub,<span class="va">self</span>.maxv <span class="op">=</span> leak,sub,maxv</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.leaky_relu(x,<span class="va">self</span>.leak) <span class="cf">if</span> <span class="va">self</span>.leak <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> F.relu(x)</span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sub <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x <span class="op">-=</span> <span class="va">self</span>.sub</span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.maxv <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x.clamp_max_(<span class="va">self</span>.maxv)</span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a><span class="co"># setup the LSUV hook to pass to the model</span></span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>hook_LUSV <span class="op">=</span> LSUVStatsHook(modules_for_hooks <span class="op">=</span> GeneralRelu, modules_for_weights <span class="op">=</span> nn.Conv2d,verbose<span class="op">=</span><span class="va">False</span>,debug<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a><span class="co"># setup the activation statics and module</span></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>act_gr <span class="op">=</span> partial(GeneralRelu, leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance((GeneralRelu, nn.ReLU)))</span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a><span class="co"># setup the model and call fit</span></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, act<span class="op">=</span>nn.ReLU):</span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span>stride, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, act())</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model(act<span class="op">=</span>nn.ReLU, nfs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> nfs <span class="kw">is</span> <span class="va">None</span>: nfs <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">64</span>]</span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [conv(nfs[i], nfs[i<span class="op">+</span><span class="dv">1</span>], act<span class="op">=</span>act) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nfs)<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers, conv(nfs[<span class="op">-</span><span class="dv">1</span>],<span class="dv">10</span>, act<span class="op">=</span><span class="va">None</span>), nn.Flatten()).to(def_device)</span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(act_gr)</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>), astats, hook_LUSV]</span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> MomentumLearner(model, dls, F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs)</span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Visualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval</p>
</div>
<div class="cell-output cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">accuracy</th>
<th style="text-align: left;">loss</th>
<th style="text-align: left;">epoch</th>
<th style="text-align: left;">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.461</td>
<td style="text-align: left;">1.575</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.693</td>
<td style="text-align: left;">1.012</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.758</td>
<td style="text-align: left;">0.783</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.754</td>
<td style="text-align: left;">0.817</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">eval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.811</td>
<td style="text-align: left;">0.622</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">train</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.799</td>
<td style="text-align: left;">0.681</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-markdown/cell-23-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="79911e6a-c525-4230-8fae-5c9f28a39a4d">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-24-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="c0969b3c-6dd2-40ab-d003-2e086e961067">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-25-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Colour chart to show inactive neurons</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-outputid="1a99dce3-16fb-4ce3-8519-92fddf84cb40">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-markdown/cell-26-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting as this really shows training got to the optimal weights after batch 75, layer 1 jumped up to 1 at batch 0, layer 2 jumped upto 1 at 25, layer 3 jumped up 1 at batch 50 and layer 3 jumped upto 75 so 25 between each of the layers. …. click to expand code</figcaption>
</figure>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>