<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Kelly">

<title>Alex Paul Kelly - The importantance of propper initializtaion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alex Paul Kelly</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/resolver101757" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/alex_paul_kelly" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexpkelly/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The importantance of propper initializtaion</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Kelly </p>
            </div>
    </div>
      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-am-i-writing-about-lsuv" id="toc-why-am-i-writing-about-lsuv" class="nav-link active" data-scroll-target="#why-am-i-writing-about-lsuv">Why am I writing about LSUV?</a></li>
  <li><a href="#why-initialization-model-weights-before-starting-the-optimization" id="toc-why-initialization-model-weights-before-starting-the-optimization" class="nav-link" data-scroll-target="#why-initialization-model-weights-before-starting-the-optimization">Why initialization model weights before starting the optimization</a></li>
  <li><a href="#lsuv-vs-other-methods" id="toc-lsuv-vs-other-methods" class="nav-link" data-scroll-target="#lsuv-vs-other-methods">LSUV vs other methods</a></li>
  <li><a href="#a-walk-through-of-the-code-and-results" id="toc-a-walk-through-of-the-code-and-results" class="nav-link" data-scroll-target="#a-walk-through-of-the-code-and-results">A walk through of the code and results</a></li>
  <li><a href="#setup-enviroment-loading-the-dataset-transforming-the-data-for-training" id="toc-setup-enviroment-loading-the-dataset-transforming-the-data-for-training" class="nav-link" data-scroll-target="#setup-enviroment-loading-the-dataset-transforming-the-data-for-training">Setup enviroment, loading the dataset, transforming the data for training</a></li>
  <li><a href="#find-the-optimal-learning-rate" id="toc-find-the-optimal-learning-rate" class="nav-link" data-scroll-target="#find-the-optimal-learning-rate">Find the optimal learning rate</a></li>
  <li><a href="#example-of-poorly-initialized-no-input-or-weight-initialization" id="toc-example-of-poorly-initialized-no-input-or-weight-initialization" class="nav-link" data-scroll-target="#example-of-poorly-initialized-no-input-or-weight-initialization">Example of poorly initialized (No input or weight initialization)</a></li>
  <li><a href="#initializing-weights-using-kaimings-method" id="toc-initializing-weights-using-kaimings-method" class="nav-link" data-scroll-target="#initializing-weights-using-kaimings-method">Initializing weights using kaimings method</a></li>
  <li><a href="#normalise-batches-using-pytorch-batchnorm2d" id="toc-normalise-batches-using-pytorch-batchnorm2d" class="nav-link" data-scroll-target="#normalise-batches-using-pytorch-batchnorm2d">Normalise batches using pytorch batchnorm2d</a></li>
  <li><a href="#implement-lsuv-initialization-as-a-class" id="toc-implement-lsuv-initialization-as-a-class" class="nav-link" data-scroll-target="#implement-lsuv-initialization-as-a-class">implement LSUV initialization as a class</a>
  <ul class="collapse">
  <li><a href="#rest-of-the-code" id="toc-rest-of-the-code" class="nav-link" data-scroll-target="#rest-of-the-code">rest of the code</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="why-am-i-writing-about-lsuv" class="level1">
<h1>Why am I writing about LSUV?</h1>
<p>I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand innerworkings of each aspect of a nerual network. This will enable me to create new techniques and improve existing techniques and enable me to piece together the right neural network for the right task.</p>
<p>On top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.</p>
</section>
<section id="why-initialization-model-weights-before-starting-the-optimization" class="level1">
<h1>Why initialization model weights before starting the optimization</h1>
<p><img src="weight_initialization.png" class="img-fluid"></p>
<p>Proper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. Its not just about the initialization of the 1st layer of weights, its about all the weights from layer 1 to the last to the outputs.</p>
<p>Here are a few key points on weight initialations :</p>
<ol type="1">
<li>The hardware has floating point limitations that mean it processes limited number of bits and stores in a limited amount of memory. If the weights are too high or too low then it ends up calculating the results with too high or too low to store into memory specified which are called exploding or vanishing neurons (i.e.. dead neurons) at anypoint level in the nereual network. This results in information lost, which are called dead neurons or neurons that dont contribute to the end result in a optimal way.<br>
</li>
<li>The mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.</li>
<li>The standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e.&nbsp;0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.</li>
</ol>
</section>
<section id="lsuv-vs-other-methods" class="level1">
<h1>LSUV vs other methods</h1>
<p>Each model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best. Here are a few examples of initialization techniques :</p>
<ul>
<li>LSUV (Layer-Sequential Unit-Variance) Initialization</li>
<li>Zero Initialization</li>
<li>Random Initialization</li>
<li>Xavier/Glorot Initialization</li>
<li>He Initialization</li>
<li>LeCun Initialization</li>
</ul>
<p>LSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.</p>
</section>
<section id="a-walk-through-of-the-code-and-results" class="level1">
<h1>A walk through of the code and results</h1>
<p>A walk through of the code and the results of the developing an LSUV callback into the FASTAI mini AI learner. We will be covering :</p>
<ul>
<li>Setting up the enviroment, loading the data set and finding the learning rate.</li>
<li>Running the learner without LSUV or any other initialization techniques and exploring the results.</li>
<li>Weight initialization using kaimings method</li>
<li>LSUV training method</li>
<li>Conclusion of the results</li>
</ul>
</section>
<section id="setup-enviroment-loading-the-dataset-transforming-the-data-for-training" class="level1">
<h1>Setup enviroment, loading the dataset, transforming the data for training</h1>
<p>This code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.</p>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698689535429,&quot;user_tz&quot;:0,&quot;elapsed&quot;:19273,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="88f11f70-df3a-47e4-fffe-8da5f493f672">
<details>
<summary>….click to expand code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install required libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install torcheval</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Python Standard Library imports</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections.abc <span class="im">import</span> Mapping</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> operator <span class="im">import</span> attrgetter</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> copy</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> contextmanager</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Third-party library imports</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> TF</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, load_dataset_builder</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastprogress <span class="im">import</span> progress_bar, master_bar</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.test <span class="im">import</span> test_close</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> init</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn,tensor</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torcheval.metrics <span class="im">import</span> MulticlassAccuracy, Mean</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom module imports</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> conv <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.datasets <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.conv <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.activations <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration settings</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>torch.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>, sci_mode<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>mpl.rcParams[<span class="st">'image.cmap'</span>] <span class="op">=</span> <span class="st">'viridis'</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>logging.disable(logging.WARNING)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># get labels</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>x,y <span class="op">=</span> <span class="st">'image'</span>,<span class="st">'label'</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co">#  Street View House Numbers dataset name</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>name <span class="op">=</span> (<span class="st">'svhn'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># fetch dataset from hugging face</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>dsd <span class="op">=</span> load_dataset(name, <span class="st">"cropped_digits"</span>,)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># remove extra (not required for initial run through)</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>dsd.pop(<span class="st">"extra"</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co"># convert images to greyscale</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_to_gray(batch):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> batch[<span class="st">'image'</span>]</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> image.mode <span class="op">!=</span> <span class="st">'L'</span>:  <span class="co"># Only convert if not already grayscale</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        gray_image <span class="op">=</span> image.convert(<span class="st">'L'</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">'image'</span>] <span class="op">=</span> gray_image</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to all datasets</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> dsd.keys():</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    dsd[key] <span class="op">=</span> dsd[key].<span class="bu">map</span>(convert_to_gray, batched<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co"># transform data</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [torch.flatten(TF.to_tensor(o)) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># extract data set</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(dd<span class="op">=</span>tds, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dt))</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>xb.shape,yb[:<span class="dv">10</span>]</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="find-the-optimal-learning-rate" class="level1">
<h1>Find the optimal learning rate</h1>
<p>The Learning Rate Finder is a tool designed to help find a good learning rate for training deep learning models. It increases the learning rate after each mini-batch and records the loss. As the learning rate increases, initially, the loss will decrease (as the model learns). But after a certain point, the learning rate might be too high causing the loss to increase due to overshooting the optimal weights. The usual method is to choose the best learning rate is to choose a figure just before the steep fall.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30069,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698689425359,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="ef29260c-650a-458a-a5d0-14a214b99633">
<details>
<summary>From looking at the chart, it looks like the best learning rate is going to be between 10^-2 and 10^-1…. click to expand code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset and loader</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>lrfind <span class="op">=</span> LRFinderCB()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [TrainCB(), DeviceCB(), lrfind]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fits data</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, epochs<span class="op">=</span><span class="dv">1</span>, xtra_cbs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(model, dls, loss_func<span class="op">=</span>F.cross_entropy, lr<span class="op">=</span><span class="fl">0.0000001</span>, cbs<span class="op">=</span>cbs<span class="op">+</span>fc.L(xtra_cbs))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    learn.fit(epochs)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># along with ni (features) input channels and output channels (feature maps)</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cnn_layers():</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">1</span> ,<span class="dv">8</span>, ks<span class="op">=</span><span class="dv">5</span>),        <span class="co">#14x14</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">8</span> ,<span class="dv">16</span>),             <span class="co">#7x7</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">16</span>,<span class="dv">32</span>),             <span class="co">#4x4</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">32</span>,<span class="dv">64</span>),             <span class="co">#2x2</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">64</span>,<span class="dv">10</span>, act<span class="op">=</span><span class="va">False</span>),  <span class="co">#1x1</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        nn.Flatten()]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>fit(model)<span class="op">;</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.plot(lrfind.lrs, lrfind.losses)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Learning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="example-of-poorly-initialized-no-input-or-weight-initialization" class="level1">
<h1>Example of poorly initialized (No input or weight initialization)</h1>
<p>This section will show training pipeline and charts that show key metrics of the training with annotations of how it faired. <br></p>
<p>Key technnical information for this section : <br></p>
<p><strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> A Convultion neural network showing the number of filters in each layer and ending with 10 output units. <br> <strong>Activation is nn.ReLU :</strong> ReLU function introduces non-linearity to the model. <br> <strong>Color_dim :</strong> Shows a small band of yellow at the bottom of the chart from batch 1 until the end representing a lot of dead neurons all the way. <br> <strong>Dead_chart :</strong> More proof neurons are dead at the start and not contributing to the end result. <br> <strong>Plot_stats :</strong> Means close to zero but standard deviations far off expected goal of 1, to far from 1 to train optimally.</p>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698701003415,&quot;user_tz&quot;:0,&quot;elapsed&quot;:32002,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="e5421bf1-2c3a-48ac-b114-adc4997dc46b">
<details>
<summary>fill in what teh chart shows …. click to expand code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># improved function to include labelling</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ActivationStats(HooksCallback):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mod_filter<span class="op">=</span>fc.noop):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(append_stats, mod_filter)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> color_dim(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">11</span>,<span class="dv">5</span>)):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      fig, axes <span class="op">=</span> get_grid(<span class="bu">len</span>(<span class="va">self</span>), figsize<span class="op">=</span>figsize)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> ax, h <span class="kw">in</span> <span class="bu">zip</span>(axes.flat, <span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>          im <span class="op">=</span> ax.imshow(get_hist(h), origin<span class="op">=</span><span class="st">'lower'</span>)  <span class="co"># Using imshow directly</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Add labels, title, and colorbar for clarity</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>          ax.set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>          ax.set_ylabel(<span class="st">"Activation Value"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>          ax.set_title(<span class="st">"Layer "</span> <span class="op">+</span> <span class="st">"str(self.index(h))"</span> <span class="op">+</span> <span class="st">" Activations"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>          cbar <span class="op">=</span> plt.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>          cbar.set_label(<span class="st">"Frequency"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>      plt.tight_layout()  <span class="co"># Prevent overlap</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dead_chart(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">11</span>,<span class="dv">5</span>)):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> get_grid(<span class="bu">len</span>(<span class="va">self</span>), figsize<span class="op">=</span>figsize)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ax, h <span class="kw">in</span> <span class="bu">zip</span>(axes.flatten(), <span class="va">self</span>):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            ax.plot(get_min(h), linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            ax.set_ylim(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            ax.set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            ax.set_ylabel(<span class="st">"Activation Value"</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            ax.set_title(<span class="st">"Layer "</span> <span class="op">+</span> <span class="st">"str(self.index(h))"</span> <span class="op">+</span> <span class="st">" Dead Activations"</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()  <span class="co"># Prevent overlap</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_stats(<span class="va">self</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>)):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>figsize)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="dv">0</span>,<span class="dv">1</span>:</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                axs[i].plot(h.stats[i])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_title(<span class="st">'Means'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_title(<span class="st">'Stdevs'</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch Number"</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_ylabel(<span class="st">"Mean Activation Value"</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">1</span>].set_ylabel(<span class="st">"Standard Deviation of Activation Value"</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        plt.legend(fc.L.<span class="bu">range</span>(<span class="va">self</span>))</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#plt.tight_layout()  # Prevent overlap</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># transform dataset and loader</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co"># setup model for learning</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> MetricsCB(accuracy<span class="op">=</span>MulticlassAccuracy())</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [TrainCB(), DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="co"># fits dataset</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, epochs<span class="op">=</span><span class="dv">1</span>, xtra_cbs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> Learner(model, dls, loss_func<span class="op">=</span>F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs<span class="op">+</span>fc.L(xtra_cbs))</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    learn.fit(epochs)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> learn</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co"># conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># along with ni (features) input channels and output channels (feature maps)</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cnn_layers():</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">1</span> ,<span class="dv">8</span>, ks<span class="op">=</span><span class="dv">5</span>),        <span class="co">#14x14</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">8</span> ,<span class="dv">16</span>),             <span class="co">#7x7</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">16</span>,<span class="dv">32</span>),             <span class="co">#4x4</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">32</span>,<span class="dv">64</span>),             <span class="co">#2x2</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        conv(<span class="dv">64</span>,<span class="dv">10</span>, act<span class="op">=</span><span class="va">False</span>),  <span class="co">#1x1</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        nn.Flatten()]</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="co">#astats = ActivationStats(fc.risinstance(GeneralRelu))</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance(nn.ReLU))</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>fit(model, xtra_cbs<span class="op">=</span>[astats])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Learning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).</p>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.186</td>
<td>2.248</td>
<td>0</td>
<td>train</td>
</tr>
<tr class="even">
<td>0.196</td>
<td>2.225</td>
<td>0</td>
<td>eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-8-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;error&quot;,&quot;timestamp&quot;:1698756719186,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="51241449-1010-44a4-b8cf-8530cb70f233" data-execution_count="1">
<details>
<summary>The color represents the frequency of activations in a specific range. Using the viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently for that batch.</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>NameError: ignored</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:951,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698701005584,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="a36e7d75-99b0-4356-cc4c-dd150d12b004">
<details>
<summary>Plots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:863,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698701006441,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="d00640f5-1b92-4e66-ae23-df59a406ca7c">
<details>
<summary>plots of dead neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of dead neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="initializing-weights-using-kaimings-method" class="level1">
<h1>Initializing weights using kaimings method</h1>
<p>This section presents the training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.</p>
<p>By paying attention to these key metrics and visual cues, we can assess the shortcomings in the model’s initialization and training regimen, specifically issues such as dead neurons and poorly tuned weight initializations. We can then compare them against previously un-initialized weights for any improvement.</p>
<p><br></p>
<p>Key Technical Information for This Section:</p>
<p><br> <strong>Conv 1,8,16,32,64 -&gt; 10 :</strong> This denotes a Convolutional Neural Network with varying numbers of filters across different layers, culminating in 10 output units. <br> <strong>Activation is nn.ReLU :</strong> The model utilizes the ReLU (Rectified Linear Unit) activation function to introduce non-linearity, aiding in better approximations of complex functions. <br> <strong>Color_dim :</strong> In the chart, a yellow band stretching from the first batch until the end indicates a high incidence of ‘dead’ neurons, which are not contributing to the network’s learning. <br> <strong>Dead_chart :</strong> This offers additional evidence that numerous neurons are inactive right from the start, leading to an inefficient learning process. <br> <strong>Plot_stats :</strong> The model’s statistics reveal mean values that are close to zero, yet standard deviations that are significantly divergent from the ideal value of 1. This indicates that the model is not optimally configured for training. <br></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:12,&quot;status&quot;:&quot;error&quot;,&quot;timestamp&quot;:1698761652192,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="7852602e-3987-4282-abed-c25d61e9a0fe" data-execution_count="2">
<details>
<summary>Get the standard deviation and mean of the batch of data….. click to expand code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the mean and standard deviation.</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the mean should be close to 0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the standard deviation wants to be close to 1</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>xmean,xstd <span class="op">=</span> xb.mean(),xb.std()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>xmean,xstd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>NameError: ignored</code></pre>
</div>
</div>
<p>Kaiming initialization sets the initial weights of each layer according to the following distribution:</p>
<p><span class="math display">\[
\text{weights} \sim \mathcal{N}\left(0, \sqrt{\frac{2}{\text{fan\_in}}}\right)
\]</span></p>
<p>weights are the weights of a particular layer<br> fan_in is the number of input features to the layer (for CNNs, it’s the number of input channels times the kernel height times the kernel width)<br></p>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614704709,&quot;user_tz&quot;:0,&quot;elapsed&quot;:2374,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="2cfd6164-e668-4a6c-fa33-da780a70a3dc">
<details>
<summary>Apply Kaiming initialization to the data set….. click to expand code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>xl,yl <span class="op">=</span> <span class="st">'image'</span>,<span class="st">'label'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[xl] <span class="op">=</span> [(TF.to_tensor(o)<span class="op">-</span>xmean)<span class="op">/</span>xstd <span class="cf">for</span> o <span class="kw">in</span> b[xl]]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dls.train))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># now the data is closer to where we want it.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>xmean,xstd <span class="op">=</span> xb.mean(),xb.std()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>xmean,xstd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:33648,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614738353,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="f9eed8c3-a0c6-441f-f0c4-e24a092780e4">
<details>
<summary>The code sets up a deep learning pipeline for training a CNN model on a dataset. It defines transformations for the dataset, specifies data loaders, metrics, and callbacks, and then creates a CNN model with specific convolutional layers before fitting the model using a learner object with a reate of lr=0.2, and includes activation statistics….. click to expand code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># slightly better than last time but definatly not perfect&gt;</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>cnn_layers())</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>fit(model, xtra_cbs<span class="op">=</span>[astats])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<p>Learning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).</p>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.188</td>
<td>2.241</td>
<td>0</td>
<td>train</td>
</tr>
<tr class="even">
<td>0.196</td>
<td>2.225</td>
<td>0</td>
<td>eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-15-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:370,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614743672,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="550515b8-58f1-47b8-c560-1fec939a8aa6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Colour chart to show dead neurons</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:902,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614745203,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="f48be214-f83a-412c-9d0e-c29ea32e443c">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stanard deviations still away from one but mean looks reasonable</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plots of means and standard deviations for each layer</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1155,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614747922,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="0ef3bcfa-7a08-4631-94c3-343f2e9464e3">
<details>
<summary>plots of dead neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first layer quite bad and last layer is totally dead.</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">plots of dead neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="normalise-batches-using-pytorch-batchnorm2d" class="level1">
<h1>Normalise batches using pytorch batchnorm2d</h1>
<p><br>conv 1,8,16,32,64 -&gt; 10 <br> leaky relu <br> learning rate 0.2 <br> best training so far <br> accuracy 0.830 <br> learning rate : 0.559 <br> also example of over training <br> What to try next ????</p>
<p>::: {.cell Avoiding Dead Neurons=‘Leaky ReLU helps to mitigate the problem of “dead neurons” that can occur with ReLU units,’}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># where neurons get stuck during training and always output a zero value. By allowing a small, non-zero output for negative inputs,</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Leaky ReLU ensures that gradients can still flow through the neuron, which can help to keep learning progressing.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeneralRelu(nn.Module):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, leak<span class="op">=</span><span class="va">None</span>, sub<span class="op">=</span><span class="va">None</span>, maxv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.leak,<span class="va">self</span>.sub,<span class="va">self</span>.maxv <span class="op">=</span> leak,sub,maxv</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.leaky_relu(x,<span class="va">self</span>.leak) <span class="cf">if</span> <span class="va">self</span>.leak <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> F.relu(x)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sub <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x <span class="op">-=</span> <span class="va">self</span>.sub</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.maxv <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x.clamp_max_(<span class="va">self</span>.maxv)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_func(f, start<span class="op">=-</span><span class="fl">5.</span>, end<span class="op">=</span><span class="fl">5.</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.linspace(start, end, steps)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, f(x))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, which<span class="op">=</span><span class="st">'both'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:24,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1696525627774,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:-60}" data-outputid="54f69005-5636-467e-b9ad-72f6cf7bddaf">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visual representation of the new relu, left values Jeremeys example</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>plot_func(GeneralRelu(leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, act<span class="op">=</span>nn.ReLU, norm<span class="op">=</span><span class="va">None</span>, bias<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bias <span class="kw">is</span> <span class="va">None</span>: bias <span class="op">=</span> <span class="kw">not</span> <span class="bu">isinstance</span>(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [nn.Conv2d(ni, nf, stride<span class="op">=</span>stride, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>, bias<span class="op">=</span>bias)]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm: layers.append(norm(nf))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: layers.append(act())</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
<p>::: {.cell 0=‘i’ 1=‘n’ 2=‘i’ 3=‘t’ 4=‘i’ 5=‘a’ 6=‘l’ 7=‘i’ 8=‘z’ 9=‘e’ 10=‘s’ 11=’ ’ 12=‘w’ 13=‘e’ 14=‘i’ 15=‘g’ 16=‘h’ 17=‘t’ 18=‘s’ 19=’ ’ 20=‘b’ 21=‘a’ 22=‘s’ 23=‘e’ 24=‘d’ 25=’ ’ 26=‘o’ 27=‘n’ 28=’ ’ 29=‘k’ 30=‘a’ 31=‘i’ 32=‘m’ 33=‘i’ 34=‘n’ 35=‘g’ 36=‘<em>’ 37=’n’ 38=’o’ 39=’r’ 40=’m’ 41=’a’ 42=’l’ 43=’</em>’}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(m, leaky<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># checks for a instance of layer and module of the neural network</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># checks for a instance of 1d, 2d, 3d neural network</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>      <span class="co"># creates the initialization of the weights, for a, anything that is not zero, standard relu is assumed.</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>      init.kaiming_normal_(m.weight, a<span class="op">=</span>leaky)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a function based on relu with the parameters already applied</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>act_gr <span class="op">=</span> partial(GeneralRelu, leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a function based on leaky being 0.1</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>iw <span class="op">=</span> partial(init_weights, leaky<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Returns a instance of a model</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model(act<span class="op">=</span>nn.ReLU, nfs<span class="op">=</span><span class="va">None</span>, norm<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># stores convolutions if not passed for later creation</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> nfs <span class="kw">is</span> <span class="va">None</span>: nfs <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">64</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creates convolutions based on conv function for each of the layers in nfs</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [conv(nfs[i], nfs[i<span class="op">+</span><span class="dv">1</span>], act<span class="op">=</span>act, norm<span class="op">=</span>norm) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nfs)<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers, conv(nfs[<span class="op">-</span><span class="dv">1</span>],<span class="dv">10</span>, act<span class="op">=</span><span class="va">None</span>, norm<span class="op">=</span><span class="va">False</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                         nn.Flatten()).to(def_device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># collects mean and standard deviations of of each layer thats a ReLu</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># astats = ActivationStats(fc.risinstance(nn.ReLU))</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance(GeneralRelu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># addeds all call backs into a list for later use.</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>), astats]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1061,&quot;status&quot;:&quot;error&quot;,&quot;timestamp&quot;:1696525642549,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:-60}" data-outputid="4668e288-7f23-40fa-d804-f94cfd7415f2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates instance of the model and then applys kaiming_normal to the weights</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(act_gr, norm<span class="op">=</span>nn.BatchNorm2d).<span class="bu">apply</span>(iw)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a instance of the learner function</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> MomentumLearner(model, dls, F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="2" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/2 00:00&lt;?]
    </div>
    


    <div>
      <progress value="0" class="" max="72" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/72 00:00&lt;?]
    </div>
    
</div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: ignored</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:25,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1694518294077,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:-60}" data-outputid="1a545df0-76b7-4053-d9d1-a26abe0869da">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:626,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1694518294684,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:-60}" data-outputid="65d772b9-6be9-4276-9e98-0c08686c7219">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>astats.plot_stats()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:416,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1694518295092,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:-60}" data-outputid="d52706dd-36bd-4834-fefc-d72f48ce281c">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="implement-lsuv-initialization-as-a-class" class="level1">
<h1>implement LSUV initialization as a class</h1>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> miniai.activations <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614756267,&quot;user_tz&quot;:0,&quot;elapsed&quot;:502,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="a38b4aa5-94b5-41d6-e825-d133a2a88d03">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="at">@inplace</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformi(b): b[x] <span class="op">=</span> [TF.to_tensor(o) <span class="cf">for</span> o <span class="kw">in</span> b[x]]</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dt))</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>xb.shape,yb[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(torch.Size([1024, 1, 32, 32]), tensor([6, 1, 8, 2, 1, 1, 4, 1, 6, 5]))</code></pre>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>tds <span class="op">=</span> dsd.with_transform(transformi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:862,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614758699,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="060bac6b-16ce-46df-bb31-6056c5b94c09">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dd(dd<span class="op">=</span>tds, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> dls.train</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dt))</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>xb.shape,yb[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>(torch.Size([1024, 1, 32, 32]), tensor([3, 1, 6, 9, 3, 3, 7, 3, 3, 0]))</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:13250,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614772677,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="8b0e7f40-884b-4d49-e4e5-59162b61d31c">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># displays the tensor</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>tds[<span class="st">"train"</span>][<span class="st">"image"</span>][<span class="dv">55</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([[[0.76, 0.76, 0.76,  ..., 0.65, 0.65, 0.61],
         [0.64, 0.64, 0.65,  ..., 0.68, 0.68, 0.62],
         [0.51, 0.51, 0.51,  ..., 0.71, 0.71, 0.63],
         ...,
         [0.67, 0.67, 0.67,  ..., 0.65, 0.69, 0.65],
         [0.73, 0.74, 0.75,  ..., 0.63, 0.67, 0.64],
         [0.76, 0.76, 0.77,  ..., 0.65, 0.68, 0.66]]])</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:12740,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614785415,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="227b7fc3-3697-4bf6-8dec-e6d7eae769c4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># shows the values 32 * 32 * 3 (RGB)</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>tds[<span class="st">"train"</span>][<span class="st">"image"</span>][<span class="dv">55</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>torch.Size([1, 32, 32])</code></pre>
</div>
</div>
<section id="rest-of-the-code" class="level2">
<h2 class="anchored" data-anchor-id="rest-of-the-code">rest of the code</h2>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This class implements Layer-Sequential Unit-Variance Initialization (LSUV), a technique used to</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the weights and biases of neural networks. LSUV aims to set these parameters such that</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># during the forward pass, the variance of the activations remains close to 1. This avoids issues</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># commonly associated with poor initialization, such as vanishing or exploding gradients.</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To achieve this, the class modifies the initial weights and biases in the context of a sample of input</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># data, targeting a specified range for hardware/software-specific floating-point representation. This approach</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># minimizes the risk of exceeding the numerical range, which can lead to unstable training dynamics, or</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># put anotherway reduces the number of neurons contributing (deactivate) and the weight into the final result.</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Key methods within this class handle the adjustment of weights and biases, based on the calculated</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">#  variances and means of the activations. This is typically invoked at the beginning of the training</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># process, prior to the main training loop.</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Initial tests have shown effective results, although chart visualizations may</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co"># require further refinement.</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSUVStatsHook(Callback):</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># initialize and store all relevent details to object</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, modules_for_hooks, modules_for_weights, verbose<span class="op">=</span><span class="va">False</span>, debug<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.mean <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.std <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log <span class="op">=</span> fc.noop <span class="cf">if</span> <span class="kw">not</span> verbose <span class="cf">else</span> <span class="bu">print</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.debug <span class="op">=</span> debug</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#fc.store_attr()</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.modules_for_hooks <span class="op">=</span> modules_for_hooks</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.modules_for_weights <span class="op">=</span> modules_for_weights</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># update hooks</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> hook(<span class="va">self</span>, module, <span class="bu">input</span>, output):</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#import pdb;pdb.set_trace()</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>    acts <span class="op">=</span> output.detach().cpu()</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.mean <span class="op">=</span> acts.mean()</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.std <span class="op">=</span> acts.std()</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># apply hooks to relus, update weights and bias to convs</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> calc_apply_LSUV_weights_bias(<span class="va">self</span>, learn, batch_of_data):</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get all of the modules that will be used for calculating the  lsuv</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"self.modules_for_hooks is type"</span>, <span class="va">self</span>.modules_for_hooks)</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"GeneralRelu is type "</span> , GeneralRelu)</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    modules_to_apply_hooks <span class="op">=</span> [o <span class="cf">for</span> o <span class="kw">in</span> learn.model.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(o, <span class="va">self</span>.modules_for_hooks)]</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.log(<span class="st">"modules to apply hooks to: "</span>, modules_to_apply_hooks)</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>    module_to_update_weights <span class="op">=</span> [o <span class="cf">for</span> o <span class="kw">in</span> learn.model.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(o, <span class="va">self</span>.modules_for_weights)]</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the weights and bias's util desired range is achieved</span></span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>    no_of_layers <span class="op">=</span> <span class="bu">len</span>(modules_to_apply_hooks)</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">range</span>(no_of_layers):</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.log(<span class="st">"entering layer : "</span>, item)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>      handle <span class="op">=</span> modules_to_apply_hooks[item].register_forward_hook(hook_LUSV.hook)</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> learn.model(batch_of_data) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> (<span class="bu">abs</span>(hook_LUSV.std<span class="op">-</span><span class="dv">1</span>)<span class="op">&gt;</span><span class="fl">1e-3</span> <span class="kw">or</span> <span class="bu">abs</span>(hook_LUSV.mean)<span class="op">&gt;</span><span class="fl">1e-3</span>):</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"update weights to modules: "</span>,  module_to_update_weights[item])</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>          module_to_update_weights[item].bias <span class="op">-=</span> hook_LUSV.mean</span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a>          module_to_update_weights[item].weight.data <span class="op">/=</span> hook_LUSV.std</span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"standard deviation is :"</span>, hook_LUSV.std)</span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.log(<span class="st">"mean is :              "</span>, hook_LUSV.mean)</span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># deregister the hook</span></span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a>    handle.remove()</span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calls calc_apply_LSUV_weights_bias to update weights and bias's</span></span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> before_fit(<span class="va">self</span>, learn):</span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.debug : <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a>    LSUVStatsHook.calc_apply_LSUV_weights_bias(<span class="va">self</span>, learn, batch_of_data<span class="op">=</span>xb)</span>
<span id="cb42-65"><a href="#cb42-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-66"><a href="#cb42-66" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MetricsCB(Callback):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>ms, <span class="op">**</span>metrics):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> o <span class="kw">in</span> ms: metrics[<span class="bu">type</span>(o).<span class="va">__name__</span>] <span class="op">=</span> o</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.metrics <span class="op">=</span> metrics</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.all_metrics <span class="op">=</span> copy(metrics)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.all_metrics[<span class="st">'loss'</span>] <span class="op">=</span> <span class="va">self</span>.loss <span class="op">=</span> Mean()</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log(<span class="va">self</span>, d): <span class="bu">print</span>(d)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_fit(<span class="va">self</span>, learn): learn.metrics <span class="op">=</span> <span class="va">self</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_epoch(<span class="va">self</span>, learn): [o.reset() <span class="cf">for</span> o <span class="kw">in</span> <span class="va">self</span>.all_metrics.values()]</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_epoch(<span class="va">self</span>, learn):</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        log <span class="op">=</span> {k:<span class="ss">f'</span><span class="sc">{</span>v<span class="sc">.</span>compute()<span class="sc">:.3f}</span><span class="ss">'</span> <span class="cf">for</span> k,v <span class="kw">in</span> <span class="va">self</span>.all_metrics.items()}</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        log[<span class="st">'epoch'</span>] <span class="op">=</span> learn.epoch</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>        log[<span class="st">'train'</span>] <span class="op">=</span> <span class="st">'train'</span> <span class="cf">if</span> learn.model.training <span class="cf">else</span> <span class="st">'eval'</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log(log)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_batch(<span class="va">self</span>, learn):</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#import pdb; pdb.set_trace()</span></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        x,y,<span class="op">*</span>_ <span class="op">=</span> to_cpu(learn.batch)</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics.values(): m.update(to_cpu(learn.preds), y)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss.update(to_cpu(learn.loss), weight<span class="op">=</span><span class="bu">len</span>(x))</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"loss : </span><span class="sc">{</span>learn<span class="sc">.</span>loss<span class="sc">}</span><span class="ss">, weight : </span><span class="sc">{</span><span class="bu">len</span>(x)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:6230,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614801168,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="7d7829ff-199d-4f2c-e8b7-2f36d6d293b5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> MetricsCB(accuracy<span class="op">=</span>MulticlassAccuracy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>
PYDEV DEBUGGER WARNING:
sys.settrace() should not be used when the debugger is being used.
This may cause the debugger to stop working correctly.
If this is needed, please check: 
http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html
to see how to restore the debug tracing back correctly.
Call Location:
  File "/usr/lib/python3.10/bdb.py", line 336, in set_trace
    sys.settrace(self.trace_dispatch)


PYDEV DEBUGGER WARNING:
sys.settrace() should not be used when the debugger is being used.
This may cause the debugger to stop working correctly.
If this is needed, please check: 
http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html
to see how to restore the debug tracing back correctly.
Call Location:
  File "/usr/lib/python3.10/bdb.py", line 347, in set_continue
    sys.settrace(None)
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(4)__init__()
      2     def __init__(self, *ms, **metrics):
      3         import pdb; pdb.set_trace()
----&gt; 4         for o in ms: metrics[type(o).__name__] = o
      5         self.metrics = metrics
      6         self.all_metrics = copy(metrics)

ipdb&gt; 
ipdb&gt; n
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(5)__init__()
      3         import pdb; pdb.set_trace()
      4         for o in ms: metrics[type(o).__name__] = o
----&gt; 5         self.metrics = metrics
      6         self.all_metrics = copy(metrics)
      7         self.all_metrics['loss'] = self.loss = Mean()

ipdb&gt; c</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeneralRelu(nn.Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, leak<span class="op">=</span><span class="va">None</span>, sub<span class="op">=</span><span class="va">None</span>, maxv<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.leak,<span class="va">self</span>.sub,<span class="va">self</span>.maxv <span class="op">=</span> leak,sub,maxv</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.leaky_relu(x,<span class="va">self</span>.leak) <span class="cf">if</span> <span class="va">self</span>.leak <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> F.relu(x)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sub <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x <span class="op">-=</span> <span class="va">self</span>.sub</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.maxv <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: x.clamp_max_(<span class="va">self</span>.maxv)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>hook_LUSV <span class="op">=</span> LSUVStatsHook(modules_for_hooks <span class="op">=</span> GeneralRelu, modules_for_weights <span class="op">=</span> nn.Conv2d,verbose<span class="op">=</span><span class="va">True</span>,debug<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv(ni, nf, ks<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, act<span class="op">=</span>nn.ReLU):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> nn.Conv2d(ni, nf, stride<span class="op">=</span>stride, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: res <span class="op">=</span> nn.Sequential(res, act())</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model(act<span class="op">=</span>nn.ReLU, nfs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> nfs <span class="kw">is</span> <span class="va">None</span>: nfs <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">64</span>]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [conv(nfs[i], nfs[i<span class="op">+</span><span class="dv">1</span>], act<span class="op">=</span>act) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(nfs)<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers, conv(nfs[<span class="op">-</span><span class="dv">1</span>],<span class="dv">10</span>, act<span class="op">=</span><span class="va">None</span>), nn.Flatten()).to(def_device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>act_gr <span class="op">=</span> partial(GeneralRelu, leak<span class="op">=</span><span class="fl">0.1</span>, sub<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>astats <span class="op">=</span> ActivationStats(fc.risinstance((GeneralRelu, nn.ReLU)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(act_gr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614807639,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="5eef4e74-b1cb-482d-a6eb-5537067e8dd6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>Sequential(
  (0): Sequential(
    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (1): Sequential(
    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (2): Sequential(
    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (3): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (4): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (5): Flatten(start_dim=1, end_dim=-1)
)</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614808691,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;},&quot;user_tz&quot;:0}" data-outputid="4d484c0e-9f67-49d5-eba8-4c5627181ea0">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>Sequential(
  (0): Sequential(
    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (1): Sequential(
    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (2): Sequential(
    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (3): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): GeneralRelu()
  )
  (4): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (5): Flatten(start_dim=1, end_dim=-1)
)</code></pre>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>cbs <span class="op">=</span> [DeviceCB(), metrics, ProgressCB(plot<span class="op">=</span><span class="va">True</span>), astats, hook_LUSV]</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co">#cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698614994043,&quot;user_tz&quot;:0,&quot;elapsed&quot;:182170,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="1f847a32-dcad-41d1-be22-175adc8cf079">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> MomentumLearner(model, dls, F.cross_entropy, lr<span class="op">=</span><span class="fl">0.2</span>, cbs<span class="op">=</span>cbs)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>self.modules_for_hooks is type &lt;class '__main__.GeneralRelu'&gt;
GeneralRelu is type  &lt;class '__main__.GeneralRelu'&gt;
modules to apply hooks to:  [GeneralRelu(), GeneralRelu(), GeneralRelu(), GeneralRelu()]
entering layer :  0
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.18)
mean is :               tensor(-0.18)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.73)
mean is :               tensor(0.34)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.92)
mean is :               tensor(0.25)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.94)
mean is :               tensor(0.14)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.97)
mean is :               tensor(0.10)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.07)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.05)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.04)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.03)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
entering layer :  1
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.20)
mean is :               tensor(-0.32)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.17)
mean is :               tensor(0.18)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.95)
mean is :               tensor(0.03)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.03)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
entering layer :  2
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.34)
mean is :               tensor(-0.23)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.06)
mean is :               tensor(0.22)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.94)
mean is :               tensor(0.07)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.06)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.04)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.03)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
entering layer :  3
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.29)
mean is :               tensor(-0.24)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.05)
mean is :               tensor(0.28)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.93)
mean is :               tensor(0.09)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.08)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.98)
mean is :               tensor(0.05)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.04)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.03)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.02)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(0.99)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.01)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
update weights to modules:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
standard deviation is : tensor(1.00)
mean is :               tensor(0.00)
loss : 2.3288421630859375, weight : 1024
loss : 2.4144644737243652, weight : 1024
loss : 2.362283706665039, weight : 1024
loss : 2.2671492099761963, weight : 1024
loss : 2.2612507343292236, weight : 1024
loss : 2.2662289142608643, weight : 1024
loss : 2.231084108352661, weight : 1024
loss : 2.2852299213409424, weight : 1024
loss : 2.2667036056518555, weight : 1024
loss : 2.2048332691192627, weight : 1024
loss : 2.1808979511260986, weight : 1024
loss : 2.163529872894287, weight : 1024
loss : 2.1725950241088867, weight : 1024
loss : 2.1274478435516357, weight : 1024
loss : 2.0760231018066406, weight : 1024
loss : 2.056678295135498, weight : 1024
loss : 2.0095269680023193, weight : 1024
loss : 1.9694942235946655, weight : 1024
loss : 1.9365414381027222, weight : 1024
loss : 1.875935673713684, weight : 1024
loss : 1.8481940031051636, weight : 1024
loss : 1.7602665424346924, weight : 1024
loss : 1.7638370990753174, weight : 1024
loss : 1.7370072603225708, weight : 1024
loss : 1.6375268697738647, weight : 1024
loss : 1.6320276260375977, weight : 1024
loss : 1.5910569429397583, weight : 1024
loss : 1.5497292280197144, weight : 1024
loss : 1.5884828567504883, weight : 1024
loss : 1.5222012996673584, weight : 1024
loss : 1.4945448637008667, weight : 1024
loss : 1.469292163848877, weight : 1024
loss : 1.5230698585510254, weight : 1024
loss : 1.3599486351013184, weight : 1024
loss : 1.3687541484832764, weight : 1024
loss : 1.3288720846176147, weight : 1024
loss : 1.3184683322906494, weight : 1024
loss : 1.2712934017181396, weight : 1024
loss : 1.337011456489563, weight : 1024
loss : 1.2683963775634766, weight : 1024
loss : 1.2224137783050537, weight : 1024
loss : 1.2288554906845093, weight : 1024
loss : 1.1310187578201294, weight : 1024
loss : 1.2161589860916138, weight : 1024
loss : 1.1464418172836304, weight : 1024
loss : 1.1548887491226196, weight : 1024
loss : 1.070115327835083, weight : 1024
loss : 0.9872815608978271, weight : 1024
loss : 1.0875775814056396, weight : 1024
loss : 1.0699241161346436, weight : 1024
loss : 1.0478270053863525, weight : 1024
loss : 1.046141266822815, weight : 1024
loss : 1.0289545059204102, weight : 1024
loss : 1.0417355298995972, weight : 1024
loss : 1.0093022584915161, weight : 1024
loss : 1.0681254863739014, weight : 1024
loss : 0.9534207582473755, weight : 1024
loss : 1.0055116415023804, weight : 1024
loss : 0.9799197912216187, weight : 1024
loss : 0.9547203779220581, weight : 1024
loss : 0.98872971534729, weight : 1024
loss : 0.890691876411438, weight : 1024
loss : 0.9401361346244812, weight : 1024
loss : 0.9673210382461548, weight : 1024
loss : 0.9231631755828857, weight : 1024
loss : 0.8658212423324585, weight : 1024
loss : 0.9172657132148743, weight : 1024
loss : 0.8740915060043335, weight : 1024
loss : 0.91790771484375, weight : 1024
loss : 0.8406455516815186, weight : 1024
loss : 0.8472331762313843, weight : 1024
loss : 0.8115912079811096, weight : 553
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c
loss : 0.9730837345123291, weight : 2048
loss : 0.9031280279159546, weight : 2048
loss : 0.9105209112167358, weight : 2048
loss : 0.9346651434898376, weight : 2048
loss : 0.9458200335502625, weight : 2048
loss : 0.9227686524391174, weight : 2048
loss : 0.8667200207710266, weight : 2048
loss : 0.9434890747070312, weight : 2048
loss : 0.9403440952301025, weight : 2048
loss : 0.9406253099441528, weight : 2048
loss : 0.9203757047653198, weight : 2048
loss : 0.9620794653892517, weight : 2048
loss : 0.8781284093856812, weight : 1456
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c
loss : 0.8880366086959839, weight : 1024
loss : 0.8387854695320129, weight : 1024
loss : 0.7737706899642944, weight : 1024
loss : 0.8279840350151062, weight : 1024
loss : 0.8342637419700623, weight : 1024
loss : 0.7931831479072571, weight : 1024
loss : 0.8856449127197266, weight : 1024
loss : 0.7678688168525696, weight : 1024
loss : 0.7933114767074585, weight : 1024
loss : 0.812987744808197, weight : 1024
loss : 0.8629277944564819, weight : 1024
loss : 0.8450809717178345, weight : 1024
loss : 0.7738507390022278, weight : 1024
loss : 0.7765510082244873, weight : 1024
loss : 0.7914267182350159, weight : 1024
loss : 0.8355770707130432, weight : 1024
loss : 0.7604403495788574, weight : 1024
loss : 0.7562274932861328, weight : 1024
loss : 0.7987656593322754, weight : 1024
loss : 0.7075282335281372, weight : 1024
loss : 0.7657550573348999, weight : 1024
loss : 0.7554774284362793, weight : 1024
loss : 0.7874916791915894, weight : 1024
loss : 0.6896514296531677, weight : 1024
loss : 0.6963939070701599, weight : 1024
loss : 0.8652840256690979, weight : 1024
loss : 0.7131621837615967, weight : 1024
loss : 0.7145207524299622, weight : 1024
loss : 0.7481014728546143, weight : 1024
loss : 0.7043192982673645, weight : 1024
loss : 0.7140964865684509, weight : 1024
loss : 0.6462891697883606, weight : 1024
loss : 0.7880403399467468, weight : 1024
loss : 0.8598031401634216, weight : 1024
loss : 0.710164487361908, weight : 1024
loss : 0.6585257053375244, weight : 1024
loss : 0.6656920909881592, weight : 1024
loss : 0.6313357949256897, weight : 1024
loss : 0.7181784510612488, weight : 1024
loss : 0.6711898446083069, weight : 1024
loss : 0.6897220611572266, weight : 1024
loss : 0.7334001064300537, weight : 1024
loss : 0.664068341255188, weight : 1024
loss : 0.7093210816383362, weight : 1024
loss : 0.706783652305603, weight : 1024
loss : 0.6914975047111511, weight : 1024
loss : 0.7816950082778931, weight : 1024
loss : 0.6877917647361755, weight : 1024
loss : 0.7508887052536011, weight : 1024
loss : 0.6346502304077148, weight : 1024
loss : 0.7258914113044739, weight : 1024
loss : 0.6305448412895203, weight : 1024
loss : 0.7142507433891296, weight : 1024
loss : 0.6869061589241028, weight : 1024
loss : 0.7385833859443665, weight : 1024
loss : 0.6395807266235352, weight : 1024
loss : 0.6993898153305054, weight : 1024
loss : 0.6538122296333313, weight : 1024
loss : 0.7005670666694641, weight : 1024
loss : 0.6563953161239624, weight : 1024
loss : 0.6985791921615601, weight : 1024
loss : 0.7279523015022278, weight : 1024
loss : 0.6758986115455627, weight : 1024
loss : 0.675121009349823, weight : 1024
loss : 0.6009018421173096, weight : 1024
loss : 0.6466466188430786, weight : 1024
loss : 0.6583306789398193, weight : 1024
loss : 0.6822420954704285, weight : 1024
loss : 0.6430891156196594, weight : 1024
loss : 0.6631003618240356, weight : 1024
loss : 0.6756288409233093, weight : 1024
loss : 0.6177034974098206, weight : 553
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c
loss : 0.7975632548332214, weight : 2048
loss : 0.6955794095993042, weight : 2048
loss : 0.692084550857544, weight : 2048
loss : 0.7599164247512817, weight : 2048
loss : 0.7207927107810974, weight : 2048
loss : 0.7163019180297852, weight : 2048
loss : 0.7078109979629517, weight : 2048
loss : 0.7346848249435425, weight : 2048
loss : 0.7191717624664307, weight : 2048
loss : 0.7297357320785522, weight : 2048
loss : 0.7149707674980164, weight : 2048
loss : 0.732688307762146, weight : 2048
loss : 0.7088093161582947, weight : 1456
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c
loss : 0.6843705177307129, weight : 1024
loss : 0.5569108128547668, weight : 1024
loss : 0.6306315064430237, weight : 1024
loss : 0.6068326830863953, weight : 1024
loss : 0.5638521909713745, weight : 1024
loss : 0.6295108795166016, weight : 1024
loss : 0.5638352632522583, weight : 1024
loss : 0.5841754674911499, weight : 1024
loss : 0.5477089285850525, weight : 1024
loss : 0.6269370317459106, weight : 1024
loss : 0.6614798307418823, weight : 1024
loss : 0.6354206204414368, weight : 1024
loss : 0.5854160785675049, weight : 1024
loss : 0.5540278553962708, weight : 1024
loss : 0.6320546865463257, weight : 1024
loss : 0.590989887714386, weight : 1024
loss : 0.6167860627174377, weight : 1024
loss : 0.5981832146644592, weight : 1024
loss : 0.6029763221740723, weight : 1024
loss : 0.5201285481452942, weight : 1024
loss : 0.6382634043693542, weight : 1024
loss : 0.6320972442626953, weight : 1024
loss : 0.5558043122291565, weight : 1024
loss : 0.5799537897109985, weight : 1024
loss : 0.6502213478088379, weight : 1024
loss : 0.5595366358757019, weight : 1024
loss : 0.5849746465682983, weight : 1024
loss : 0.6117687821388245, weight : 1024
loss : 0.568320631980896, weight : 1024
loss : 0.5815733671188354, weight : 1024
loss : 0.6008715629577637, weight : 1024
loss : 0.6258318424224854, weight : 1024
loss : 0.5669270157814026, weight : 1024
loss : 0.5870412588119507, weight : 1024
loss : 0.5743781924247742, weight : 1024
loss : 0.5516061186790466, weight : 1024
loss : 0.58219975233078, weight : 1024
loss : 0.6493639945983887, weight : 1024
loss : 0.5526720285415649, weight : 1024
loss : 0.6125084161758423, weight : 1024
loss : 0.6012840867042542, weight : 1024
loss : 0.5451006889343262, weight : 1024
loss : 0.6147339344024658, weight : 1024
loss : 0.6402489542961121, weight : 1024
loss : 0.5672426223754883, weight : 1024
loss : 0.5484284162521362, weight : 1024
loss : 0.5326359272003174, weight : 1024
loss : 0.5489372611045837, weight : 1024
loss : 0.6194791197776794, weight : 1024
loss : 0.5482236742973328, weight : 1024
loss : 0.5862277746200562, weight : 1024
loss : 0.6231827735900879, weight : 1024
loss : 0.5570252537727356, weight : 1024
loss : 0.6244350075721741, weight : 1024
loss : 0.5422749519348145, weight : 1024
loss : 0.4367848038673401, weight : 1024
loss : 0.6601131558418274, weight : 1024
loss : 0.5058317184448242, weight : 1024
loss : 0.6108315587043762, weight : 1024
loss : 0.611691415309906, weight : 1024
loss : 0.49689844250679016, weight : 1024
loss : 0.5779708623886108, weight : 1024
loss : 0.5982049703598022, weight : 1024
loss : 0.5716090202331543, weight : 1024
loss : 0.5216257572174072, weight : 1024
loss : 0.5167094469070435, weight : 1024
loss : 0.5855164527893066, weight : 1024
loss : 0.6681312322616577, weight : 1024
loss : 0.6069802045822144, weight : 1024
loss : 0.5212547779083252, weight : 1024
loss : 0.5775220990180969, weight : 1024
loss : 0.5691319108009338, weight : 553
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c
loss : 0.7225995063781738, weight : 2048
loss : 0.6287037134170532, weight : 2048
loss : 0.6565004587173462, weight : 2048
loss : 0.685636579990387, weight : 2048
loss : 0.6428790092468262, weight : 2048
loss : 0.6547917127609253, weight : 2048
loss : 0.64454585313797, weight : 2048
loss : 0.6653342843055725, weight : 2048
loss : 0.6515735983848572, weight : 2048
loss : 0.6568635106086731, weight : 2048
loss : 0.6662516593933105, weight : 2048
loss : 0.6745580434799194, weight : 2048
loss : 0.640282928943634, weight : 1456
&gt; &lt;ipython-input-44-35d4a9117c1f&gt;(15)after_epoch()
     13     def after_epoch(self, learn):
     14         import pdb; pdb.set_trace()
---&gt; 15         log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
     16         log['epoch'] = learn.epoch
     17         log['train'] = 'train' if learn.model.training else 'eval'

ipdb&gt; c</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.502</td>
<td>1.477</td>
<td>0</td>
<td>train</td>
</tr>
<tr class="even">
<td>0.717</td>
<td>0.927</td>
<td>0</td>
<td>eval</td>
</tr>
<tr class="odd">
<td>0.776</td>
<td>0.730</td>
<td>1</td>
<td>train</td>
</tr>
<tr class="even">
<td>0.787</td>
<td>0.726</td>
<td>1</td>
<td>eval</td>
</tr>
<tr class="odd">
<td>0.823</td>
<td>0.586</td>
<td>2</td>
<td>train</td>
</tr>
<tr class="even">
<td>0.805</td>
<td>0.661</td>
<td>2</td>
<td>eval</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-60-output-4.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1698615011943,&quot;user_tz&quot;:0,&quot;elapsed&quot;:2280,&quot;user&quot;:{&quot;displayName&quot;:&quot;alex kelly&quot;,&quot;userId&quot;:&quot;14681700393893986055&quot;}}" data-outputid="e2a4e354-a784-4c81-fc7a-f02b436933aa">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>astats.dead_chart()<span class="op">;</span>astats.plot_stats()<span class="op">;</span>astats.color_dim()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-61-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="LSUV_for_the_blog_files/figure-html/cell-61-output-3.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>