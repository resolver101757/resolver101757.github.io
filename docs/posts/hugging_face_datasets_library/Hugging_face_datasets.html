<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Paul Kelly">
<meta name="dcterms.date" content="2024-02-06">

<title>Alex Paul Kelly - Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alex Paul Kelly</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/resolver101757" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/alex_paul_kelly" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexpkelly/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Deep Learning</div>
                <div class="quarto-category">Computer Vision</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Paul Kelly </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 6, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-is-hugging-face" id="toc-what-is-hugging-face" class="nav-link" data-scroll-target="#what-is-hugging-face">What is hugging face</a>
  <ul>
  <li><a href="#how-does-hugging-face-compare-to-the-competition" id="toc-how-does-hugging-face-compare-to-the-competition" class="nav-link" data-scroll-target="#how-does-hugging-face-compare-to-the-competition">How does Hugging Face compare to the competition</a></li>
  </ul></li>
  <li><a href="#hugging-faces-key-libaries" id="toc-hugging-faces-key-libaries" class="nav-link" data-scroll-target="#hugging-faces-key-libaries">Hugging faces key libaries</a>
  <ul>
  <li><a href="#hugging-python-client-library" id="toc-hugging-python-client-library" class="nav-link" data-scroll-target="#hugging-python-client-library">Hugging Python client library</a>
  <ul class="collapse">
  <li><a href="#hugging-face-transformers" id="toc-hugging-face-transformers" class="nav-link" data-scroll-target="#hugging-face-transformers">Hugging face Transformers</a></li>
  <li><a href="#hugging-face-diffuers" id="toc-hugging-face-diffuers" class="nav-link" data-scroll-target="#hugging-face-diffuers">Hugging Face Diffuers</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a>
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#why-use-hugging-face-datasets-library" id="toc-why-use-hugging-face-datasets-library" class="nav-link" data-scroll-target="#why-use-hugging-face-datasets-library">Why use Hugging Face datasets library</a></li>
  </ul></li>
  <li><a href="#how-to-transform-the-dataset-into-a-format-that-can-be-used-by-the-model" id="toc-how-to-transform-the-dataset-into-a-format-that-can-be-used-by-the-model" class="nav-link" data-scroll-target="#how-to-transform-the-dataset-into-a-format-that-can-be-used-by-the-model">How to transform the dataset into a format that can be used by the model</a></li>
  <li><a href="#creating-a-hugging-face-dataset-from-scratch" id="toc-creating-a-hugging-face-dataset-from-scratch" class="nav-link" data-scroll-target="#creating-a-hugging-face-dataset-from-scratch">Creating a hugging face dataset from scratch</a>
  <ul>
  <li><a href="#create-the-dataset" id="toc-create-the-dataset" class="nav-link" data-scroll-target="#create-the-dataset">Create the dataset</a></li>
  <li><a href="#transforming-the-dataset-into-tensors-ready-for-pytorch" id="toc-transforming-the-dataset-into-tensors-ready-for-pytorch" class="nav-link" data-scroll-target="#transforming-the-dataset-into-tensors-ready-for-pytorch">Transforming the dataset into tensors ready for pytorch</a>
  <ul class="collapse">
  <li><a href="#create-a-tensor-from-a-list-of-integers" id="toc-create-a-tensor-from-a-list-of-integers" class="nav-link" data-scroll-target="#create-a-tensor-from-a-list-of-integers">Create a tensor from a list of integers</a></li>
  </ul></li>
  <li><a href="#using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data" id="toc-using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data" class="nav-link" data-scroll-target="#using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data">Using Hugging Faces with_tranform with pytorch vision library to transform the data</a></li>
  <li><a href="#using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data" id="toc-using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data" class="nav-link" data-scroll-target="#using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data">using hugginfaces set_format and hugging faces own torch function to transform the data</a></li>
  </ul></li>
  <li><a href="#the-dataloader" id="toc-the-dataloader" class="nav-link" data-scroll-target="#the-dataloader">The dataloader</a></li>
  <li><a href="#passing-the-dataloader-to-the-pytorch-training-loop" id="toc-passing-the-dataloader-to-the-pytorch-training-loop" class="nav-link" data-scroll-target="#passing-the-dataloader-to-the-pytorch-training-loop">passing the dataloader to the pytorch training loop</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This blog aims to provide a brief overview of the creation of a machine learning workflow from creating a dataset up to the stage of training a model. I have started a project to develop a neural network eye-tracking model to replace the mouse and this is a series of blogs of the journey, see the <a href="posts\eye_tracking_start\eye_tracking.qmd">introduction to the project here</a>.</p>
<p>I will start the process by creating a dataset using the hugging faces dataset Python library. I will give a go over why it’s worth taking the time to learn what hugging face is and what it offers. We will then talk about how to transform the data into a format that can be used by PyToch and how to use the Pytorch dataloader to pass the data to the Pytorch training loop. Finally, we pass the data to a Pytorch training loop. The aim of this is not to create an optimised model but to show the pipeline up to the model.</p>
</section>
<section id="what-is-hugging-face" class="level1">
<h1>What is hugging face</h1>
<p><img src="C:\development\github%20projects\AlexPaulKelly\docs\posts\hugging_face_datasets_library\ac04bdd3-4808-4fd1-ad9c-58953c15684c.webp" class="img-fluid"></p>
<p>Hugging Face is a pivotal open-source AI hub, offering more than just a repository for models, datasets, and scripts. It’s a collaborative platform for AI professionals and enthusiasts, featuring tools like Gradio and comprehensive resources for machine learning. Hugging Face has 496,981 models, 104,714 datasets, and over 150k demo apps (Spaces).</p>
<p>They aim to make it easy for people to do anything machine learning and build a community around it. Hugging Face employees and users alike create blogs and tutorials, share papers and are hosted on the site. They have a discord and a forum that will help you with any questions you have and have open-source libraries to make it easy to get started in machine learning.</p>
<ul>
<li><strong>Models</strong> it has various ways for you to model, and download on your personal computer/laptop, serverless end point which is dynamically loaded on shared infrastructure, inference endpoint, integrations with other platforms like AWS, and Azure, and convert to a space.<br></li>
<li><strong>Spaces</strong> is a hosted area to place your models. This is a good way to create a portfolio of all your work and share it with the world free of cost. You can collaborate with other people using git and use front-end libraries like streamlit and gradio to make it easy to use.<br></li>
<li><strong>Libraries</strong> are a place to find all the open-source libraries that Hugging Face has created and has links to. They have a large collection of libraries that are used to make it easy to get started in machine learning. They have libraries for natural language processing, computer vision, and more.<br></li>
<li><strong>Datasets</strong> is a place to find datasets and share your own. You can also use the datasets to train your models. They have a large collection of datasets and you can also use the datasets to train your models. There’s more coverage of datasets in the next section as it’s the main focus of this notebook.</li>
</ul>
<section id="how-does-hugging-face-compare-to-the-competition" class="level2">
<h2 class="anchored" data-anchor-id="how-does-hugging-face-compare-to-the-competition">How does Hugging Face compare to the competition</h2>
<p>The Hugging Face dataset library does have some competition in Kaggle which is also a machine learning platform which Hosts Datasets, Notebooks and competitions and is more oriented to competitions and using notebooks on their platform. The hugging face dataset library is more oriented to datasets and has a large number of datasets that are ready to use and prebuilt piplines that you can use own hardware or another platform. The <a href="https://github.com/Kaggle/kaggle-api">kaggle python api can be found here</a></p>
<p>The dataset library also has some competition in pytorch and tensorflow. The pytorch dataset library can be <a href="https://pytorch.org/docs/stable/data.html">found here</a> and the tensorflow dataset library can be <a href="https://www.tensorflow.org/datasets/catalog/overview">found here</a> but are more geared at using their frameworks.</p>
</section>
</section>
<section id="hugging-faces-key-libaries" class="level1">
<h1>Hugging faces key libaries</h1>
<section id="hugging-python-client-library" class="level2">
<h2 class="anchored" data-anchor-id="hugging-python-client-library">Hugging Python client library</h2>
<p>Hugging Python client library allows you to manage all things hugging face hub and is aimed at individuals and teams collaborating on shared machine learning projects. You can create new repositories, download files from the hub, upload to the hub, search for models and run inference (run queries against models) and deploy models. See the <a href="https://huggingface.co/docs/huggingface_hub/quick-start">quick start guide here for more information</a></p>
<section id="hugging-face-transformers" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-transformers">Hugging face Transformers</h3>
<p>The library here is a wide-ranging library, originally intended for NLP tasks but has since expanded to computer vision, audio and multimodal. Its a high-level API that allows you to use pretrained models and fine-tune among other features. The list of supported models and frameworks can be found <a href="https://huggingface.co/docs/transformers/index">here</a>. The library compatable of jax, Pytorch and TensorFlow.</p>
<p>Some of the key features include: - <a href="https://huggingface.co/docs/transformers/main_classes/pipelines">pipelines is a high-level, easy-to-use, API for doing inference over a variety of downstream-tasks</a> - <a href="https://huggingface.co/docs/transformers/main_classes/trainer">Trainer is a high-level API for PyTorch that makes training a much simpler task</a> - <a href="https://huggingface.co/docs/transformers/main_classes/quantization">Quantization for reducing memory requirements and inference speed</a></p>
<ul>
<li>and many more</li>
</ul>
<p>### Gradio</p>
<p>Gradio is an open-source Python library that allows you to quickly create UIs for your machine-learning models. It allows you to create a UI for your model in 3 lines of code making it easy to showcase your work. It also allows you to share your model with others. It can be used locally and Hugging Face has a tight integration where you can host on Hugging Face for free. It has several features including:</p>
<ul>
<li><a href="https://gradio.app/getting_started">Create a UI for your model in 3 lines of code</a></li>
<li><a href="https://www.gradio.app/docs/chatinterface">one of the new features is the chat interface to help with the growth of all the language models</a></li>
<li><a href="https://gradio.app/getting_started">share your model with others</a></li>
</ul>
</section>
<section id="hugging-face-diffuers" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-diffuers">Hugging Face Diffuers</h3>
<p>This model is the go-to library for pre trained diffusion for generating for images, audio and 3d structures of modecules. It has high level pipeline api for creating inference with just a few lines of code. It has interchangeable noise schedulers for balancing speed and quality and pretrained models that can be used as a starting point for your own models. <a href="https://huggingface.co/docs/diffusers/index">“find more informatiohn here”</a>.</p>
<p>and finally, the last library we will talk about in more detail is the datasets library.</p>
</section>
</section>
</section>
<section id="datasets" class="level1">
<h1>Datasets</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>The purpose of the hugging face datasets is to make it easy to access and share and shape datasets. The library is the backbone of the hugging face hub and is used to organise, and transfer the datasets so they can be used within a machine learning pipeline. Nearly every deep learning workflow starts with a dataset so understanding the hugging face datasets library is important to aquire data training and fine-tuning models. Once you have a dataset, the next step is to pass this to a dataset loader, this could be in pytorch or Tensorflow or just use with one of the higher level APIs that hugging face provides (e.g.&nbsp;transformer python library) and you wont need to worry about the underlying architecture.</p>
<p>Hugging faces dataset library is built on top of Apache Arrow making it fast and efficient for data loading and supporting caching making it even more efficient. Arrow allows fast processing and is column-oriented, memory mapping and gives incredible performance gains. It includes features for processing and preparing data, like filtering, splitting, and shuffling.</p>
</section>
<section id="why-use-hugging-face-datasets-library" class="level2">
<h2 class="anchored" data-anchor-id="why-use-hugging-face-datasets-library">Why use Hugging Face datasets library</h2>
<p>In simple terms, the Hugging Face dataset library gives you everything you need to use an existing dataset or create datasets and get straight into the machine learning pipeline. <a href="https://huggingface.co/docs/datasets/index">find more information here</a>. Its platform agnostic and can be used with any framework, for example, you can pass to Pytorch or Tensorflow. It has a large number of datasets that are ready to use and can be used with the Transformers library. It’s well-documented and has a large community that can help you with any questions you have. Get started with just 3 lines of code, load a dataset and start exploring in your notebook or script. The code below will show the beans dataset, you can also view the dataset in the hugging face hub <a href="https://huggingface.co/datasets/beans">here</a>.</p>
<div class="cell" data-execution_count="201">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install and import the necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install datasets[vision] </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, Image </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Downloads the dataset called beans</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"beans"</span>, split<span class="op">=</span><span class="st">"train"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="how-to-transform-the-dataset-into-a-format-that-can-be-used-by-the-model" class="level1">
<h1>How to transform the dataset into a format that can be used by the model</h1>
<p>I found the transform part of the dataset library the most confusing part of the library. There are lots of ways to transform the data using the hugging face dataset library but also incorporating other libraries like Pytorches transformations or tensor flow. Understanding the different ways to transform and which one is the best is an important skill to master. Here are some of the ways you can use the hugging face library to transform data :</p>
<ul>
<li><a href="https://huggingface.co/docs/datasets/process">Reordering Rows and Splitting the Dataset</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/process">Renaming and Removing Columns</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/process">Applying Processing Functions to Each Example</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/process">Concatenating Datasets</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/quickstart">Applying a Custom Formatting Transform</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/use_dataset">Applying Transforms to Images</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/use_dataset">Data Augmentation</a><br></li>
<li><a href="https://huggingface.co/docs/datasets/v2.16.1/process">Exporting a Dataset</a><br></li>
</ul>
<p>We’ve already imported dataset using the hugging face library. The next step is to pass this over to pytorch so we can train our model but we could pass this over to tensorflow if required. Before we pass it over we need to transform the data to tensors instead of PIL.jpeg format as shown below.</p>
<div class="cell" data-execution_count="202">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"image"</span>][:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="202">
<pre><code>[&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;,
 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;,
 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;,
 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;,
 &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;]</code></pre>
</div>
</div>
<p>This requires importing transforms from the torchvision library from pytorch, this will do the transformations. Create a function to convert jpg into Tensors and then pass this function to datasets object using the with_transform function. This will then format the data into a format that can be used by pytorch. The code below and further comments will show how to do this along with the results of the transformation and the data types produced.</p>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates a ToTensor object that converts the image to a tensor</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>to_tensor <span class="op">=</span> transforms.ToTensor()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="213">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates a function that takes in a batch and returns the batch a tensors (previously images were in PIL format)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transform_images(batch):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'image'</span>] <span class="op">=</span> [to_tensor(image) <span class="cf">for</span> image <span class="kw">in</span> batch[<span class="st">'image'</span>]]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="214">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Executes what we have set above the transform on the dataset, the returning dataset[image] will be a tensor  </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.with_transform(transform_images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="215">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can access the first image as a tensor</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="dv">0</span>][<span class="st">"image"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="215">
<pre><code>tensor([[[0.2196, 0.2196, 0.2196,  ..., 0.8078, 0.2627, 0.2118],
         [0.2118, 0.2157, 0.2157,  ..., 0.6706, 0.3843, 0.2000],
         [0.1961, 0.2000, 0.2039,  ..., 0.6706, 0.3961, 0.3294],
         ...,
         [0.2196, 0.1882, 0.2275,  ..., 0.5804, 0.6353, 0.5529],
         [0.1961, 0.1922, 0.2706,  ..., 0.6196, 0.5843, 0.6157],
         [0.2275, 0.2118, 0.1882,  ..., 0.5647, 0.6196, 0.6000]],

        [[0.1490, 0.1490, 0.1490,  ..., 0.6039, 0.0667, 0.0196],
         [0.1412, 0.1451, 0.1451,  ..., 0.4980, 0.2078, 0.0314],
         [0.1333, 0.1373, 0.1412,  ..., 0.5216, 0.2510, 0.1882],
         ...,
         [0.1020, 0.0706, 0.1098,  ..., 0.7020, 0.7569, 0.6745],
         [0.0784, 0.0745, 0.1529,  ..., 0.7255, 0.6824, 0.7137],
         [0.1098, 0.0941, 0.0706,  ..., 0.6471, 0.7020, 0.6784]],

        [[0.0078, 0.0078, 0.0078,  ..., 0.5490, 0.0314, 0.0039],
         [0.0000, 0.0039, 0.0039,  ..., 0.3843, 0.1255, 0.0000],
         [0.0000, 0.0039, 0.0078,  ..., 0.3804, 0.1373, 0.1020],
         ...,
         [0.0706, 0.0392, 0.0706,  ..., 0.4039, 0.4588, 0.3686],
         [0.0471, 0.0431, 0.1137,  ..., 0.4627, 0.4235, 0.4549],
         [0.0784, 0.0627, 0.0314,  ..., 0.4157, 0.4706, 0.4471]]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="216">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prints the labels of the first image, notice the value isnt a torch tensor</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="dv">0</span>][<span class="st">"labels"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="216">
<pre><code>0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="218">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># its a python int type</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(dataset[<span class="dv">0</span>][<span class="st">"labels"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="218">
<pre><code>int</code></pre>
</div>
</div>
<div class="cell" data-execution_count="221">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sets the output of the dataset to be a torch tensor</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>dataset.set_format(<span class="bu">type</span><span class="op">=</span><span class="st">'torch'</span>, columns<span class="op">=</span>[<span class="st">'image'</span>, <span class="st">'labels'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="222">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can access the first element of the dataset as a tensor</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="dv">1</span>][<span class="st">"labels"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="222">
<pre><code>tensor(0)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="224">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The datatype is a torch tensor type</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="dv">1</span>][<span class="st">"labels"</span>].<span class="bu">type</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="224">
<pre><code>'torch.LongTensor'</code></pre>
</div>
</div>
</section>
<section id="creating-a-hugging-face-dataset-from-scratch" class="level1">
<h1>Creating a hugging face dataset from scratch</h1>
<section id="create-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="create-the-dataset">Create the dataset</h2>
<p>For this I will be using a dataset ive created myself for mapping pictures of me looking at the screen and co-ordinates on screen. The data was collected by writing a program that placed “x” on the screen at random coordinates. The program then recorded the coordinates of the “x” and a picture of the user’s face. The program then saved the image and named the file as the coordinates of the “x”. The process was repeated until the model was able to predict the coordinates of the “x” with a high degree of accuracy.</p>
<div class="cell" data-execution_count="225">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset, Image</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset location on my drive </span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>data_science_folder <span class="op">=</span> <span class="st">'G:\My Drive\Learning\data_science'</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>sys.path.append(data_science_folder)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> data_science_folder <span class="op">+</span> <span class="st">"\datasets_folder\gaze-points\work-laptop"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A typical file name looks like the :</p>
<ul>
<li>20240123-140252-hieght2560-width1440-computerwork-laptop_2232_230.png</li>
</ul>
<p>with the targets (the pixel im looking at on screen) in the file name and the file contents is a image of me looking at the screen. The last 2 numbers 2232 and 230 are the pixel co-ordinates that need to be stripped out of the file name. Below, ill detail how to strip out the co-ordinates.</p>
<div class="cell" data-execution_count="227">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extracts screen coordinates from the filenames and stores in a list of tensors</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>label_tensors <span class="op">=</span> [torch.tensor([<span class="bu">int</span>(f.split(<span class="st">'_'</span>)[<span class="op">-</span><span class="dv">2</span>]), <span class="bu">int</span>(f.split(<span class="st">'_'</span>)[<span class="op">-</span><span class="dv">1</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>])]) <span class="cf">for</span> f <span class="kw">in</span> os.listdir(dataset_path) <span class="cf">if</span> os.path.isfile(os.path.join(dataset_path, f))]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="228">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the last 20 elements for testings purposes</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>label_tensors <span class="op">=</span> label_tensors[:<span class="dv">20</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have imported the dataset using the hugging face library. The next step is to get a list of the full file names to pass to the Dataset object to be load the images.</p>
<div class="cell" data-execution_count="229">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gets a list of all images in a directory and stores in a list of strings</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>image_files <span class="op">=</span> [os.path.join(dataset_path, f) <span class="cf">for</span> f <span class="kw">in</span> os.listdir(dataset_path) <span class="cf">if</span> os.path.isfile(os.path.join(dataset_path, f))]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="230">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>image_files <span class="op">=</span> image_files[:<span class="dv">20</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="231">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm that the length of the labels and images are the same so they can be paired together during the creation of the dataset</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>len_image_files <span class="op">=</span> <span class="bu">len</span>(image_files)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>len_labels <span class="op">=</span> <span class="bu">len</span>(label_tensors)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"confirm length of labels </span><span class="sc">{</span>len_labels<span class="sc">}</span><span class="ss"> and length of image_files </span><span class="sc">{</span>len_image_files<span class="sc">}</span><span class="ss"> are the same"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>confirm length of labels 20 and length of image_files 20 are the same</code></pre>
</div>
</div>
<p>Load the images and cast (use the pil library to convert the images)</p>
<div class="cell" data-execution_count="232">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dataset from the image files and labels</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Dataset.from_dict({<span class="st">"image"</span>: image_files}).cast_column(<span class="st">"image"</span>, Image())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="233">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new dictionary with the images and labels</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># i'm not happy with having to add the labels to the dataset after as it takes alot longer</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># but i'm not sure how to do it in the the from_dict method above.</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>updated_dataset_dict <span class="op">=</span> {<span class="st">"image"</span>: dataset[<span class="st">"image"</span>], <span class="st">"label"</span>: label_tensors}</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>updated_dataset <span class="op">=</span> Dataset.from_dict(updated_dataset_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="234">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>updated_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="234">
<pre><code>Dataset({
    features: ['image', 'label'],
    num_rows: 20
})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="235">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>updated_dataset[<span class="st">"image"</span>][<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="235">
<p><img src="Hugging_face_datasets_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="236">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>updated_dataset[<span class="st">"label"</span>][<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="236">
<pre><code>[526, 1015]</code></pre>
</div>
</div>
</section>
<section id="transforming-the-dataset-into-tensors-ready-for-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="transforming-the-dataset-into-tensors-ready-for-pytorch">Transforming the dataset into tensors ready for pytorch</h2>
<p>We will need to transform the data to tensors instead of PIL.jpeg format and the labels will need to be tensors. You can see from viewing the first element in the cell above that its a list of 2 integers. The image above is a picture of me looking at the screen. We now need to convert the labels and images to tensors using pytorches vision library. We can then pass this to the dataset object using the with_transform function.</p>
<p>There a couple of ways to do this in hugging face datasets library. The first is to use the map function and the second is to use the with_transform function. The map function is applies straight away but consumes a lot of memory and the with_transform function is applied when the data is loaded (or requested from the dataset object). The with_transform function is the best option for large datasets. The with_transform method is shown below.</p>
<section id="create-a-tensor-from-a-list-of-integers" class="level3">
<h3 class="anchored" data-anchor-id="create-a-tensor-from-a-list-of-integers">Create a tensor from a list of integers</h3>
<p>A detailed pytorch tensor is out of the scope of this notebook but you can find more information <a href="https://pytorch.org/docs/stable/tensors.html">here</a>. In simple terms, a tensor is a multi-dimensional array that can be used to store and process data. Below is a simple example of how to create a tensor from a list of integers and show the tensor datatype and the tensor itself.</p>
<div class="cell" data-execution_count="237">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example of how to use the torch stack function</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The following will take a list of inter </span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>list_of_ints_1 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>list_of_ints_2 <span class="op">=</span> [<span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>list_of_ints_3 <span class="op">=</span> [<span class="dv">11</span>, <span class="dv">12</span>, <span class="dv">13</span>, <span class="dv">14</span>, <span class="dv">15</span>]</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>pytorch_stacked <span class="op">=</span> torch.stack([torch.tensor(list_of_ints_1), torch.tensor(list_of_ints_2), torch.tensor(list_of_ints_3)], dim<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the resulting tensor will be a 3x5 tensor</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>pytorch_stacked</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="238">
<pre><code>tensor([[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10],
        [11, 12, 13, 14, 15]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="239">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pick out the first element of the first row</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>pytorch_stacked[<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="239">
<pre><code>tensor(1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="240">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># shows the stored datatype of the tensor</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>pytorch_stacked[<span class="dv">0</span>][<span class="dv">0</span>].<span class="bu">type</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="240">
<pre><code>'torch.LongTensor'</code></pre>
</div>
</div>
</section>
</section>
<section id="using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data" class="level2">
<h2 class="anchored" data-anchor-id="using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data">Using Hugging Faces with_tranform with pytorch vision library to transform the data</h2>
<p>The Hugging Faces with_transform is applied on-the-fly on batches of data when iterating over the dataset. The with_transform function is the best option for large datasets.</p>
<p>The PyTorch vision library is comprehensive and consists of popular datasets, model architectures, and common image transformations for computer vision. Tensor images are expected to be of shape (C, H, W). The torchvision ToTensor() transform the pil/jpg into shape (C, H, W) with the tensor type as FloatTensor.</p>
<p>The code below shows how to transform the dataset required to pass the data to the dataloader using Hugging Faces with_transformation function and the pytorch vision library. It will also shows the results of the transformation along with comments.</p>
<div class="cell" data-execution_count="241">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates a transform that converts the image to a tensor</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>to_tensor <span class="op">=</span> transforms.ToTensor()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="352">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transform_images_with_stack(batch):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"image"</span> <span class="kw">in</span> batch:</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert all images in the batch to tensors and collect them in a list</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        images_tensor <span class="op">=</span> torch.stack([to_tensor(image) <span class="cf">for</span> image <span class="kw">in</span> batch[<span class="st">'image'</span>]])</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">'image'</span>] <span class="op">=</span> images_tensor  <span class="co"># Replace the list of images with a stacked tensor</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"label"</span> <span class="kw">in</span> batch:</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert all labels in the batch to tensors and collect them in a list</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        labels_tensor <span class="op">=</span> torch.stack([torch.tensor(label) <span class="cf">for</span> label <span class="kw">in</span> batch[<span class="st">'label'</span>]])</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">'label'</span>] <span class="op">=</span> labels_tensor  <span class="co"># Replace the list of labels with a stacked tensor</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="353">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Executes the transform on the dataset, the returning dataset[image] will be a tensor  </span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform <span class="op">=</span> updated_dataset.with_transform(transform_images_with_stack)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="354">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the image now in tensor format</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="dv">0</span>][<span class="st">"image"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="354">
<pre><code>tensor([[[0.8157, 0.8157, 0.8157,  ..., 0.8196, 0.8196, 0.8196],
         [0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8235, 0.8235],
         [0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8196, 0.8235],
         ...,
         [0.8078, 0.8196, 0.8314,  ..., 0.5922, 0.5961, 0.6000],
         [0.8000, 0.8118, 0.8314,  ..., 0.5882, 0.5961, 0.5961],
         [0.8196, 0.8314, 0.8431,  ..., 0.5882, 0.5922, 0.5961]],

        [[0.7647, 0.7647, 0.7647,  ..., 0.7608, 0.7608, 0.7608],
         [0.7686, 0.7686, 0.7686,  ..., 0.7608, 0.7647, 0.7647],
         [0.7686, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7725],
         ...,
         [0.8314, 0.8431, 0.8588,  ..., 0.5098, 0.5098, 0.5137],
         [0.8235, 0.8392, 0.8588,  ..., 0.5020, 0.5098, 0.5098],
         [0.8431, 0.8588, 0.8706,  ..., 0.5020, 0.5059, 0.5098]],

        [[0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7412, 0.7412],
         [0.7373, 0.7373, 0.7373,  ..., 0.7412, 0.7451, 0.7451],
         [0.7373, 0.7373, 0.7373,  ..., 0.7451, 0.7451, 0.7490],
         ...,
         [0.8627, 0.8745, 0.8941,  ..., 0.4431, 0.4510, 0.4549],
         [0.8549, 0.8706, 0.8941,  ..., 0.4471, 0.4549, 0.4588],
         [0.8745, 0.8902, 0.9059,  ..., 0.4471, 0.4549, 0.4627]]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="355">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># notice that the shape is channel first, height, and then width</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="dv">0</span>][<span class="st">"image"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="355">
<pre><code>torch.Size([3, 480, 640])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="356">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the value in the first channel, top row, and first column so top left pixel of the image</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="dv">0</span>][<span class="st">"image"</span>][<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="356">
<pre><code>tensor(0.8157)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="357">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># shows the stored datatype of the tensor</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the value in the first channel, top row, and first column so top left pixel of the image</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="dv">0</span>][<span class="st">"image"</span>][<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>].<span class="bu">type</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="357">
<pre><code>'torch.FloatTensor'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="358">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints all the labels in the dataset</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="st">"label"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="358">
<pre><code>torch.Size([20, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="359">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints  X, Y coordinates of the first label in the dataset</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_with_transform[<span class="dv">0</span>][<span class="st">"label"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="359">
<pre><code>tensor([1073,    4])</code></pre>
</div>
</div>
</section>
<section id="using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data" class="level2">
<h2 class="anchored" data-anchor-id="using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data">using hugginfaces set_format and hugging faces own torch function to transform the data</h2>
<p>Hugging faces set_format is a applied is also applied on-the-fly on batches of data when iterating over the dataset. The set_format function is the best option for large datasets.</p>
<p>The torch set_format torches converts them into a pytorch format, however the torch function is not as comprehensive as the torchvision library. It converts to a tensor but to a different shape (H, W, C) and THE FORMAT IS uint8 instead of a FloatTensor.</p>
<div class="cell" data-execution_count="385">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create copy of the dataset for the next example</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format <span class="op">=</span> updated_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="386">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the set_format method to the dataset using hugging faces torch format</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format.set_format(<span class="bu">type</span><span class="op">=</span><span class="st">'torch'</span>, columns<span class="op">=</span>[<span class="st">'image'</span>,<span class="st">'label'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="387">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># returns no of images, height, width, and channels all in a tensor</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="st">"image"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="387">
<pre><code>torch.Size([20, 480, 640, 3])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="388">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Notice the shape is different to before, height, width, channel </span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="dv">0</span>][<span class="st">"image"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="388">
<pre><code>torch.Size([480, 640, 3])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="389">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the image now in tensor format</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="dv">0</span>][<span class="st">"image"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="389">
<pre><code>tensor([[[208, 195, 187],
         [208, 195, 187],
         [208, 195, 187],
         ...,
         [209, 194, 189],
         [209, 194, 189],
         [209, 194, 189]],

        [[209, 196, 188],
         [209, 196, 188],
         [209, 196, 188],
         ...,
         [209, 194, 189],
         [210, 195, 190],
         [210, 195, 190]],

        [[209, 196, 188],
         [209, 196, 188],
         [209, 196, 188],
         ...,
         [209, 196, 190],
         [209, 196, 190],
         [210, 197, 191]],

        ...,

        [[206, 212, 220],
         [209, 215, 223],
         [212, 219, 228],
         ...,
         [151, 130, 113],
         [152, 130, 115],
         [153, 131, 116]],

        [[204, 210, 218],
         [207, 214, 222],
         [212, 219, 228],
         ...,
         [150, 128, 114],
         [152, 130, 116],
         [152, 130, 117]],

        [[209, 215, 223],
         [212, 219, 227],
         [215, 222, 231],
         ...,
         [150, 128, 114],
         [151, 129, 116],
         [152, 130, 118]]], dtype=torch.uint8)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="393">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the value in the chanels </span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="dv">0</span>][<span class="st">"image"</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="393">
<pre><code>tensor([208, 195, 187], dtype=torch.uint8)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="390">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints number of labels,  the x and y coordinates </span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="st">"label"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="390">
<pre><code>torch.Size([20, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="391">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the first element of the label tensor</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_set_format[<span class="st">"label"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="391">
<pre><code>tensor([1073,    4])</code></pre>
</div>
</div>
</section>
</section>
<section id="the-dataloader" class="level1">
<h1>The dataloader</h1>
<p>The DataLoader in PyTorch allows for efficient and customizable data iteration for training models. It supports:</p>
<ul>
<li>Map-style datasets (access data by index) and iterable-style datasets (sequential access).</li>
<li>Order customization for data loading.</li>
<li>Automatic batching to process multiple data items simultaneously.</li>
<li>Parallel data loading using single or multiple processes to speed up preparation.</li>
<li>Memory pinning for faster GPU transfers.</li>
</ul>
<p>Essentially, it streamlines feeding data into models, whether you’re adjusting load order, batching for efficiency, or accelerating GPU data transfer.</p>
<p>We’ll be passing the data from the Hugging faces dataset library and into the pytorches dataset library. The data will need splitting. The code below and comments will show how to do this and the results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates a DataLoader object that can be used to iterate through the dataset</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="368">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># splits the dataset into a training and test set</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the test set is 20% of the dataset</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the training set is 80% of the dataset</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>updated_dataset_split <span class="op">=</span> updated_dataset_with_transform.train_test_split(test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="369">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># craete a dataset object from the training set</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>updated_dataset_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="369">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'label'],
        num_rows: 16
    })
    test: Dataset({
        features: ['image', 'label'],
        num_rows: 4
    })
})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="370">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sets the batch size for the data loader</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="371">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into train and test (if not already split)</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> updated_dataset_split[<span class="st">'train'</span>]</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a PyTorch DataLoader for the train dataset</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="372">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> updated_dataset_split[<span class="st">'test'</span>]</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Similarly for the test dataset (optional)</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="373">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the first element of the first train_loader batch</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> train_loader:</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item[<span class="st">'image'</span>].shape)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item[<span class="st">'label'</span>].shape)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'image': tensor([[[[0.8157, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8039],
          [0.8196, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8078],
          [0.8196, 0.8196, 0.8196,  ..., 0.8118, 0.8118, 0.8078],
          ...,
          [0.7882, 0.8000, 0.8078,  ..., 0.5961, 0.6000, 0.6039],
          [0.7765, 0.7882, 0.7961,  ..., 0.6000, 0.6000, 0.6000],
          [0.8039, 0.8078, 0.8078,  ..., 0.6000, 0.6000, 0.6000]],

         [[0.7569, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7608],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7686, 0.7686, 0.7647],
          ...,
          [0.8353, 0.8471, 0.8627,  ..., 0.4902, 0.4941, 0.4980],
          [0.8314, 0.8431, 0.8510,  ..., 0.4902, 0.4941, 0.4980],
          [0.8588, 0.8627, 0.8627,  ..., 0.4902, 0.4941, 0.5020]],

         [[0.7294, 0.7373, 0.7373,  ..., 0.7412, 0.7412, 0.7373],
          [0.7333, 0.7373, 0.7373,  ..., 0.7412, 0.7412, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          ...,
          [0.8863, 0.8980, 0.9098,  ..., 0.4471, 0.4510, 0.4549],
          [0.8824, 0.8941, 0.9020,  ..., 0.4471, 0.4510, 0.4510],
          [0.9098, 0.9137, 0.9137,  ..., 0.4471, 0.4510, 0.4549]]],


        [[[0.8196, 0.8196, 0.8157,  ..., 0.8000, 0.8000, 0.8039],
          [0.8196, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8078],
          [0.8235, 0.8196, 0.8196,  ..., 0.8118, 0.8118, 0.8118],
          ...,
          [0.7961, 0.8039, 0.8078,  ..., 0.5922, 0.5922, 0.5961],
          [0.7882, 0.7922, 0.7922,  ..., 0.5882, 0.5922, 0.5961],
          [0.8078, 0.8078, 0.7961,  ..., 0.5961, 0.6039, 0.6039]],

         [[0.7608, 0.7608, 0.7569,  ..., 0.7608, 0.7608, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7608, 0.7608, 0.7608],
          [0.7647, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7647],
          ...,
          [0.8510, 0.8588, 0.8667,  ..., 0.4941, 0.4941, 0.4980],
          [0.8431, 0.8471, 0.8510,  ..., 0.4902, 0.4941, 0.4980],
          [0.8627, 0.8627, 0.8549,  ..., 0.4902, 0.4980, 0.4980]],

         [[0.7333, 0.7333, 0.7294,  ..., 0.7333, 0.7333, 0.7373],
          [0.7333, 0.7333, 0.7333,  ..., 0.7373, 0.7373, 0.7373],
          [0.7373, 0.7333, 0.7333,  ..., 0.7412, 0.7412, 0.7412],
          ...,
          [0.8627, 0.8706, 0.8863,  ..., 0.4510, 0.4471, 0.4510],
          [0.8549, 0.8588, 0.8706,  ..., 0.4471, 0.4471, 0.4510],
          [0.8745, 0.8745, 0.8745,  ..., 0.4510, 0.4549, 0.4549]]],


        [[[0.8039, 0.8039, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          [0.8039, 0.8078, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          [0.8078, 0.8078, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          ...,
          [0.7961, 0.8157, 0.8314,  ..., 0.5961, 0.6000, 0.6000],
          [0.7882, 0.8118, 0.8235,  ..., 0.5922, 0.5961, 0.5961],
          [0.8000, 0.8157, 0.8275,  ..., 0.5922, 0.5922, 0.5922]],

         [[0.7725, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7647],
          [0.7686, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7647],
          [0.7686, 0.7686, 0.7686,  ..., 0.7725, 0.7725, 0.7686],
          ...,
          [0.8431, 0.8627, 0.8784,  ..., 0.5020, 0.5059, 0.5059],
          [0.8431, 0.8627, 0.8745,  ..., 0.4980, 0.5020, 0.5020],
          [0.8549, 0.8706, 0.8824,  ..., 0.4980, 0.4980, 0.4980]],

         [[0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          ...,
          [0.8510, 0.8745, 0.8980,  ..., 0.4549, 0.4588, 0.4588],
          [0.8471, 0.8706, 0.8941,  ..., 0.4510, 0.4549, 0.4549],
          [0.8588, 0.8784, 0.8980,  ..., 0.4510, 0.4510, 0.4510]]],


        [[[0.8157, 0.8157, 0.8196,  ..., 0.8118, 0.8157, 0.8157],
          [0.8196, 0.8196, 0.8196,  ..., 0.8157, 0.8196, 0.8196],
          [0.8196, 0.8196, 0.8196,  ..., 0.8157, 0.8196, 0.8196],
          ...,
          [0.7961, 0.8039, 0.8235,  ..., 0.5922, 0.5961, 0.6000],
          [0.7922, 0.8039, 0.8235,  ..., 0.5922, 0.5961, 0.6000],
          [0.8118, 0.8275, 0.8392,  ..., 0.5922, 0.5961, 0.5961]],

         [[0.7569, 0.7569, 0.7608,  ..., 0.7608, 0.7647, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7686, 0.7686],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7686, 0.7686],
          ...,
          [0.8392, 0.8471, 0.8667,  ..., 0.5020, 0.5059, 0.5098],
          [0.8353, 0.8471, 0.8667,  ..., 0.5020, 0.5059, 0.5098],
          [0.8549, 0.8706, 0.8824,  ..., 0.5020, 0.5059, 0.5059]],

         [[0.7294, 0.7294, 0.7333,  ..., 0.7373, 0.7412, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7451, 0.7451],
          [0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7451, 0.7451],
          ...,
          [0.8549, 0.8627, 0.8863,  ..., 0.4431, 0.4431, 0.4471],
          [0.8510, 0.8627, 0.8863,  ..., 0.4431, 0.4431, 0.4471],
          [0.8706, 0.8863, 0.9020,  ..., 0.4431, 0.4431, 0.4431]]],


        [[[0.8039, 0.8039, 0.8118,  ..., 0.8275, 0.8314, 0.8314],
          [0.8078, 0.8118, 0.8118,  ..., 0.8235, 0.8275, 0.8275],
          [0.8118, 0.8118, 0.8157,  ..., 0.8235, 0.8275, 0.8275],
          ...,
          [0.7961, 0.8118, 0.8235,  ..., 0.6039, 0.6039, 0.6039],
          [0.8000, 0.8157, 0.8275,  ..., 0.6039, 0.6039, 0.6000],
          [0.8039, 0.8196, 0.8275,  ..., 0.6000, 0.6000, 0.6000]],

         [[0.7647, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7608],
          [0.7608, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7608],
          [0.7647, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7647],
          ...,
          [0.8431, 0.8588, 0.8745,  ..., 0.4941, 0.4941, 0.4941],
          [0.8471, 0.8627, 0.8745,  ..., 0.4941, 0.4941, 0.4902],
          [0.8471, 0.8627, 0.8745,  ..., 0.4902, 0.4902, 0.4902]],

         [[0.7373, 0.7373, 0.7373,  ..., 0.7451, 0.7490, 0.7451],
          [0.7373, 0.7412, 0.7373,  ..., 0.7451, 0.7451, 0.7451],
          [0.7412, 0.7412, 0.7373,  ..., 0.7451, 0.7451, 0.7451],
          ...,
          [0.8588, 0.8745, 0.8941,  ..., 0.4510, 0.4510, 0.4510],
          [0.8627, 0.8784, 0.8941,  ..., 0.4471, 0.4510, 0.4510],
          [0.8627, 0.8784, 0.8941,  ..., 0.4431, 0.4471, 0.4510]]]]), 'label': tensor([[1838, 1249],
        [1908,  769],
        [ 949,   77],
        [2263,  920],
        [ 865,  306]])}
torch.Size([5, 3, 480, 640])
torch.Size([5, 2])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the lavels for the first batch </span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>item[<span class="st">'label'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="374">
<pre><code>tensor([[1838, 1249],
        [1908,  769],
        [ 949,   77],
        [2263,  920],
        [ 865,  306]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="375">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints the first image in the first batch </span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="co"># most will be missing off screen as there's alot of data </span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>item[<span class="st">"image"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="375">
<pre><code>tensor([[[[0.8157, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8039],
          [0.8196, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8078],
          [0.8196, 0.8196, 0.8196,  ..., 0.8118, 0.8118, 0.8078],
          ...,
          [0.7882, 0.8000, 0.8078,  ..., 0.5961, 0.6000, 0.6039],
          [0.7765, 0.7882, 0.7961,  ..., 0.6000, 0.6000, 0.6000],
          [0.8039, 0.8078, 0.8078,  ..., 0.6000, 0.6000, 0.6000]],

         [[0.7569, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7608],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7686, 0.7686, 0.7647],
          ...,
          [0.8353, 0.8471, 0.8627,  ..., 0.4902, 0.4941, 0.4980],
          [0.8314, 0.8431, 0.8510,  ..., 0.4902, 0.4941, 0.4980],
          [0.8588, 0.8627, 0.8627,  ..., 0.4902, 0.4941, 0.5020]],

         [[0.7294, 0.7373, 0.7373,  ..., 0.7412, 0.7412, 0.7373],
          [0.7333, 0.7373, 0.7373,  ..., 0.7412, 0.7412, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          ...,
          [0.8863, 0.8980, 0.9098,  ..., 0.4471, 0.4510, 0.4549],
          [0.8824, 0.8941, 0.9020,  ..., 0.4471, 0.4510, 0.4510],
          [0.9098, 0.9137, 0.9137,  ..., 0.4471, 0.4510, 0.4549]]],


        [[[0.8196, 0.8196, 0.8157,  ..., 0.8000, 0.8000, 0.8039],
          [0.8196, 0.8196, 0.8196,  ..., 0.8078, 0.8078, 0.8078],
          [0.8235, 0.8196, 0.8196,  ..., 0.8118, 0.8118, 0.8118],
          ...,
          [0.7961, 0.8039, 0.8078,  ..., 0.5922, 0.5922, 0.5961],
          [0.7882, 0.7922, 0.7922,  ..., 0.5882, 0.5922, 0.5961],
          [0.8078, 0.8078, 0.7961,  ..., 0.5961, 0.6039, 0.6039]],

         [[0.7608, 0.7608, 0.7569,  ..., 0.7608, 0.7608, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7608, 0.7608, 0.7608],
          [0.7647, 0.7608, 0.7608,  ..., 0.7647, 0.7647, 0.7647],
          ...,
          [0.8510, 0.8588, 0.8667,  ..., 0.4941, 0.4941, 0.4980],
          [0.8431, 0.8471, 0.8510,  ..., 0.4902, 0.4941, 0.4980],
          [0.8627, 0.8627, 0.8549,  ..., 0.4902, 0.4980, 0.4980]],

         [[0.7333, 0.7333, 0.7294,  ..., 0.7333, 0.7333, 0.7373],
          [0.7333, 0.7333, 0.7333,  ..., 0.7373, 0.7373, 0.7373],
          [0.7373, 0.7333, 0.7333,  ..., 0.7412, 0.7412, 0.7412],
          ...,
          [0.8627, 0.8706, 0.8863,  ..., 0.4510, 0.4471, 0.4510],
          [0.8549, 0.8588, 0.8706,  ..., 0.4471, 0.4471, 0.4510],
          [0.8745, 0.8745, 0.8745,  ..., 0.4510, 0.4549, 0.4549]]],


        [[[0.8039, 0.8039, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          [0.8039, 0.8078, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          [0.8078, 0.8078, 0.8078,  ..., 0.8118, 0.8118, 0.8078],
          ...,
          [0.7961, 0.8157, 0.8314,  ..., 0.5961, 0.6000, 0.6000],
          [0.7882, 0.8118, 0.8235,  ..., 0.5922, 0.5961, 0.5961],
          [0.8000, 0.8157, 0.8275,  ..., 0.5922, 0.5922, 0.5922]],

         [[0.7725, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7647],
          [0.7686, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7647],
          [0.7686, 0.7686, 0.7686,  ..., 0.7725, 0.7725, 0.7686],
          ...,
          [0.8431, 0.8627, 0.8784,  ..., 0.5020, 0.5059, 0.5059],
          [0.8431, 0.8627, 0.8745,  ..., 0.4980, 0.5020, 0.5020],
          [0.8549, 0.8706, 0.8824,  ..., 0.4980, 0.4980, 0.4980]],

         [[0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7451, 0.7451, 0.7412],
          ...,
          [0.8510, 0.8745, 0.8980,  ..., 0.4549, 0.4588, 0.4588],
          [0.8471, 0.8706, 0.8941,  ..., 0.4510, 0.4549, 0.4549],
          [0.8588, 0.8784, 0.8980,  ..., 0.4510, 0.4510, 0.4510]]],


        [[[0.8157, 0.8157, 0.8196,  ..., 0.8118, 0.8157, 0.8157],
          [0.8196, 0.8196, 0.8196,  ..., 0.8157, 0.8196, 0.8196],
          [0.8196, 0.8196, 0.8196,  ..., 0.8157, 0.8196, 0.8196],
          ...,
          [0.7961, 0.8039, 0.8235,  ..., 0.5922, 0.5961, 0.6000],
          [0.7922, 0.8039, 0.8235,  ..., 0.5922, 0.5961, 0.6000],
          [0.8118, 0.8275, 0.8392,  ..., 0.5922, 0.5961, 0.5961]],

         [[0.7569, 0.7569, 0.7608,  ..., 0.7608, 0.7647, 0.7647],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7686, 0.7686],
          [0.7608, 0.7608, 0.7608,  ..., 0.7647, 0.7686, 0.7686],
          ...,
          [0.8392, 0.8471, 0.8667,  ..., 0.5020, 0.5059, 0.5098],
          [0.8353, 0.8471, 0.8667,  ..., 0.5020, 0.5059, 0.5098],
          [0.8549, 0.8706, 0.8824,  ..., 0.5020, 0.5059, 0.5059]],

         [[0.7294, 0.7294, 0.7333,  ..., 0.7373, 0.7412, 0.7412],
          [0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7451, 0.7451],
          [0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7451, 0.7451],
          ...,
          [0.8549, 0.8627, 0.8863,  ..., 0.4431, 0.4431, 0.4471],
          [0.8510, 0.8627, 0.8863,  ..., 0.4431, 0.4431, 0.4471],
          [0.8706, 0.8863, 0.9020,  ..., 0.4431, 0.4431, 0.4431]]],


        [[[0.8039, 0.8039, 0.8118,  ..., 0.8275, 0.8314, 0.8314],
          [0.8078, 0.8118, 0.8118,  ..., 0.8235, 0.8275, 0.8275],
          [0.8118, 0.8118, 0.8157,  ..., 0.8235, 0.8275, 0.8275],
          ...,
          [0.7961, 0.8118, 0.8235,  ..., 0.6039, 0.6039, 0.6039],
          [0.8000, 0.8157, 0.8275,  ..., 0.6039, 0.6039, 0.6000],
          [0.8039, 0.8196, 0.8275,  ..., 0.6000, 0.6000, 0.6000]],

         [[0.7647, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7608],
          [0.7608, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7608],
          [0.7647, 0.7647, 0.7647,  ..., 0.7647, 0.7647, 0.7647],
          ...,
          [0.8431, 0.8588, 0.8745,  ..., 0.4941, 0.4941, 0.4941],
          [0.8471, 0.8627, 0.8745,  ..., 0.4941, 0.4941, 0.4902],
          [0.8471, 0.8627, 0.8745,  ..., 0.4902, 0.4902, 0.4902]],

         [[0.7373, 0.7373, 0.7373,  ..., 0.7451, 0.7490, 0.7451],
          [0.7373, 0.7412, 0.7373,  ..., 0.7451, 0.7451, 0.7451],
          [0.7412, 0.7412, 0.7373,  ..., 0.7451, 0.7451, 0.7451],
          ...,
          [0.8588, 0.8745, 0.8941,  ..., 0.4510, 0.4510, 0.4510],
          [0.8627, 0.8784, 0.8941,  ..., 0.4471, 0.4510, 0.4510],
          [0.8627, 0.8784, 0.8941,  ..., 0.4431, 0.4471, 0.4510]]]])</code></pre>
</div>
</div>
</section>
<section id="passing-the-dataloader-to-the-pytorch-training-loop" class="level1">
<h1>passing the dataloader to the pytorch training loop</h1>
<p>Create a simple linear model for demostration purposes and pass the dataloader to the pytorch training loop. The code below comments will show how to do this and the results.</p>
<div class="cell" data-execution_count="381">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleLinearRegressor(nn.Module):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size):</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleLinearRegressor, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define a single linear layer</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input size is the flattened image size (3*480*640 for your case)</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output size is 2 (for the two coordinates you want to predict)</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(input_size, output_size)</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Flatten the input tensor to match the linear layer's expected input size</span></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the input through the linear layer</span></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming the input image size is 3*480*640</span></span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">480</span> <span class="op">*</span> <span class="dv">640</span>  <span class="co"># Number of input features (pixels in the image)</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Number of output features (the two coordinates)</span></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleLinearRegressor(input_size, output_size)</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SimpleLinearRegressor(
  (linear): Linear(in_features=921600, out_features=2, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="382">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the loss function and optimizer for regression</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop for regression</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span>  <span class="co"># Adjust as needed</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="384">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>    batch_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> train_loader:</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the inputs; data is a list of [inputs, labels]</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> item[<span class="st">'image'</span>]</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> item[<span class="st">'label'</span>]</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Zero the parameter gradients</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward + backward + optimize</span></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(inputs)</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels.<span class="bu">float</span>())  <span class="co"># Ensure labels are float for regression</span></span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print statistics</span></span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Batch </span><span class="sc">{</span>batch_counter<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_counter <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">199</span>:    <span class="co"># Print every 200 mini-batches</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'[</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">] loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> <span class="dv">200</span><span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Finished Training'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch 0 loss: 262365331456.0
Batch 0 loss: 2.998479878094848e+16
Batch 0 loss: 3.439901749731253e+21
Batch 0 loss: 3.959848549424854e+26
Batch 0 loss: 4.571041369465859e+31
Batch 0 loss: 5.239716181450215e+36
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: inf
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Batch 0 loss: nan
Finished Training</code></pre>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In summary, the journey through the Hugging Face ecosystem reveals a treasure trove of resources for anyone delving into the realms of machine learning and deep learning. From its vast repository of models and datasets to its innovative libraries such as Transformers and Datasets, Hugging Face stands as a beacon for both newcomers and seasoned professionals seeking to expedite their AI projects. The practical examples and insights offered in this blog underscore the simplicity and power of integrating Hugging Face with PyTorch, showcasing the seamless path from dataset acquisition to model training.</p>
<p>As we’ve explored, the unique advantages of Hugging Face, including its collaborative community, extensive model hub, and user-friendly libraries, set it apart from other platforms. Whether you’re looking to fine-tune a state-of-the-art model, contribute to the ever-growing repository, or simply learn about AI, Hugging Face offers an accessible and enriching environment to do so.</p>
<p>I encourage readers to dive into the Hugging Face ecosystem, experiment with its libraries, and engage with its community. The possibilities are vast, and the opportunity for learning and innovation is immense. As the field of AI continues to evolve, platforms like Hugging Face play a pivotal role in democratizing access to cutting-edge AI, empowering us to push the boundaries of what’s possible.</p>
<p>Let’s embark on this journey together, exploring the potential of AI and contributing to a future where technology enhances every aspect of our lives. Happy modeling!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>