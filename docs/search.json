[
  {
    "objectID": "useful_info.html",
    "href": "useful_info.html",
    "title": "Commands to run quarto",
    "section": "",
    "text": "Commands to run quarto\nto create a project (run once at the beginning to create the project files) - quarto create project_name –template=blog\nto run a preview locally - quarto preview .\nto Create update the files (run before pushing to github) - quarto render .\n\n\nlinks :\n\nVideo of somone showing how to use quarto https://www.youtube.com/watch?v=nllKcuX7rEc\nilovetensoor coppied a good link here : https://discord.com/channels/689892369998676007/1096981866248147014/1167453441291984936\nexample blog http://vishalbakshi.com/blog/posts/2023-10-09-nn-rf-embeddings/\nexample blog https://ilovetensor.tech/posts/neural-nets-from-scratch/\n\n\n\nquarto reference page :\nhttps://quarto.org/docs/reference/cells/cells-jupyter.html\n\n\nsetup for using colab with quarto\n\nCreate a new notebook in colab and add markup in markup - no need for RAW as documentation says.\ninstall the drive app in windows which will reveal the drive folder in the file explorer, i used streams so no files are stored locally.\nCreate a script to copy file to local installation and run “quarto render .” to create the html file. see file located C:projects_lsuv_from_colab.bat\nEdit the notebook in colab and run the script to render the html file, repeat until happy with the result and publish to github.\n\n\n\nUploaded to netlify\nquarto publish netlify"
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html",
    "href": "posts/transformers_paper/paper_explanation.html",
    "title": "Paper reading group - Attention is All You Need",
    "section": "",
    "text": "Every 2nd Saturday we have a paper reading group where we submit papers we are interested in reading and vote for our favorite paper to read together. This week we discussed the paper “Attention is All You Need” (2017). This paper introduced the Transformer model, which has since become the foundation for many state-of-the-art models such as ChatGPT. This is my take on Transformers, why it is so important and why it’s used so widely. The aim is not to go into extensive details as numerous resources online already exist but to give an intuitive understanding and provide resources I found useful in understanding transformers from different perspectives, e.g. from a visual, maths, code and conceptual. I will assume you have basic understanding of Neural networks, back propagation ect and I will be using the token and words interchangeably (token are what language models actually use)."
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#easy-analogy-for-qkv-mechanism",
    "href": "posts/transformers_paper/paper_explanation.html#easy-analogy-for-qkv-mechanism",
    "title": "Paper reading group - Attention is All You Need",
    "section": "Easy Analogy for QKV Mechanism",
    "text": "Easy Analogy for QKV Mechanism\nImagine you’re at a large dinner party, trying to follow the conversations around the table:\nQuery (Q): This is like you trying to understand a comment made by someone. You’re focused on this comment and trying to figure out its context and relevance to the conversation. Key (K): Think of every person at the table as holding a “key” to different pieces of information. Some of what they say will be more relevant to understanding the comment you’re focused on, and some less so. Value (V): The “value” is the actual content or meaning each person (key) can contribute to help you understand the comment in question. After deciding whose input is most relevant, you’ll pay more attention to what those few people are saying. The transformer, like you in this scenario, uses the QKV mechanism to decide which parts of the input (the conversation) to pay attention to when trying to understand each piece (word or comment)."
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#easy-analogy-for-multiple-heads",
    "href": "posts/transformers_paper/paper_explanation.html#easy-analogy-for-multiple-heads",
    "title": "Paper reading group - Attention is All You Need",
    "section": "Easy Analogy for Multiple Heads",
    "text": "Easy Analogy for Multiple Heads\nContinuing with the dinner party analogy, imagine now that you’re not just trying to understand the content of the conversation but also the emotional tone, who’s leading the conversation, and how topics are changing over time.\nHaving multiple heads is like having several versions of you at the party, each with a different focus. One version of you is trying to follow the main topic, another is paying attention to the emotional undercurrents, another is noting how the conversation topics shift, and so on. Each “version” of you can focus on different aspects of the conversation simultaneously, ensuring that you get a fuller understanding of what’s happening at the dinner party than you would if you were just trying to follow the main topic. In essence, the QKV mechanism with multiple heads allows the transformer to “attend” to a complex sequence (like a conversation) from multiple perspectives at once, ensuring it captures the rich, multifaceted nature of the data it’s processing."
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#video-summaries",
    "href": "posts/transformers_paper/paper_explanation.html#video-summaries",
    "title": "Paper reading group - Attention is All You Need",
    "section": "Video Summaries",
    "text": "Video Summaries\n15 min—really fantastic animated summary 30 min–video supplement to The Illustrated Transformer 50 min—fantastic code walkthrough of encoder 40 min—fantastic code walkthrough of decoder"
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#statquest-youtube-videos",
    "href": "posts/transformers_paper/paper_explanation.html#statquest-youtube-videos",
    "title": "Paper reading group - Attention is All You Need",
    "section": "Statquest youtube videos",
    "text": "Statquest youtube videos\nTransformers Decoders only trasnformers LSTMs Seq2Seq Attention for neural networks Cosine similarity"
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html",
    "href": "posts/iot_in_a_factory/index.html",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "",
    "text": "As someone deeply involved in factory operations, I’ve witnessed firsthand the remarkable transformation brought about by the Internet of Things (IoT). Let me share with you the journey we’ve embarked on to automate and streamline our manufacturing processes, blending traditional methods with cutting-edge technology."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#introduction",
    "href": "posts/iot_in_a_factory/index.html#introduction",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "",
    "text": "As someone deeply involved in factory operations, I’ve witnessed firsthand the remarkable transformation brought about by the Internet of Things (IoT). Let me share with you the journey we’ve embarked on to automate and streamline our manufacturing processes, blending traditional methods with cutting-edge technology."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#the-need-for-automation",
    "href": "posts/iot_in_a_factory/index.html#the-need-for-automation",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "The Need for Automation",
    "text": "The Need for Automation\nMy day typically started with a walkthrough of the factory, assessing everything from energy usage to production progress. This routine made it clear that automating some of these processes was essential. We needed to evolve from traditional ERP systems that required manual logging to a more integrated approach using IoT technologies."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#integrating-real-time-monitoring",
    "href": "posts/iot_in_a_factory/index.html#integrating-real-time-monitoring",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Integrating Real-Time Monitoring",
    "text": "Integrating Real-Time Monitoring\nIn our control room, we combined advanced monitoring systems with traditional tools like whiteboards. The objective was to create a seamless blend of new and old, ensuring that the transition to high-tech did not overshadow the simplicity and effectiveness of conventional methods."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#practical-applications-of-iot-in-our-factory",
    "href": "posts/iot_in_a_factory/index.html#practical-applications-of-iot-in-our-factory",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Practical Applications of IoT in Our Factory",
    "text": "Practical Applications of IoT in Our Factory\n\nAdopting Wireless Technology: Despite initial skepticism due to our factory’s challenging environment, technologies like LoRaWAN proved to be a game-changer, offering robust and reliable wireless communication.\nImplementing Safety and Efficiency Sensors: We strategically placed cost-effective IoT sensors throughout the factory to monitor crucial parameters like temperature, humidity, and sound levels, ensuring a safer and more efficient working environment."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#gaining-a-competitive-edge",
    "href": "posts/iot_in_a_factory/index.html#gaining-a-competitive-edge",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Gaining a Competitive Edge",
    "text": "Gaining a Competitive Edge\nThe economic benefits of integrating IoT have been substantial. Reduced sensor costs and the ease of deploying edge devices have played a critical role in maintaining our competitive edge, especially in the international market. Open-source software has also been a boon, helping us cut operational costs significantly."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#overcoming-challenges",
    "href": "posts/iot_in_a_factory/index.html#overcoming-challenges",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Overcoming Challenges",
    "text": "Overcoming Challenges\nImplementing IoT in a factory setting wasn’t without its challenges. Wireless technologies initially struggled with the electromagnetic interference prevalent in our environment. However, adopting solutions like LoRaWAN helped us overcome these obstacles effectively."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#our-iot-system-architecture",
    "href": "posts/iot_in_a_factory/index.html#our-iot-system-architecture",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Our IoT System Architecture",
    "text": "Our IoT System Architecture\nOur system architecture is a blend of LoRaWAN devices, Industrial IoT devices, and legacy systems like Raspberry Pi. We store our data in an Influx database and use Grafana for real-time alerting and monitoring. This setup exemplifies our commitment to integrating modern technology while respecting the reliability of proven systems."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#conclusion",
    "href": "posts/iot_in_a_factory/index.html#conclusion",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Conclusion",
    "text": "Conclusion\nAs we continue to embrace IoT and its myriad applications, we’re not just improving our factory operations. We are actively participating in the fourth industrial revolution, paving the way for a more connected, efficient, and innovative future in manufacturing."
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "",
    "text": "This blog aims to provide a brief overview of the creation of a machine learning workflow from creating a dataset up to the stage of training a model. I have started a project to develop a neural network eye-tracking model to replace the mouse and this is a series of blogs of the journey, see the introduction to the project here.\nI will start the process by creating a dataset using the hugging faces dataset Python library. I will give a go over why it’s worth taking the time to learn what hugging face is and what it offers. We will then talk about how to transform the data into a format that can be used by PyToch and how to use the Pytorch dataloader to pass the data to the Pytorch training loop. Finally, we pass the data to a Pytorch training loop. The aim of this is not to create an optimised model but to show the pipeline up to the model."
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#how-does-hugging-face-compare-to-the-competition",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#how-does-hugging-face-compare-to-the-competition",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "How does Hugging Face compare to the competition",
    "text": "How does Hugging Face compare to the competition\nThe Hugging Face dataset library does have some competition in Kaggle which is also a machine learning platform which Hosts Datasets, Notebooks and competitions and is more oriented to competitions and using notebooks on their platform. The hugging face dataset library is more oriented to datasets and has a large number of datasets that are ready to use and prebuilt piplines that you can use own hardware or another platform. The kaggle python api can be found here\nThe dataset library also has some competition in pytorch and tensorflow. The pytorch dataset library can be found here and the tensorflow dataset library can be found here but are more geared at using their frameworks."
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#hugging-python-client-library",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#hugging-python-client-library",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Hugging Python client library",
    "text": "Hugging Python client library\nHugging Python client library allows you to manage all things hugging face hub and is aimed at individuals and teams collaborating on shared machine learning projects. You can create new repositories, download files from the hub, upload to the hub, search for models and run inference (run queries against models) and deploy models. See the quick start guide here for more information\n\nHugging face Transformers\nThe library here is a wide-ranging library, originally intended for NLP tasks but has since expanded to computer vision, audio and multimodal. Its a high-level API that allows you to use pretrained models and fine-tune among other features. The list of supported models and frameworks can be found here. The library compatable of jax, Pytorch and TensorFlow.\nSome of the key features include: - pipelines is a high-level, easy-to-use, API for doing inference over a variety of downstream-tasks - Trainer is a high-level API for PyTorch that makes training a much simpler task - Quantization for reducing memory requirements and inference speed\n\nand many more\n\n### Gradio\nGradio is an open-source Python library that allows you to quickly create UIs for your machine-learning models. It allows you to create a UI for your model in 3 lines of code making it easy to showcase your work. It also allows you to share your model with others. It can be used locally and Hugging Face has a tight integration where you can host on Hugging Face for free. It has several features including:\n\nCreate a UI for your model in 3 lines of code\none of the new features is the chat interface to help with the growth of all the language models\nshare your model with others\n\n\n\nHugging Face Diffuers\nThis model is the go-to library for pre trained diffusion for generating for images, audio and 3d structures of modecules. It has high level pipeline api for creating inference with just a few lines of code. It has interchangeable noise schedulers for balancing speed and quality and pretrained models that can be used as a starting point for your own models. “find more informatiohn here”.\nand finally, the last library we will talk about in more detail is the datasets library."
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#overview",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#overview",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Overview",
    "text": "Overview\nThe purpose of the hugging face datasets is to make it easy to access and share and shape datasets. The library is the backbone of the hugging face hub and is used to organise, and transfer the datasets so they can be used within a machine learning pipeline. Nearly every deep learning workflow starts with a dataset so understanding the hugging face datasets library is important to aquire data training and fine-tuning models. Once you have a dataset, the next step is to pass this to a dataset loader, this could be in pytorch or Tensorflow or just use with one of the higher level APIs that hugging face provides (e.g. transformer python library) and you wont need to worry about the underlying architecture.\nHugging faces dataset library is built on top of Apache Arrow making it fast and efficient for data loading and supporting caching making it even more efficient. Arrow allows fast processing and is column-oriented, memory mapping and gives incredible performance gains. It includes features for processing and preparing data, like filtering, splitting, and shuffling."
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#why-use-hugging-face-datasets-library",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#why-use-hugging-face-datasets-library",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Why use Hugging Face datasets library",
    "text": "Why use Hugging Face datasets library\nIn simple terms, the Hugging Face dataset library gives you everything you need to use an existing dataset or create datasets and get straight into the machine learning pipeline. find more information here. Its platform agnostic and can be used with any framework, for example, you can pass to Pytorch or Tensorflow. It has a large number of datasets that are ready to use and can be used with the Transformers library. It’s well-documented and has a large community that can help you with any questions you have. Get started with just 3 lines of code, load a dataset and start exploring in your notebook or script. The code below will show the beans dataset, you can also view the dataset in the hugging face hub here.\n\n# install and import the necessary libraries\n!pip install datasets[vision] \nfrom datasets import load_dataset, Image \n# Downloads the dataset called beans\ndataset = load_dataset(\"beans\", split=\"train\")"
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#create-the-dataset",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#create-the-dataset",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Create the dataset",
    "text": "Create the dataset\nFor this I will be using a dataset ive created myself for mapping pictures of me looking at the screen and co-ordinates on screen. The data was collected by writing a program that placed “x” on the screen at random coordinates. The program then recorded the coordinates of the “x” and a picture of the user’s face. The program then saved the image and named the file as the coordinates of the “x”. The process was repeated until the model was able to predict the coordinates of the “x” with a high degree of accuracy.\n\nimport os\nimport sys\nfrom datasets import Dataset, Image\nimport torch\n\n\n# dataset location on my drive \ndata_science_folder = 'G:\\My Drive\\Learning\\data_science'\nsys.path.append(data_science_folder)\ndataset_path = data_science_folder + \"\\datasets_folder\\gaze-points\\work-laptop\"\n\nA typical file name looks like the :\n\n20240123-140252-hieght2560-width1440-computerwork-laptop_2232_230.png\n\nwith the targets (the pixel im looking at on screen) in the file name and the file contents is a image of me looking at the screen. The last 2 numbers 2232 and 230 are the pixel co-ordinates that need to be stripped out of the file name. Below, ill detail how to strip out the co-ordinates.\n\n# extracts screen coordinates from the filenames and stores in a list of tensors\nlabel_tensors = [torch.tensor([int(f.split('_')[-2]), int(f.split('_')[-1].split('.')[0])]) for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n\n\n# get the last 20 elements for testings purposes\nlabel_tensors = label_tensors[:20]\n\nWe have imported the dataset using the hugging face library. The next step is to get a list of the full file names to pass to the Dataset object to be load the images.\n\n# gets a list of all images in a directory and stores in a list of strings\nimage_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n\n\nimage_files = image_files[:20]\n\n\n# confirm that the length of the labels and images are the same so they can be paired together during the creation of the dataset\nlen_image_files = len(image_files)\nlen_labels = len(label_tensors)\n\nprint(f\"confirm length of labels {len_labels} and length of image_files {len_image_files} are the same\")\n\nconfirm length of labels 20 and length of image_files 20 are the same\n\n\nLoad the images and cast (use the pil library to convert the images)\n\n# create the dataset from the image files and labels\ndataset = Dataset.from_dict({\"image\": image_files}).cast_column(\"image\", Image())\n\n\n# create a new dictionary with the images and labels\n# i'm not happy with having to add the labels to the dataset after as it takes alot longer\n# but i'm not sure how to do it in the the from_dict method above.\nupdated_dataset_dict = {\"image\": dataset[\"image\"], \"label\": label_tensors}\nupdated_dataset = Dataset.from_dict(updated_dataset_dict)\n\n\nupdated_dataset\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 20\n})\n\n\n\nupdated_dataset[\"image\"][1]\n\n\n\n\n\nupdated_dataset[\"label\"][1]\n\n[526, 1015]"
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#transforming-the-dataset-into-tensors-ready-for-pytorch",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#transforming-the-dataset-into-tensors-ready-for-pytorch",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Transforming the dataset into tensors ready for pytorch",
    "text": "Transforming the dataset into tensors ready for pytorch\nWe will need to transform the data to tensors instead of PIL.jpeg format and the labels will need to be tensors. You can see from viewing the first element in the cell above that its a list of 2 integers. The image above is a picture of me looking at the screen. We now need to convert the labels and images to tensors using pytorches vision library. We can then pass this to the dataset object using the with_transform function.\nThere a couple of ways to do this in hugging face datasets library. The first is to use the map function and the second is to use the with_transform function. The map function is applies straight away but consumes a lot of memory and the with_transform function is applied when the data is loaded (or requested from the dataset object). The with_transform function is the best option for large datasets. The with_transform method is shown below.\n\nCreate a tensor from a list of integers\nA detailed pytorch tensor is out of the scope of this notebook but you can find more information here. In simple terms, a tensor is a multi-dimensional array that can be used to store and process data. Below is a simple example of how to create a tensor from a list of integers and show the tensor datatype and the tensor itself.\n\n# example of how to use the torch stack function\n# The following will take a list of inter \nlist_of_ints_1 = [1, 2, 3, 4, 5]\nlist_of_ints_2 = [6, 7, 8, 9, 10]\nlist_of_ints_3 = [11, 12, 13, 14, 15]\npytorch_stacked = torch.stack([torch.tensor(list_of_ints_1), torch.tensor(list_of_ints_2), torch.tensor(list_of_ints_3)], dim=0)\n\n\n# the resulting tensor will be a 3x5 tensor\npytorch_stacked\n\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10],\n        [11, 12, 13, 14, 15]])\n\n\n\n# pick out the first element of the first row\npytorch_stacked[0][0]\n\ntensor(1)\n\n\n\n# shows the stored datatype of the tensor\npytorch_stacked[0][0].type()\n\n'torch.LongTensor'"
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#using-hugging-faces-with_tranform-with-pytorch-vision-library-to-transform-the-data",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "Using Hugging Faces with_tranform with pytorch vision library to transform the data",
    "text": "Using Hugging Faces with_tranform with pytorch vision library to transform the data\nThe Hugging Faces with_transform is applied on-the-fly on batches of data when iterating over the dataset. The with_transform function is the best option for large datasets.\nThe PyTorch vision library is comprehensive and consists of popular datasets, model architectures, and common image transformations for computer vision. Tensor images are expected to be of shape (C, H, W). The torchvision ToTensor() transform the pil/jpg into shape (C, H, W) with the tensor type as FloatTensor.\nThe code below shows how to transform the dataset required to pass the data to the dataloader using Hugging Faces with_transformation function and the pytorch vision library. It will also shows the results of the transformation along with comments.\n\n# Creates a transform that converts the image to a tensor\nfrom torchvision import transforms\nto_tensor = transforms.ToTensor()\n\n\ndef transform_images_with_stack(batch):\n    if \"image\" in batch:\n        # Convert all images in the batch to tensors and collect them in a list\n        images_tensor = torch.stack([to_tensor(image) for image in batch['image']])\n        batch['image'] = images_tensor  # Replace the list of images with a stacked tensor\n    if \"label\" in batch:\n        # Convert all labels in the batch to tensors and collect them in a list\n        labels_tensor = torch.stack([torch.tensor(label) for label in batch['label']])\n        batch['label'] = labels_tensor  # Replace the list of labels with a stacked tensor\n    return batch\n\n\n# Executes the transform on the dataset, the returning dataset[image] will be a tensor  \nupdated_dataset_with_transform = updated_dataset.with_transform(transform_images_with_stack)\n\n\n# prints the image now in tensor format\nupdated_dataset_with_transform[0][\"image\"]\n\ntensor([[[0.8157, 0.8157, 0.8157,  ..., 0.8196, 0.8196, 0.8196],\n         [0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8235, 0.8235],\n         [0.8196, 0.8196, 0.8196,  ..., 0.8196, 0.8196, 0.8235],\n         ...,\n         [0.8078, 0.8196, 0.8314,  ..., 0.5922, 0.5961, 0.6000],\n         [0.8000, 0.8118, 0.8314,  ..., 0.5882, 0.5961, 0.5961],\n         [0.8196, 0.8314, 0.8431,  ..., 0.5882, 0.5922, 0.5961]],\n\n        [[0.7647, 0.7647, 0.7647,  ..., 0.7608, 0.7608, 0.7608],\n         [0.7686, 0.7686, 0.7686,  ..., 0.7608, 0.7647, 0.7647],\n         [0.7686, 0.7686, 0.7686,  ..., 0.7686, 0.7686, 0.7725],\n         ...,\n         [0.8314, 0.8431, 0.8588,  ..., 0.5098, 0.5098, 0.5137],\n         [0.8235, 0.8392, 0.8588,  ..., 0.5020, 0.5098, 0.5098],\n         [0.8431, 0.8588, 0.8706,  ..., 0.5020, 0.5059, 0.5098]],\n\n        [[0.7333, 0.7333, 0.7333,  ..., 0.7412, 0.7412, 0.7412],\n         [0.7373, 0.7373, 0.7373,  ..., 0.7412, 0.7451, 0.7451],\n         [0.7373, 0.7373, 0.7373,  ..., 0.7451, 0.7451, 0.7490],\n         ...,\n         [0.8627, 0.8745, 0.8941,  ..., 0.4431, 0.4510, 0.4549],\n         [0.8549, 0.8706, 0.8941,  ..., 0.4471, 0.4549, 0.4588],\n         [0.8745, 0.8902, 0.9059,  ..., 0.4471, 0.4549, 0.4627]]])\n\n\n\n# notice that the shape is channel first, height, and then width\nupdated_dataset_with_transform[0][\"image\"].shape\n\ntorch.Size([3, 480, 640])\n\n\n\n# prints the value in the first channel, top row, and first column so top left pixel of the image\nupdated_dataset_with_transform[0][\"image\"][0][0][0]\n\ntensor(0.8157)\n\n\n\n# shows the stored datatype of the tensor\n# prints the value in the first channel, top row, and first column so top left pixel of the image\nupdated_dataset_with_transform[0][\"image\"][0][0][0].type()\n\n'torch.FloatTensor'\n\n\n\n# prints all the labels in the dataset\nupdated_dataset_with_transform[\"label\"].shape\n\ntorch.Size([20, 2])\n\n\n\n# prints  X, Y coordinates of the first label in the dataset\nupdated_dataset_with_transform[0][\"label\"]\n\ntensor([1073,    4])"
  },
  {
    "objectID": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data",
    "href": "posts/hugging_face_datasets_library/Hugging_face_datasets.html#using-hugginfaces-set_format-and-hugging-faces-own-torch-function-to-transform-the-data",
    "title": "Leveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking",
    "section": "using hugginfaces set_format and hugging faces own torch function to transform the data",
    "text": "using hugginfaces set_format and hugging faces own torch function to transform the data\nHugging faces set_format is a applied is also applied on-the-fly on batches of data when iterating over the dataset. The set_format function is the best option for large datasets.\nThe torch set_format torches converts them into a pytorch format, however the torch function is not as comprehensive as the torchvision library. It converts to a tensor but to a different shape (H, W, C) and THE FORMAT IS uint8 instead of a FloatTensor.\n\n# Create copy of the dataset for the next example\nupdated_dataset_set_format = updated_dataset\n\n\n# apply the set_format method to the dataset using hugging faces torch format\nupdated_dataset_set_format.set_format(type='torch', columns=['image','label'])\n\n\n# returns no of images, height, width, and channels all in a tensor\nupdated_dataset_set_format[\"image\"].shape\n\ntorch.Size([20, 480, 640, 3])\n\n\n\n# Notice the shape is different to before, height, width, channel \nupdated_dataset_set_format[0][\"image\"].shape\n\ntorch.Size([480, 640, 3])\n\n\n\n# prints the image now in tensor format\nupdated_dataset_set_format[0][\"image\"]\n\ntensor([[[208, 195, 187],\n         [208, 195, 187],\n         [208, 195, 187],\n         ...,\n         [209, 194, 189],\n         [209, 194, 189],\n         [209, 194, 189]],\n\n        [[209, 196, 188],\n         [209, 196, 188],\n         [209, 196, 188],\n         ...,\n         [209, 194, 189],\n         [210, 195, 190],\n         [210, 195, 190]],\n\n        [[209, 196, 188],\n         [209, 196, 188],\n         [209, 196, 188],\n         ...,\n         [209, 196, 190],\n         [209, 196, 190],\n         [210, 197, 191]],\n\n        ...,\n\n        [[206, 212, 220],\n         [209, 215, 223],\n         [212, 219, 228],\n         ...,\n         [151, 130, 113],\n         [152, 130, 115],\n         [153, 131, 116]],\n\n        [[204, 210, 218],\n         [207, 214, 222],\n         [212, 219, 228],\n         ...,\n         [150, 128, 114],\n         [152, 130, 116],\n         [152, 130, 117]],\n\n        [[209, 215, 223],\n         [212, 219, 227],\n         [215, 222, 231],\n         ...,\n         [150, 128, 114],\n         [151, 129, 116],\n         [152, 130, 118]]], dtype=torch.uint8)\n\n\n\n# prints the value in the chanels \nupdated_dataset_set_format[0][\"image\"][0][0]\n\ntensor([208, 195, 187], dtype=torch.uint8)\n\n\n\n# prints number of labels,  the x and y coordinates \nupdated_dataset_set_format[\"label\"].shape\n\ntorch.Size([20, 2])\n\n\n\n# prints the first element of the label tensor\nupdated_dataset_set_format[\"label\"][0]\n\ntensor([1073,    4])"
  },
  {
    "objectID": "posts/eye_tracking_start/eye_tracking.html",
    "href": "posts/eye_tracking_start/eye_tracking.html",
    "title": "Eye-Tracking: quick overview of the project",
    "section": "",
    "text": "In the realm of human-computer interaction, the mouse has been a stalwart companion. However, with advancements in AI and computer vision and modern hardware, high-definition cameras, efficient GPU and large high-definition screens, we stand on the cusp of a revolution: eye-tracking technology. This project isn’t just about replacing a mouse; it’s about leveraging the capabilities of modern computing to create a more natural, intuitive, and efficient way to interact with digital environments. Keyboards and mice are the tools of the past and contribute to bad posture. The future is in our eyes.\n\n\n\neye tracking"
  },
  {
    "objectID": "posts/eye_tracking_start/eye_tracking.html#improving-accuracy-with-advanced-features",
    "href": "posts/eye_tracking_start/eye_tracking.html#improving-accuracy-with-advanced-features",
    "title": "Eye-Tracking: quick overview of the project",
    "section": "Improving Accuracy with Advanced Features",
    "text": "Improving Accuracy with Advanced Features\nTo enhance accuracy, we’re considering the integration of additional parameters such as face-to-screen distance, head and eye rotation, and environmental factors. This involves not only refining the model but also exploring new algorithms and neural network architectures capable of processing complex, multi-dimensional data."
  },
  {
    "objectID": "posts/eye_tracking_start/eye_tracking.html#exploring-cutting-edge-technologies",
    "href": "posts/eye_tracking_start/eye_tracking.html#exploring-cutting-edge-technologies",
    "title": "Eye-Tracking: quick overview of the project",
    "section": "Exploring Cutting-Edge Technologies",
    "text": "Exploring Cutting-Edge Technologies\nDepth cameras and infrared sensors are on our radar as potential tools to improve tracking accuracy, especially in less-than-ideal lighting conditions. These technologies could provide richer data for our model, but they bring their own set of technical hurdles."
  },
  {
    "objectID": "posts/eye_tracking_start/eye_tracking.html#experimenting-with-different-models",
    "href": "posts/eye_tracking_start/eye_tracking.html#experimenting-with-different-models",
    "title": "Eye-Tracking: quick overview of the project",
    "section": "Experimenting with Different Models",
    "text": "Experimenting with Different Models\nWe’re open to experimenting with various machine-learning models to find the optimal balance between accuracy and efficiency. This includes testing transformer models, which might offer advantages in processing sequential data like eye movements and experimenting with deeper CNN architectures."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html",
    "href": "posts/developing_the_skils_for_smart_futures/index.html",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "",
    "text": "In an era where leadership skills are pivotal for success across various sectors, the EEUK Leadership Event 2020 provided a platform for vibrant discussions and shared learning. The event, conducted virtually, brought academics from Sheffield Hallam, Sheffield University, Liverpool John Moores university together with industry professionals, and students to explore the development of leadership skills in the context of emerging trends and challenges."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#innovative-teaching-methods",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#innovative-teaching-methods",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Innovative Teaching Methods",
    "text": "Innovative Teaching Methods\nThe event highlighted innovative approaches to teaching leadership skills. Lisa McMullan and Matthias Feist praised the use of visual minutes in capturing key learnings, emphasizing the importance of reflective and engaging teaching methods."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#interdisciplinary-learning",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#interdisciplinary-learning",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Interdisciplinary Learning",
    "text": "Interdisciplinary Learning\nGary Wood from Sheffield Hallam University and other panelists discussed the significance of interdisciplinary learning in leadership education. They stressed that integrating different disciplines could enhance students’ understanding and application of leadership skills in diverse contexts."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#collaboration-between-academia-and-industry",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#collaboration-between-academia-and-industry",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Collaboration Between Academia and Industry",
    "text": "Collaboration Between Academia and Industry\nThe dialogue emphasized the importance of academia-industry collaboration. Lesley Lambert from LJMU LCR 4 START discussed accessing funding for projects integrating industry issues into the curriculum, highlighting the benefits of such collaborations for students, industry, and academics."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#experiential-learning-opportunities",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#experiential-learning-opportunities",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Experiential Learning Opportunities",
    "text": "Experiential Learning Opportunities\nThe role of extracurricular and experiential learning was a focal point. Gary Wood spoke about the Sheffield Engineering Leadership Academy (SELA), mentioning its non-credit bearing nature and the creative freedom it offers for exploring new teaching methods."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#enhancing-student-engagement",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#enhancing-student-engagement",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Enhancing Student Engagement",
    "text": "Enhancing Student Engagement\nHayley Jones and Laura Foster raised concerns about effectively communicating opportunities to students. They discussed strategies like leveraging university PR channels and integrating success stories to increase student participation in leadership development programs."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#digital-transformation-and-leadership",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#digital-transformation-and-leadership",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Digital Transformation and Leadership",
    "text": "Digital Transformation and Leadership\nAlex Kelly emphasized the growing importance of digital skills in leadership, citing the relevance of coding and AI understanding for future leaders. He also highlighted the potential of hackspaces in fostering creativity, communication skills, and leadership."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#the-future-of-leadership-education",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#the-future-of-leadership-education",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "The Future of Leadership Education",
    "text": "The Future of Leadership Education\nThe event concluded with discussions on the future trajectory of leadership education, particularly in light of the pandemic’s impact. Speakers like Daniel Habbershaw and Nick Cooper discussed adapting to current challenges and the importance of reflective and adaptive learning approaches."
  },
  {
    "objectID": "posts/apache_arrow/apache_arrow.html",
    "href": "posts/apache_arrow/apache_arrow.html",
    "title": "The rise of Apache Arrow",
    "section": "",
    "text": "Why spend the effort to write about Apache Arrow\nI came across Apache Arrow when I was looking into the Hugging Face library and they made a big deal of it. So I searched for more information about it and I found out Wes McKinney was a big part of it. I’ve followed Wes before from his time at Pandas and Everything he touches turns to gold so I started asking myself, what is Apache Arrow, what it means to the industry and how might this help me in job. Other related projects will get a boost from this and Wes is part of some of those too. Time to dive deeper.\nSome of the projects i’ce heard about using Apache Arrow are :\n\nPandas a data frame library for manipulating data and presenting said data.\nPolars similar to pandas but written in Rust and are much faster but not as feature-rich as pandas and as many integrations.\nHugging Face Datasets and SafeTensors a library for AI and machine learning.\nApache Spark is a big data processing library that is used in many big data pipelines and is also used as part of Databricks.\nR is a programming lanaguage for data analysis and visualisation.\nInfluxDB which I’ve used extensively in work. Its a time series database used for sensor data and other time series data like servers.\n\nmore examples can be found here.\n\n\nWhat is Apache Arrow\nIn summary, it’s an in-memory columnar format that is not designed for storage but for in-memory use, hence the in-memory. Columular format allows acceleration on multicore CPUs and GPUs for faster data processing and faster due to the columns all having the same datatype (rows are usually different data types), and less waiting for your queries. The memory in use part means that data is optimized to be processed straight away whereas some formats are compressed or need to be converted, again slowing the data pipeline down.\n\n\nWhy are people (and companies) developing with Apache Arrow\nWhy invent something that already exists. If you’re creating a new data manipulation library, why not use Apache Arrow as the data backend, it’s widely used and lots of people are actively making it better and it has Apache Arrow behind it with all their governance. It has a big community behind it and is well-maintained. Just create an issue on their git page to get issues fixed and it’s all transparent.\nThe thing that interests me for my day job is moving data between one library (e.g polars) and another (e.g. pandas) or one programming language (e.g. python) and another (e.g rust), using something they call zero copy which I understand means the data stays in the same format/location and you just pass the pointer to the different libraries.\nI listened to a podcast by the creator of Influx DB listen here and his take was that if you spend all the time to developing a data backend. By the time it’s finished, you probably already lost against the competition as competition has already built using Apache Arrow and grabbed your customers. It’s also hard running a big team of developers, not just technically but dealing with people and all the other things that come with it. So why not use Apache Arrow and work on customer experience features that make your product unique. He also mentioned that the Apache Arrow team are doing a great job, if you cant beat them, join them!\nHugging Faces is using it for its efficient handling of data which is incredibly important in AI and Machine Learning pipelines. They make a lot of reference to the zero-copy of the data for things like saving weights of (large) models.\n\n\nHow will this affect me and the tasks I do regularly\nFor work, I perform data manipulation in Pandas but it’s slow whereas something like Polar is quick but is still young so doesn’t have all the features and integrations that Pandas has. I have some slow routines in Pandas where it’s looping through GBs of data, I’m going to use Polar for the data intensice part and then pass to Pandas for things like charting and other intergrations. I’m mostly seeing positives from libraries using Apache Arrow rather than companies/developers designing their data backends. From an end-user perspective, it’s great that I can use different tools throughout the pipeline and not have any incompatibility issues because the data is always the same datatype. This means I can use the best tool for the job at any particular part of the data pipeline.\nI’ve also heard good things about DuckDB which is a in-memory database that uses Apache Arrow.\nI use Pytorch pretty much every day and I’ve seen a project on their website torcharrow that’s in beta. It’s said to be similar to torch.tensor but I’ve not used it yet, I will when I get time or maybe when it’s more mature.\nIt is going to be interesting to see where else Apache Arrow pops up as I keep hearing more about it as the days go by."
  },
  {
    "objectID": "posts/AI_ethics/index.html",
    "href": "posts/AI_ethics/index.html",
    "title": "Navigating the Evolving Landscape of AI Ethics and Regulation: Key Insights from a Recent Discussion with AI Tech North",
    "section": "",
    "text": "The world of Artificial Intelligence (AI) is witnessing a significant transformation, not just technologically, but also in terms of ethics, regulations, and legal frameworks. A recent comprehensive discussion delved into these changes, offering valuable insights into the future of AI governance. Here’s a breakdown of the key points discussed, which are crucial for anyone involved in AI, whether as a developer, user, or regulator.\n\n\n\nEthics\n\n\n\nFrom Guidelines to Enforceable Laws The discourse is shifting from mere guidelines and standards to the creation of enforceable laws, particularly around data privacy. This transition is seeing countries like the U.S. and Europe setting legal precedents, expected to be followed globally. The emphasis is on aligning these laws to create a cohesive international legal framework for AI.\nThe Primacy of Data Privacy Data privacy emerges as a central theme. For AI applications like ChatGPT, the primary concern is the management and protection of personal data. The consensus is that as long as AI tools do not breach privacy norms, their use is considered ethically safe, especially when used in content creation, as this involves minimal personal data transfer.\nLaw and Ethics in New Territories An interesting analogy with hypothetical laws on Mars highlighted how new environments (like the digital space for AI) might require rethinking legal and ethical norms. The discussion distinguished between product manufacturing responsibilities and user behavior, relating it back to AI’s realm.\nGlobal Law Synchronization There’s an acknowledgment of the need for global law synchronization. This approach suggests that products made in one country but sold in another (like the U.S. or EU) must comply with the destination country’s laws. A set of universal moral codes and best practices for AI is seen as an emerging necessity.\nCombatting Bias in Health AI The discussion also touched upon the critical issue of bias in AI, particularly in healthcare. The EU AI Act was cited, emphasizing the need for AI systems to have unbiased datasets and the mandatory requirement for developers to proactively seek and eliminate any biases.\nRespecting Intellectual Privacy and Authority Intellectual property, especially in professional fields like teaching, law, and medicine, was highlighted. The discussion suggested AI tools should be designed to respect and protect the intellectual contributions of professionals in these fields.\nComprehensive AI Impact Assessment Assessing the societal, job-related, systemic, and environmental impacts of AI is crucial. The talk stressed the importance of proactive, ethical AI design and deployment, considering the far-reaching consequences AI can have.\nEthics Management in AI A tool named “Smart Ethics” was introduced, aimed at helping assess the ethical alignment of AI. It follows a continuous governance process, aligning with international standards and helping developers ensure their AI systems are ethically sound.\nThe Challenge of AI Pollution and the Importance of Trust The discussion warned against “AI pollution,” or the unchecked proliferation of AI tools. Building AI with trust and transparency was emphasized as critical for distinguishing meaningful AI development in a crowded market.\nRegulatory Challenges for Small AI Innovators A concern was raised that stringent regulations might inadvertently favor large corporations and stifle innovation among smaller players. This point highlighted the need for a balanced approach to AI regulation that supports innovation while ensuring ethical compliance.\n\nConclusion The session concluded with an emphasis on feedback and continuous improvement in the AI ethics field. Attendees were encouraged to provide input and participate in certification programs to stay abreast of these important developments.\nAs AI continues to evolve, discussions like these are vital for guiding its development in a responsible, ethical, and beneficial direction for society. Whether you are deeply embedded in the AI industry or just a casual observer, understanding these dynamics is crucial for navigating the future of AI.\nRead more about AI Tech North’s events, news, and courses here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI: The Catalyst of the 4th Industrial transformation",
    "section": "",
    "text": "Stepping into the heart of the 4th industrial revolution, this blog series examines the future of manufacturing, with AI and data analytics positioned at the core of this momentous transformation.\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPaper reading group - Attention is All You Need\n\n\n\n\n\n\n\nAI\n\n\nNLP\n\n\nAttention\n\n\nTransformer\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of Apache Arrow\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nWeights and bias to keep track of training run\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nAlex Kelly\n\n\n\n\n\n\n  \n\n\n\n\nLeveraging Hugging Face for Deep Learning: A Guide to Datasets to Model Training - part 1 Eye-Tracking\n\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nEye-Tracking: quick overview of the project\n\n\n\n\n\n\n\nAI\n\n\ncomputer vision\n\n\neye-tracking\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nNavigating the Evolving Landscape of AI Ethics and Regulation: Key Insights from a Recent Discussion with AI Tech North\n\n\n\n\n\n\n\nAI\n\n\nethics\n\n\nregulation\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nAdam optimizer with annealing learning rate\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nAlex Kelly\n\n\n\n\n\n\n  \n\n\n\n\nAI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.\n\n\n\n\n\n\n\nAI\n\n\nmanufacturing\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe Power of Retrieval-Augmented Generation in AI Language Models\n\n\n\n\n\n\n\ndeep learning\n\n\nnlp\n\n\nrag\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe Importance of Proper Initialization.\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nAlex Kelly\n\n\n\n\n\n\n  \n\n\n\n\nMaximizing Your Commute: Learning on the Go with ChatGPT Voice\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nchatbot\n\n\ngpt\n\n\nnlp\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nExploring GPT4 vision, fastai study hacks and what it means for industry 4.0\n\n\n\n\n\n\n\nai\n\n\nmanufacturing\n\n\nfastai\n\n\ngpt4\n\n\nchatgpt\n\n\nrpa\n\n\nautomation\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nEmbracing IoT in Manufacturing: My Journey to Transforming Factory Operations\n\n\n\n\n\n\n\nIoT\n\n\nManufacturing\n\n\nLoRaWAN\n\n\nIndustrial IoT\n\n\nRaspberry Pi\n\n\nInfluxDB\n\n\nGrafana\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nNurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020\n\n\n\n\n\n\n\nLeadership\n\n\nEducation\n\n\nEEUK\n\n\nEvent\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces\n\n\n\n\n\n\n\nIoT\n\n\nLoRaWAN\n\n\nMakerspaces\n\n\nSELA\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2020\n\n\nAlex Paul Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Alex Kelly Seasoned Technologist & Leader | Empowering Future Innovations in IT | Advocate of AI & IoT | Transforming Business Processes in Manufacturing Sector\nIn this space, I aim to unravel the intricacies of the 4th Industrial Revolution, shedding light on how AI, IoT, Robotics, and Deep Learning are converging to redefine the manufacturing landscape. Through a series of insightful blogs, I will share my explorations and findings on the synergies between these cutting-edge technologies and their transformative impact on industrial operations and processes. This journey delves into not just the technological advancements steering this revolution, but also the potential challenges and solutions that come along, offering a holistic view on the unfolding industrial epoch.\nJoin me as we navigate through the myriad facets of this transformative era, seeking to understand and harness the immense potential that lies at the intersection of AI, data analytics, and modern industrial practices."
  },
  {
    "objectID": "posts/adam_optimizer/index.html",
    "href": "posts/adam_optimizer/index.html",
    "title": "Adam optimizer with annealing learning rate",
    "section": "",
    "text": "The aim of this blog is to explain the role of a optimizer in a neural network training loop. I will explain what a neural network is, what a optimizer is and go deeper into maths of a optimizer to gain a more intuative understanding. All the code to run the models and charts are included in this blog post.\n\n\nA neural network, inspired by the human brain, is a form of machine learning model. The human brain comprises neurons interconnected by synapses, with electrical pulses transmitting information. In a similar manner, a neural network processes input, like reading a book, by activating relevant neurons involved in text interpretation, akin to the thinking process, before producing an output, such as a summary of the text. An artificial neural network (ANN) consists of artificial neurons organized in a specific architecture within a computer. Examples of ANN architectures include Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), or a hybrid of these and other layers. For instance, in image processing, the model receives a sequence of pixels (an image) and employs sequential CNNs to generate a corresponding sequence of pixels as an output.\n\n\nCode\n# ![\"Human neural network vs artificial neural network\"](\"https://clevertap.com/wp-content/uploads/2019/04/Neural_Network_Brain_Mimic.jpeg\")"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#neural-networks",
    "href": "posts/adam_optimizer/index.html#neural-networks",
    "title": "Adam optimizer with annealing learning rate",
    "section": "",
    "text": "A neural network, inspired by the human brain, is a form of machine learning model. The human brain comprises neurons interconnected by synapses, with electrical pulses transmitting information. In a similar manner, a neural network processes input, like reading a book, by activating relevant neurons involved in text interpretation, akin to the thinking process, before producing an output, such as a summary of the text. An artificial neural network (ANN) consists of artificial neurons organized in a specific architecture within a computer. Examples of ANN architectures include Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), or a hybrid of these and other layers. For instance, in image processing, the model receives a sequence of pixels (an image) and employs sequential CNNs to generate a corresponding sequence of pixels as an output.\n\n\nCode\n# ![\"Human neural network vs artificial neural network\"](\"https://clevertap.com/wp-content/uploads/2019/04/Neural_Network_Brain_Mimic.jpeg\")"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#what-is-an-optimizer-in-deep-learning-and-why-it-is-important",
    "href": "posts/adam_optimizer/index.html#what-is-an-optimizer-in-deep-learning-and-why-it-is-important",
    "title": "Adam optimizer with annealing learning rate",
    "section": "What is an optimizer in deep learning and why it is important?",
    "text": "What is an optimizer in deep learning and why it is important?\nAn optimizer is a method to update the weights of the neural network to minimize the loss function. A loss function is a measure between the predicted output and the actual output. In simple terms the closer these two are the better the model will perform, you will get what you expect from the model. The weights are updated by using the gradient of the loss function. The gradient is the slope of the loss function. It can be thought of as the direction the weights should be updated. If the gradient is positive then the weights should be increased. If the gradient is negative then the weights should be decreased. If the weights are increased/descreased by too much then the model will over shoot. This pattern can continue forever and not get you closer to your desired result, a smaller loss. The aim of the optimizer is to get to the smallest loss possible in the quickest amount of time or with the least amount of resources."
  },
  {
    "objectID": "posts/adam_optimizer/index.html#adam-optimizer-with-annealing-learning-rate",
    "href": "posts/adam_optimizer/index.html#adam-optimizer-with-annealing-learning-rate",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Adam optimizer with annealing learning rate",
    "text": "Adam optimizer with annealing learning rate\nThe aim of this blog is to show how to use the Adam optimizer with annealing learning rate. The Adam optimizer is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. The Adam optimizer is one of the most popular optimizers used in deep learning. The annealing learning rate is used to prevent the model from overfitting and is also used to speed up the training process."
  },
  {
    "objectID": "posts/adam_optimizer/index.html#setup-enviroment",
    "href": "posts/adam_optimizer/index.html#setup-enviroment",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Setup enviroment",
    "text": "Setup enviroment\n\n\nCode\nimport os\nimport sys\n\n\n\n\nCode\n# checks if the environment is local or remote\ndef check_if_local():\n    # Checking for common remote environment indicators\n    remote_indicators = ['COLAB_GPU', 'JUPYTERHUB_SERVICE_PREFIX']\n\n    # If any of the indicators are present, it's likely not a local environment\n    if any(indicator in os.environ for indicator in remote_indicators):\n        return False\n    else:\n        # Assuming local environment if none of the remote indicators are found\n        return True\n\n\n\n\nCode\n# checks if the environment is local or remote and sets the path accordingly\nif check_if_local() == False:\n    print('Running in a remote environment, mounting Google Drive...')\n    from google.colab import drive\n    drive.mount('/content/drive')\n    sys.path.append('/content/drive/MyDrive/Learning/data_science/')\n    !pip install datasets\n    !pip install torcheval\nelse :\n    print('Running in a local environment...')\n    sys.path.append('G:\\My Drive\\Learning\\data_science')\n\n\n\n\nRunning in a remote environment, mounting Google Drive...\n\n\n\n\nCode\nimport os\nimport sys\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#get-the-data",
    "href": "posts/adam_optimizer/index.html#get-the-data",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Get the data",
    "text": "Get the data\n\n\nCode\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\nbs = 1024\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=0)"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#setup-the-call-backs",
    "href": "posts/adam_optimizer/index.html#setup-the-call-backs",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Setup the call backs",
    "text": "Setup the call backs\n\n\nCode\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nlrf_cbs = [DeviceCB(), LRFinderCB()]"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#overview-of-optimizers",
    "href": "posts/adam_optimizer/index.html#overview-of-optimizers",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Overview of optimizers",
    "text": "Overview of optimizers\nWe will start of with one of the simplest optimizers and build up, taking in the results and how they improve as we introduce more sophisticated optimisers. We will cover SGD, Adam, Adam with Automatic Annealer, and then introduce Adam with Automatic Annealing as a call back. Abit about each of the optimizers:\n\nSGD is a simple optimizer that takes in the learning rate and the model parameters and updates the model parameters every time a backwards pass (every epoch) by subtracting the learning rate multiplied by the gradient of the loss function. Its beauty is that its a simple optimizer that only takes in learning rate making it low resource and simple to understand. The learning rate is a hyperparameter that needs to be tuned to get the best results. If the learning rate is too high then the model will overshoot and not get to the minimum loss. If the learning rate is too low then the model will take a long time to get to the minimum loss. The learning rate is a constant and does not change during the training process.\nAdam is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. It is one of the most popular optimizers used in deep learning. It is a adaptive learning rate optimizer that uses the first and second moments of the gradient to update the model parameters. The first moment is the mean of the gradient and the second moment is the uncentered variance of the gradient. The first moment is used to calculate the direction of the gradient and the second moment is used to calculate the size of the gradient. The Adam optimizer has three hyperparameters that need to be tuned to get the best results. The first hyperparameter is the learning rate. The second hyperparameter is the beta1 which is the exponential decay rate for the first moment. The third hyperparameter is the beta2 which is the exponential decay rate for the second moment. The learning rate is a constant and does not change during the training process. The beta1 and beta2 are constants and do not change during the training process. Adam is more resource intensive than SGD as it needs to keep a copy of the beta1 parameters and beta2 parameters in memory and also has more operations to calculate the results.\nAdam with Automatic Annealing is a same as Adam in that is has a adaptive estimation of first-order and second-order moments but also has a 3rd element that descreases every epoch. It does come in different flavours and the one demostrated here is one of the simplest. Its also one of the most popular optimizers used in deep learning. It is a adaptive learning rate optimizer that uses the first and second moments of the gradient to update the model parameters. The first moment is the mean of the gradient and the second moment is the uncentered variance of the gradient. The first moment is used to calculate the direction of the gradient and the second moment is used to calculate the size of the gradient. The Adam optimizer has three hyperparameters that need to be tuned to get the best results. The first hyperparameter is the learning rate. The second hyperparameter is the beta1 which is the exponential decay rate for the first moment. The third hyperparameter is the beta2 which is the exponential decay rate for the second moment. The fourth is the anneal_rate and this descreases the learning rate each epoch. The beta1 and beta2 are constants and do not change during the training process."
  },
  {
    "objectID": "posts/adam_optimizer/index.html#the-sgd-optimizer",
    "href": "posts/adam_optimizer/index.html#the-sgd-optimizer",
    "title": "Adam optimizer with annealing learning rate",
    "section": "The SGD optimizer",
    "text": "The SGD optimizer\nThe SGD optimizer class is a simple class that takes in params, learning rate (lr), and weight decay. It has 4 functions :\n\nStep\nOpt_step\nReg_step\nZero_grad\n\nThe key function we’re intersted in relating to TrainLearner is :\n\nThe step function which calls opt_step (updates the parameters from the learning rate) reg_step isnt used (updates parameters from the weight decay if given).\nZero_grad is called after every batch to zero the gradients, by default in pytorch they acculmtate which isnt desired.\n\n\n\nCode\n# SGD optimizer is a type of gradient descent optimizer.  It is a first order optimizer (only uses the first derivative)\n# It is a stochastic optimizer (it uses a random sample of the data to calculate the gradient)\n\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        \"\"\"\n        Initializes the SGD  optimizer.\n\n        Args:\n            params (iterable): Iterable of parameters to optimize.\n            lr (float): Learning rate.\n            wd (float, optional): Weight decay (default: 0).\n\n        \"\"\"\n        params = list(params)\n        fc.store_attr()\n        self.i = 0\n    # calculates the parameters and weight decays. Step occurs after the backward pass (when the gradients are calculated)\n    def step(self):\n        # calls torch.no_grad() to disable gradient tracking\n        with torch.no_grad():\n            # iterates over the parameters\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n\n    # Updates the parameters using the gradient and the learning rate\n    def opt_step(self, p): p -= p.grad * self.lr\n\n    # Calculates the weight decay and updates the parameters\n    # The purpose of weight decay is to prevent overfitting. It is calculated by multiplying the learning rate by the weight decay\n    # essentially it is a penalty for having large weights (it reduces the value of the weights)\n    def reg_step(self, p):\n        if self.wd != 0: p *= 1 - self.lr*self.wd\n\n    # Zeros out the gradient for all parameters.  This is useful because the gradients are accumulated by default (useful for RNNs)\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\n\n\nCode\nset_seed(42)                                             # sets the seed for reproducibility\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw) # gets the model and applies the init_weights function\n\n# creates the learner object and passes the model, dataloaders (the data), loss function, learning rate,\n# callbacks and optimizer function\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=SGD)\nlearn.fit(3)                                             # fits the model for 3 epochs\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.396\n1.808\n0\ntrain\n\n\n0.558\n1.467\n0\neval\n\n\n0.611\n1.304\n1\ntrain\n\n\n0.644\n1.179\n1\neval\n\n\n0.672\n1.093\n2\ntrain\n\n\n0.688\n1.029\n2\neval"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#adam-optimizer",
    "href": "posts/adam_optimizer/index.html#adam-optimizer",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Adam Optimizer",
    "text": "Adam Optimizer\nAdam optimizer (also known as Adaptive Moment Estimation) is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. It’s a combination of RMSprop and Momentum and commonly used in deep learning. It’s a little more complex than the SGD discussed above. There are some learning utilities and also numbered code explanations to help understand the code further.\nFor a deeper dive deeper, the following resources are good references.\n\nhttps://www.mathsisfun.com/calculus/second-derivative.html\nhttps://www.mathsisfun.com/calculus/second-derivative-animation.html\n\nThe Adam optimizer below inherits all the code from SGD and replaces opt_step with new code to update the parameters. The important variables opt_step takes in for our case are :\n\nParams (also known as p in opt_step) also known as the weights and bias’s\nBeta1 which really should be called something like “momentum_decay_rate”, it is akin to momentum in physics and helps to accelerate the optimizer in the direction of consistent and persistent gradients over time.\nBeta2 which should really be called something like “variance_decay_rate”. It is used for adaptive learning rate purposes, helping to adjust the learning rate based on the variability of the gradients\n\nA intuative way to think about beta1 (“momentum_decay_rate”) and beta2 (“variance_decay_rate”) is : \n\nBeta1 is the compass\nBeta2 is the distance to travel\n\nThe reason for this will be become clearer as i explain later on.\n\n\nCode\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n\n    def opt_step(self, p):\n1        if not hasattr(p, 'avg'): p.avg = torch.zeros_like(p.grad.data)\n2        if not hasattr(p, 'sqr_avg'): p.sqr_avg = torch.zeros_like(p.grad.data)\n\n        # Beta 1 calculations\n3        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n4        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n\n        # Beta 2 calculations\n5        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n6        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n\n        # Combination of beta1 and beta2 combinations\n7        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\n\n1\n\nChecks avg tensor doesnt exist in p, if doesnt existit creates a matching size tensor of all parameters to store averages, they will all be zeros.\n\n2\n\nChecks sqr_avg doesnt exist in p, if it doesnt exist it creates a matching sized tensor of all parameters to store sqr averages, they will all be zeros.\n\n3\n\np.avg is to store the averages, it can be thought of as a balance between the old gradients and the new gradiants Beta is set between 1 or 0, when its closer to 1 it takes more of the historical gradients into account and when close to 0, it takes more of the new gradients into account. When its closer to 1, it will be a smoother path and when its zero it will respond to changes in gradients much quicker and appear more erratic. Note, on the first run the calculation of p.avg is solely based on the latter part of the equation “(1 - self.beta1) * p.grad”. The first part of the equation “self.beta1 * p.avg” is zero meaning due to p.avg being set to zeros in the line “p.avg = torch.zeros_like(p.grad.data)”. The next time opt_step is called, The first part of the equation will influence the result “self.beta1 * p.avg”.\n\n4\n\nUnbias_avg purpose is to correct p.avg for the earier few times called when p.avg is initialised as zero as described in point 1. Something is required to reduce the depency on p.avg during the early calls, this is where unbias comes into play and will be more significant for the early iterations and gradually becomes less significant for later iterations.\n\n5\n\nSquaring the gradients (p.grad**2) emphasizes larger gradients and diminishes the impact of smaller ones. We dont want this but will become clearer on point 7.\n\n6\n\nunbias_sqr_avg purpose is to correct p.sqr_avg for the first few times called. Remember that in the previous step, the p.sqr_avg is zero so isnt included in the calculations. The unbias will is more significant for the early iterations and gradually becomes less significant for later iterations.\n\n7\n\nThe final peice is to update the parameters (weights and bias’s). The second part of the equation is to “(unbias_sqr_avg + self.eps).sqrt()” is to get the distance (the 2nd momentum), unbias_sqr_avg will lead to overshooting so square root the value to get a smoother line. The 1st part of equation is 1st moment (the direction of travel) and its to get at the average direction of the travel taking into a account the learning rate, following on from the anology of the compass, this means the more epoch’s (one pass through the dat) through the data, the less the direction changes.\n\n\n\n\n\nDeeper dive into the maths and charts\nI wanted to dive a bit deeper into the maths, it didnt make intuative sense until I started to plot the results in various ways. The 2 charts below really helped me understand whats going on. As mentioned below, I’ve kept hearing that you can think of the first moment as a compass and the second moment as a distance to travel but i didnt understand what it meant, hopefully you understand after you have read this.\nNote, the 2 illistrates below are using randomized data to illistrate points\nThe chart below shows the calculations for beta1 and beta2 in a flow chart style which shows that beta1 and beta 2 are really calculated in pararel until they are combined in item 7 mentioned above.\n\n\n\n\ngraph TD\n    A[beta calculations]\n\n    A --&gt; D1[Beta 1 - the compass]\n    D1 --&gt; E1[\"p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\"]\n    E1 --&gt; F1[\"unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\"]\n    F1 --&gt; G[End of Parallel Calculations]\n\n    A --&gt; D2[Beta 2 - The distance to travel]\n    D2 --&gt; E2[\"p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\"]\n    E2 --&gt; F2[\"unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\"]\n    F2 --&gt; G\n\n    G --&gt; H[Combine of Beta 1 and Beta 2 along with learning rate and self.eps to prevent divide by zero errors]\n    H --&gt; I[\"p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\"]\n    I --&gt; J[Update Parameter p]\n    J --&gt; K[End]\n\n\n\n\n\n\n\nThe compass - The first momentum\nThe first momentum (the compass) tells you the direction which is either negative or posotive, do you want to negativly update the paremeters or positivly. I’ve plotted beta1 results from 0.1 to 0.9 with the lower betas more swayed to the most recent calculated gradients (aka the most recent backward pass) to and ligher colours plotted to more to the older averaged gradients (historical gradients).  - The darker colours (lower beta1) swing more wildley going from negative to positve from itteration to itteration. - The lighter colours (higher beta1) have a much smoother line and stay positive or negative for more itterations.\nThere’s 2 important take aways.\n\nThe higher beta1 is, the smoother shorter the path.\nThe result from this part of the equation can be negative and positive. It fits the compass anology, it tells you the direction to update the weights and bias’s (parameters). This is key to understand the relationship with beta2 part of the equation.\n\n\n\nCode\n# Setting up an updated simulation with a wider range of beta1 values\n\niterations = 50\n\n# New range of beta1 values\nbeta1_values = np.arange(0.1, 1.0, 0.1)\n\n# Initialize a dictionary to store p.avg values for each beta1 across iterations\np_avg_values = {beta1: [0] for beta1 in beta1_values}  # Start with 0\n\n# Simulate p.avg updates over iterations\nfor beta1 in beta1_values:\n    for i in range(1, iterations):\n        # Simulate a random gradient at each step\n        p_grad = np.random.randn()\n\n        # Update p.avg using the formula: beta1*p.avg + (1-beta1)*p.grad\n        new_p_avg = beta1 * p_avg_values[beta1][-1] + (1 - beta1) * p_grad\n        p_avg_values[beta1].append(new_p_avg)\n\n# Plotting with relevant colors\nplt.figure(figsize=(12, 8))\n# Generate a color map to assign a unique color to each beta1 value\ncolors = plt.cm.viridis(np.linspace(0, 1, len(beta1_values)))\n\nfor beta1, color in zip(beta1_values, colors):\n    plt.plot(p_avg_values[beta1], label=f'beta1 = {beta1:.1f}', color=color)\n\nplt.xlabel('Iterations')\nplt.ylabel('p.avg')\nplt.title('Evolution of p.avg Over Iterations for Different beta1 Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nThe distance to travelled - The second momentum\nThe second momentum (the distance to travel) tells you the distance, that is how much to change the parameters and only the distance, not the direction (Beta2 is always positive).\nTo get to this result:\n\nWe first square the gradients which will turn the results positive, this has the effect of making any large number that is either negative or positive into a even larger positive number. Smaller numbers that are either negative or positive will also become positive but to a lesser degree.\nThen we will square root the previously squared number to scaled the number down.\n\nThe graphs below dispicts the operations mentioned above visually. - The left chart shows how squaring (Green) the results increases the magnitude positively - The right chart shows how smooth the line looks when square rooting (blue) the previously squared values. - The original gradients (marked in red) jump up and down over time over epochs (iterations).\nThe blue line shows a steady path (line in this case) to minimize the loss is key to getting to a loss function in the most predictable way. If you was to measure the red line, it would be much shorter than the blue (the gradient).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some gradients\nnp.random.seed(0)\ngradients = np.random.randn(100)  # Random gradients for simulation\n\n# Compute squared gradients\nsquared_gradients = gradients**2\n\n# Compute square root of the average of squared gradients (simulating the second moment in Adam)\nsqrt_avg_squared_gradients = np.sqrt(np.cumsum(squared_gradients) / np.arange(1, 101))\n\n# Plotting\nplt.figure(figsize=(12, 6))\n\n# Updating the plot with distinct colors for each curve\n\n# Plotting with distinct colors\nplt.figure(figsize=(12, 6))\n\n# Plot for Gradients and Squared Gradients\nplt.subplot(1, 2, 1)\nplt.plot(gradients, label='Gradients', color='blue')\nplt.plot(squared_gradients, label='Squared Gradients', color='green')\nplt.title('Gradients and Squared Gradients')\nplt.xlabel('Time Steps')\nplt.ylabel('Magnitude')\nplt.legend()\n\n# Plot for Gradients and Square Root of Avg of Squared Gradients\nplt.subplot(1, 2, 2)\nplt.plot(gradients, label='Gradients', color='blue', alpha=0.7)\nplt.plot(sqrt_avg_squared_gradients, label='Sqrt of Avg of Squared Gradients', color='red')\nplt.title('Gradients and Sqrt of Avg of Squared Gradients Over Time')\nplt.xlabel('Time Steps')\nplt.ylabel('Magnitude')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 1200x600 with 0 Axes&gt;\n\n\n\n\n\n\n\nBringing it all together\nNow bringing the compass aka direction (aka BETA1) and distance (aka BETA2) together.\nThe way I like to think of it, is we have the compass being positve or negative and we divide by the distance which is always positve. So the compass will either be pointing up or down at different magnatudes and the distance will either be long (a high number) or short (a low number).\n\n\nCode\n# first run with adam as the optimizer\nset_seed(42)                                                # sets the seed for reproducibility\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)    # gets the model and applies the init_weights function\n\n# creates the learner object and passes the model, dataloaders (the data), loss function, learning rate,\n# callbacks and optimizer function\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(3)                                                # fits the model for 3 epochs\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.796\n0.574\n0\ntrain\n\n\n0.843\n0.433\n0\neval\n\n\n0.868\n0.363\n1\ntrain\n\n\n0.867\n0.373\n1\neval\n\n\n0.884\n0.318\n2\ntrain\n\n\n0.875\n0.349\n2\neval"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#adam-with-automatic-annealer",
    "href": "posts/adam_optimizer/index.html#adam-with-automatic-annealer",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Adam with Automatic Annealer",
    "text": "Adam with Automatic Annealer\nAdam optimizer (also known as Adaptive Moment Estimation) with annealing is the same as vanlila Adam (i.e. adaptive estimation of first-order and second-order moments). However it also has a extra parameter named Annealing_rate.\nThis is a simplified annealing rate that reduces the learning rate each time step method is called (inherited from SGD). The only change is in opt_step and that is the line below and includes the annealing rate. This will reduce the learning rate each time opt_step is called. The idea being that at the beginning of the training we want to update the weights more to get to a loss reduced quicker as there is a lot more learning to do. However, the longer training process is running, there is a less learning to do and its more about fine tuning weights to get slight improvements to the loss.\n\n“annealed_lr = self.lr * (self.anneal_rate ** self.i)”\n\nInstead of updating the paremeters with the learning rate, it now uses annealed_lr as shown below.\n\np -= annealed_lr * unbias_avg / (unbias_sqr_avg.sqrt() + self.eps)\n\n\n\nCode\n# Run with adam with annealing as the optimizer\nclass AdamWithAnnealing(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5, anneal_rate=0.97):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n        self.anneal_rate = anneal_rate\n\n    def opt_step(self, p):\n        if not hasattr(p, 'avg'): p.avg = torch.zeros_like(p.grad.data)\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = torch.zeros_like(p.grad.data)\n\n        # Update averages\n        p.avg = self.beta1 * p.avg + (1 - self.beta1) * p.grad\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i + 1)))\n        p.sqr_avg = self.beta2 * p.sqr_avg + (1 - self.beta2) * (p.grad**2)\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i + 1)))\n\n        # Apply annealing to learning rate\n        annealed_lr = self.lr * (self.anneal_rate ** self.i)\n\n        # Update parameters\n        p -= annealed_lr * unbias_avg / (unbias_sqr_avg.sqrt() + self.eps)\n\n\n\n\nCode\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nlrf_cbs = [DeviceCB(), LRFinderCB()]\n\n\n\n\nCode\nset_seed(42)                                               # sets the seed for reproducibility\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)   # gets the model and applies the init_weights function\n\n# creates the learner object and passes the model, dataloaders (the data), loss function, learning rate,\n# callbacks and optimizer function\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=AdamWithAnnealing)\nlearn.fit(3)                                               # fits the model for 3 epochs\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.793\n0.580\n0\ntrain\n\n\n0.835\n0.452\n0\neval\n\n\n0.854\n0.406\n1\ntrain\n\n\n0.845\n0.430\n1\neval\n\n\n0.859\n0.392\n2\ntrain\n\n\n0.846\n0.427\n2\neval"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#sgd",
    "href": "posts/adam_optimizer/index.html#sgd",
    "title": "Adam optimizer with annealing learning rate",
    "section": "SGD",
    "text": "SGD\nThe key function we’re interested in relating to TrainLearner is the step function which calls opt_step (updates the parameters from the learning rate), reg_step isn’t used (updates parameters from the weight decay if given).\n\n\nCode\nclass SGDCallback_TrainCB(TrainCB):\n    def __init__(self, lr, wd=0., n_inp=1):\n        self.n_inp = n_inp\n        fc.store_attr()\n        self.i = 0\n        # calculates the parameters and weight decays. Step occurs after the backward pass (when the gradients are calculated)\n    def step(self, learn):\n        # calls torch.no_grad() to disable gradient tracking\n        with torch.no_grad():\n            # iterates over the parameters\n            for p in learn.model.parameters():\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n    # Updates the parameters using the gradient and the learning rate\n    def opt_step(self, p): p -= p.grad * self.lr\n\n    # Calculates the weight decay and updates the parameters\n    # The purpose of weight decay is to prevent overfitting. It is calculated by multiplying the learning rate by the weight decay\n    # essentially it is a penalty for having large weights (it reduces the value of the weights)\n    def reg_step(self, p):\n        if self.wd != 0: p *= 1 - self.lr*self.wd\n    def zero_grad(self, learn): learn.opt.zero_grad()\n\n\n\n\nCode\n# SGDCallback_TrainCB added to callbacks and no optimizer function passed\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, SGDCallback_TrainCB(lr=6e-3)]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\n\n\n\nCode\n# Training with SGD as the optimizer\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = Learner(model, dls, F.cross_entropy, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.396\n1.808\n0\ntrain\n\n\n0.558\n1.467\n0\neval\n\n\n0.611\n1.304\n1\ntrain\n\n\n0.644\n1.179\n1\neval\n\n\n0.672\n1.093\n2\ntrain\n\n\n0.688\n1.029\n2\neval"
  },
  {
    "objectID": "posts/adam_optimizer/index.html#adam-with-annealing",
    "href": "posts/adam_optimizer/index.html#adam-with-annealing",
    "title": "Adam optimizer with annealing learning rate",
    "section": "Adam with Annealing",
    "text": "Adam with Annealing\n\n\nCode\nclass AdamWithAnnealingCallback(TrainCB):\n    def __init__(self, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5, anneal_rate=0.97, n_inp=1):\n        self.lr, self.wd, self.beta1, self.beta2, self.eps = lr, wd, beta1, beta2, eps\n        self.anneal_rate = anneal_rate\n        self.n_inp = n_inp\n        self.i = 0\n\n\n    def step(self, learn):\n        # calls torch.no_grad() to disable gradient tracking\n        with torch.no_grad():\n            # iterates over the parameters\n            for p in learn.model.parameters():\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n\n    def reg_step(self, p):\n         if self.wd != 0: p *= 1 - self.lr*self.wd\n\n    def opt_step(self, p):\n\n\n\n        if not hasattr(p, 'avg'): # Checks avg tensor doesnt exist in p\n          p.avg = torch.zeros_like(p.grad.data) # creates a matching size tensor of all parameters to store averages, they will all be zeros.\n        if not hasattr(p, 'sqr_avg'): # checks sqr_avg doesnt exist in p\n          p.sqr_avg = torch.zeros_like(p.grad.data)  # creates a matching sized tensor of all parameters to store sqr averages, they will all be zeros.\n\n        # A break down on the formulas for the Adam Optimizer\n\n        # p.avg is to store the averages, it can be thought of as a balance between the old gradients and the new gradiants\n        # Beta is set between 1 or 0, when its closer to 1 it takes more of the historical gradients into account and when close to 0, it takes the new gradients into account.\n        # When its closer to 1, it will be a smoother path and when its zero it will respond to changes in gradients much quicker and appear more erratic.\n\n        # Note, on the first run the calculation of p.avg is solely based on the latter part of the equation \"(1 - self.beta1) * p.grad\".\n        # The first part of the equation \"self.beta1 * p.avg\" is zero meaning due to p.avg being set to zeros in the line \"p.avg = torch.zeros_like(p.grad.data)\".\n        # The next time opt_step is called, The first part of the equation will influence the result \"self.beta1 * p.avg\".\n        p.avg = self.beta1 * p.avg + (1 - self.beta1) * p.grad\n\n        # Unbias_avg purpose is to correct p.avg for the first few times called. Remember that in the previous step, the p.avg is zero so isnt included in the calculations.\n        # The unbias will  is more significant for the early iterations and gradually becomes less significant for later iterations.\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i + 1)))\n\n        # Squaring the gradients (p.grad**2) emphasizes larger gradients and diminishes the impact of smaller ones.\n        p.sqr_avg = self.beta2 * p.sqr_avg + (1 - self.beta2) * (p.grad**2)\n\n        # unbias_sqr_avg purpose is to correct p.sqr_avg for the first few times called. Remember that in the previous step, the p.sqr_avg is zero so isnt included in the calculations.\n        # The unbias will  is more significant for the early iterations and gradually becomes less significant for later iterations.\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i + 1)))\n\n        # reduces the learning rate with each run by the annealing rate\n        annealed_lr = self.lr * (self.anneal_rate ** self.i)\n\n        # Update parameters\n        p -= annealed_lr * unbias_avg / (unbias_sqr_avg.sqrt() + self.eps)\n\n    # Calculates the weight decay and updates the parameters\n    # The purpose of weight decay is to prevent overfitting. It is calculated by multiplying the learning rate by the weight decay\n    # essentially it is a penalty for having large weights (it reduces the value of the weights)\n\n    def zero_grad(self, learn): learn.opt.zero_grad()\n\n\n\n\nCode\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, AdamWithAnnealingCallback(lr=6e-3)]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\n\n\n\nCode\n# Training with SGD as the optimizer\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = Learner(model, dls, F.cross_entropy, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.793\n0.580\n0\ntrain\n\n\n0.835\n0.452\n0\neval\n\n\n0.854\n0.406\n1\ntrain\n\n\n0.845\n0.430\n1\neval\n\n\n0.859\n0.392\n2\ntrain\n\n\n0.846\n0.427\n2\neval"
  },
  {
    "objectID": "posts/all_things_RAG_paper_discussion/all_things_rag_paper_discussion.html",
    "href": "posts/all_things_RAG_paper_discussion/all_things_rag_paper_discussion.html",
    "title": "The Power of Retrieval-Augmented Generation in AI Language Models",
    "section": "",
    "text": "I’ve recently joined a discord group where we discuss deep learning papers and this week its about RAG (Retrieval-Augmented Generation). It consisted listening to the author of the paper and questions and answers session for hte majory of the time. Some of questions on the paper were interesting and we got to shape the direction of the discussion by our questions.\nThe paper can be found here\nThe discord group can be found here\n\nabout the paper discussed\nIn the ever-evolving landscape of Artificial Intelligence, some development has emerged from the labs of Facebook AI Research and University College London. Patrick Lewis and his team, brings to light an innovative approach to enhancing language models - the Retrieval-Augmented Generation (RAG) model. This breakthrough addresses a critical challenge in AI language processing: the efficient access and manipulation of knowledge for complex tasks.\n\n\nBridging the Knowledge Gap in Language Models\nTraditional language models, despite their size and sophistication, often stumble when dealing with knowledge-intensive tasks. They struggle to retrieve and accurately use specific information. This is where the RAG model steps in, blending the prowess of a pre-trained seq2seq model with a rich, non-parametric memory bank, primarily sourced from Wikipedia.\n\n\nThe Mechanics of RAG: A Symphony of Parametric and Non-Parametric Memory\nAt the heart of RAG lies a seamless integration of two components: a powerful seq2seq model and a neural retriever accessing a dense vector index of Wikipedia. This dual approach allows the model to dynamically pull in relevant information during the generation process, leading to more precise and informed responses. Pararmetric is deep learning models which are trained and in this case non-parametric are stored values in a vector database and retrieved when appropriate.\n\n\nRAG in Action: A Leap Forward in NLP Tasks\nThe effectiveness of RAG models is evident across a spectrum of natural language processing (NLP) tasks. From open-domain question answering to abstractive summarization, Jeopardy-style question generation, and fact verification. The paper also highlights the potential of RAG models in the field of dialogue systems, where they can be used to generate more contextually relevant responses.\n\n\nBeyond Performance: The Societal Impact of RAG Models\nWhile the technical achievements of RAG models are impressive, the paper also thoughtfully examines their broader impact. The potential societal benefits are significant, particularly in areas where accurate and diverse information generation is crucial. However, the team is also mindful of the downsides, including the ethical considerations and challenges that accompany advanced AI technologies.\n\n\nThe Road Ahead: A New Era in AI Language Processing\nThe introduction of Retrieval-Augmented Generation models marks a significant milestone in the field of AI and NLP. By effectively addressing the knowledge limitations of previous models, RAG paves the way for more nuanced and contextually aware AI systems. As we stand on the brink of this new era, the possibilities seem as boundless as they are exciting.\n\n\nQuestions and answers findings\n\nLarger models are going to play a key role in more accurate and diverse information generation.\nUse rag if you want the model to be more contextually aware - You could think of it being more biased to data supplied to the datastore. To be more specific, if you want know the solutions to know your address, eye color, customer specific information etc. rag is the way to go.\nRag is not a silver bullet, it has its own limitations. e.g. It can’t generate information which is not present in the knowledge base.\nThe future of rag is multimodel, where the knowledge base is not just text but also images, videos, audio etc.\n\nThis is going to be interesting but I imagine it will be computationally expensive.\nHows it going to split the attention between the different modalities.\nHows it going to chunk the information from the different modalities.\nHows it going to generate the information in the different modalities."
  },
  {
    "objectID": "posts/chatGPT_audio_conversation/index.html",
    "href": "posts/chatGPT_audio_conversation/index.html",
    "title": "Maximizing Your Commute: Learning on the Go with ChatGPT Voice",
    "section": "",
    "text": "Overview\nI have 20 minute drive into work and back home everyday where I usually listen to a podcast, audio book or sometimes just stair at the road. I’m always thinking, how can I make better use of my time…. I saw a twitter post from Le Cunn, the legendary Data Scientist from Meta and that i fearously agree with ““books are a user interface to knowledge.”.\nThat’s what AI assistants are poised to become: “AI assistants will be a better user interface to knowledge.”\nThis is a short blog on how im using chatgpt to study and making use of spare time on journes to work.\n\n\n\nWhat is ChatGPT voice\nChatgpt voice is based on GPT 3.5 model and includes features giving that give it the ability to hear and speak.\nFor the voice feature, OpenAI uses Whisper, its speech recognition system, to transcribe a user’s spoken words into text and a new text-to-speech model that can generate human-like audio from text with just a few seconds of speech.\nI have a selection of 5 voices to choose from and have been using “sky” and it sounds really natural. You can still tell its a computer speech which I imagine is on purpose, it gets the pauses and tone, emotion right, im really impressed.\nIt available on the plus and enterprise edition, not the free edition of ChatGPT.\n\n\nWhats wrong with podcast and audiobooks\nI’ve been using ChatGPT voice conversations for over a month now (it’s still in beta) and for me its a game changer for studying and making the spare time useful on the journey to work and back.\nListening to podcasts or audiobooks can be enriching, yet they come with limitations. Sometimes they delve into topics that don’t interest you. At other times, they might present information in a convoluted manner. There’s also the issue of pacing: some podcasts assume advanced knowledge, leaving you lost, while others may cover familiar ground, leading to frustration\n\n\nwhy use ChatGPT voice\nChatGPT Voice offers a versatile learning experience by tapping into a wealth of internet knowledge. It can adapt its conversational style to mimic various tones, such as Shakespearean language or the distinctive styles of well-known educators like Richard Feynman and Jeremy Howard. While I haven’t personally tried these specific modes, they could resonate better with your learning preferences\n\n\nHow to use ChatGPT Voice\nDo you have a favourite educator? Ask GPT voice to give explanations like that educator, don’t understand something, ask it to explain it in a different way. Ask it to explain it to a 5 year old, a 10 year old, say what you understand and what you dont understand and for me, its filled in the gaps.\nAsk it to give you a topic and ask for a questions at the end to make sure you understand.\nWhat are the most important facts, dates, or formulas related to (topic)? Help me create a memorization technique to remember them easily.\nGive it a statement and ask chatgpt to give feedback, any corrections or improvements.\nAsk voice GPT to create models or analogies to help me understand and remember “optimization techinques in deep learning”.\nDo you have a difficult concept to understand, ask Chatgpt to Guide you through a visualization exercise to help me internalize the term optimization techinques and imagine yourself successfully applying it to a real-life situation. This has really helped me on difficult concepts, highly recommend you try it.\nMy favourite is to ask chatgpt voice “I want you to act as a Socrat and use the Socratic method to help me improve my critical thinking, logic, and reasoning skills. Your task is to ask open-ended questions to the statement ‘optimization techinques in deep learning’, give me constructive feedback to each response before you ask the next question.””\nAll conversations are saved in text format when your return to your computer and phone, you can review the conversation and save it to your notes or share it.\n\n\nOther Uses\nDo you have a important conversation coming up, talk it through with ChatGPT first to get your thoughts in order and ask for opionions on how other positions, react and respond and how to frame the conversation. You can have this conversation as many times as you want, it will never get bored, it will never get angry and its always available 24 hours a day 7 days a week, 365 days a year.\nPotential use cases:\n\nNegotiations\nInterviews\nSales\nPresentations\nConflict resolution\nDifficult conversations\nAsking for a raise\nAsking for a promotion\nAsking for a date\n\n\n\nThings that need improving\n\nSometimes Chatgpt voices cuts the converstation off half way throgh me saying something if i pause while im thinking about how to phrase something. I wish it would wait a bit longer before cutting off the conversation.\nSometimes I want to cut off what the chatgpt voice is saying because it/I haven’t fully explained the context. I wish it would stop speaking and go into listening mode or be able to do both.\nI wish it would give me a summary of the conversation at the end, it would be useful to have a summary of the conversation to help me remember what we talked about.\n\n\n\nConclusion\nI will be using this more and more, I think its a game changer for me, I can see myself using it. I’m really excited about the future of AI assistants and how they will help us learn and understand the world together. It will be interesting to know how bigger or better models will improve the experience. I’m sure there will be a lot of research in this area.\nTry ChatGPT, I think it will change the way you think about AI assistants and how you can use them to learn and understand together.\n\nReferences\n(“How to Use ChatGPT to Easily Learn Any Skill You Want” n.d.)\n\n\n\n\n\n\nReferences\n\n“How to Use ChatGPT to Easily Learn Any Skill You Want.” n.d. Accessed October 26, 2023. https://www.youtube.com/watch?v=MnDudvCyWpc&t=365s."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html",
    "href": "posts/Exploring_GTP4V_paper/index.html",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Hi, this is a explority view of GPT4 vision, study hacks and it’s uses in manufacturing and industry 4.0. Its a commentary of microsoft paper TheDawnofLMMs: PreliminaryExplorationswithGPT-4V(ision) picking out the highlights, my take on how GPT4 vision can be used in daily life, how to maximise my studying of the fast.ai course and what GPT4 vision means for the future of manufacturing and industry 4.0.\nThe paper and chatgpt vision was discussed with the Fast.AI study group. The study group is a multi-national group of people who are passionate about deep learning and AI. We meet online on Saturdays to run through the course work provided by fastAI, discuss papers and latest trends and get to know each other.\n\n\n\nGPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA.\n\n\n\n\n\nSomeone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog.\n\n\n\n\n\nIn the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more.\n\n\n\n\nDuring the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing.\n\n\n\n\nI beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks\n\n\n\n\n\nDefect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "GPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Someone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "href": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "In the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "href": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "During the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "href": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "I beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks"
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "href": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Defect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html",
    "href": "posts/future_skills_in_ai/index.html",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "",
    "text": "Manufacturing is a key sector for the UK economy, accounting for 10% of GDP and 44% of exports, the UK is the 8th largest exporter in the world, its important. In this article, we delve into the transformative impact of Artificial Intelligence (AI) on the manufacturing sector, a pivotal segment of the UK economy. Traditionally, manufacturing focused on materials and processes, but the advent of computing and data analytics has ushered in a new era where these technologies are central to the sector’s future. We explore how AI influences manufacturing jobs and the broader workforce, examining the creation of new roles and the evolution of existing ones, and how these changes are empowering the UK manufacturing sector to redefine its future.\nThe journey from the third to the potential fifth industrial revolution marks a significant transition. Starting from the initial integration of IT systems in the third revolution to the interconnected IoT systems in the fourth, and envisaging a future where AI and robotics seamlessly integrate in the fifth, we trace the trajectory of technological advancements in manufacturing. This evolution is not just about increased efficiency and productivity; it’s also about personalization, sustainability, and a significant shift in workforce dynamics and skills.\nThe current state of AI in manufacturing, while still developing, shows immense promise. From predictive maintenance to supply chain optimization, AI is beginning to redefine how manufacturing processes are conceived and executed. This article also shares a personal account of how AI has revolutionized work processes from an IT perspective within the manufacturing industry, highlighting the practical implications and benefits of these advancements.\nAs we analyze the economic aspects of AI in manufacturing, focusing on the increasing demand for GPUs and the implications for AI development, we observe a trend of growing accessibility and democratization of AI technologies. This trend is paving the way for more inclusive and widespread use of AI across various levels of the manufacturing industry.\nThe evolution of software in manufacturing, from traditional hand-coded systems to machine learning models and now to large pre-trained foundation models, represents a fundamental shift in how manufacturing processes are managed and optimized. We examine the current and future states of manufacturing software, emphasizing the critical role of AI in driving innovation and efficiency.\nFinally, we look at the emerging role of the AI engineer in manufacturing, a role that blends technical proficiency with visionary application. The unique challenges and requirements of the manufacturing sector, such as the need for specialized sensors and precise measurements, underscore the importance of customizing AI solutions or developing new models tailored to specific contexts. This article concludes by highlighting the ongoing journey of AI in manufacturing, a journey that is reshaping the workforce and the industry in profound ways."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#personal-experience-ais-impact-on-my-workflow",
    "href": "posts/future_skills_in_ai/index.html#personal-experience-ais-impact-on-my-workflow",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "Personal Experience: AI’s Impact on My Workflow",
    "text": "Personal Experience: AI’s Impact on My Workflow\nAs a IT worker in manufacturing, my work flow has change due to the advancements in AI. I’m a ahead of the rest of the business in terms of using AI to improve my workflow and results it gives to the business.\nI use to speak to the people in business (my customers), understanding their/the businesses requirements, and then building a solution and iterate between the two and maybe use a 3rd party for extra resources to finish the solution in a quicker time.\nThe approach I use has evolved. While I still communicate with colleagues, I now employ AI and data science techniques to extract and organize information from various structured and unstructured systems. This allows me to present data to the business in a more efficient and effective manner. Prior to recent technological advancements, managing this process alone wouldn’t have been cost-effective. Currently, I leverage these tools to distil information, which aids in making decisions or uncovering additional insights and requirements.\nWhen a solution is required, I’m now using AI to help me build the solution in record time and with better quality, giving the business a better return on their investment in me, effectivly increasing my worth. The AI models I’m using are general purpose models that have been trained on lots of data but im using them to build unique products and solutions that are a fit for our business. Building solutions using AI and building on existing foundation AI models is the future of manufacturing.\nOther people in the business will improve their work flow by using generalized AI models and systems to help them make decisions and automate processes their own processes. They will also get the benefit of the work I’m doing (IT department), as I am be able to build solutions to improve their processes.\nThis will free up their time to do more value added work. This will increase their worth to the business. This will increase the UK economy. This will increase the UK productivity."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#the-economics-of-ai-in-manufacturing",
    "href": "posts/future_skills_in_ai/index.html#the-economics-of-ai-in-manufacturing",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "The Economics of AI in Manufacturing",
    "text": "The Economics of AI in Manufacturing\nGraphics cards (tensor proceses) are the hardware that compute AI models. The sale of these cards gives a great insight into the future demand of AI, it’s one of the limiting factors in more models and end users using those models. GPU demand is outstripping supply and is likley to do for a long time. Companies are racing to train the latest models, updating existing models and providing compute for inference (querying models for results). All of this requires more GPU’s. Here’s some stats from [GPU economics] (https://www.latent.space/p/semianalysis) on the growth of GPU’s:\n\nNVIDIA is forecasted to sell over 3 million GPUs next year, about 3x their 2023 sales of about 1 million H100s.\nAMD is forecasting $2B of sales for their new MI300X datacenter GPU. They are also indirectly getting a boost from the work that companies like Modular and tiny are doing in making it easier to actually use these chips (will ROCm ever catch up?).\nGoogle’s TPUv5 supply is going to increase rapidly going into 2024.\nMicrosoft just announced Maia 100, a new AI accelerator built “with feedback” from OpenAI. and much more.\n\nHaving more compute will increase competition and drive down prices. This will allow more companies to use AI and more people to use AI. This will increase the demand for AI and the demand for AI workers."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#the-future-of-manufacturing-software",
    "href": "posts/future_skills_in_ai/index.html#the-future-of-manufacturing-software",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "The Future of Manufacturing Software",
    "text": "The Future of Manufacturing Software\nIn the manufacturing sector, Software 3.0 opens up new horizons. It allows for greater customization, efficiency in processes, and enhanced predictive capabilities for maintenance and supply chain management. The integration of AI with IoT and robotics leads to smarter, more connected factories."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#democratization-of-advanced-machine-learning-technologies-in-manufacturing",
    "href": "posts/future_skills_in_ai/index.html#democratization-of-advanced-machine-learning-technologies-in-manufacturing",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "Democratization of Advanced Machine Learning Technologies in Manufacturing",
    "text": "Democratization of Advanced Machine Learning Technologies in Manufacturing\nThe democratization of AI is a key driver of this evolution. The availability of foundation models and the ease of use of AI platforms are lowering the barrier to entry. This is enabling more people to leverage AI in their work, including those without extensive data science expertise. The result is a shift in the role of software engineers, from building models to consuming them. This is the rise of the AI engineer."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#over-view",
    "href": "posts/future_skills_in_ai/index.html#over-view",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "Over view",
    "text": "Over view\nIt used to be that you needed expert maths skills, data science and coding skills to be able to build and train models and then a lot of experience in all of the roles. You would need to know how to prepare data, how to train models, how to evaluate models, how to deploy models, how to monitor models, how to retrain models, how to update models, how to use models. You would need in depth knowledge of them all or more likely, be in a team where have, a data preparation expert, an expert in models for the different types of data, and architecture to be able to communicate between these roles and many more roles. A lot of the tooling improvements have enabled this to be done by one person or a very small team depending on how big the project is.\nThe most talked about model is ChatGPT. It’s a chatbot interface where you ask questions (prompt) and it will give you an answer. There are other less known models where you just ask what you want in text and it return in other forms such as images, text, video, audio, 3d models. Anyone who can use a computer can use these models. All it takes is understanding what models are available, what they do for you and how to get the best outputs for your inputs. This will be a key skill to have.\nFor more advance users/developers there are more specific solutions for producing code within a development environment like Microsoft copilot offers. There are over 15,000 off the shelf models from hugging face ranging from nlp, multimodal, computer vision, speech processing and many more. All these models have their weights release which means that they can be use as is or customize (fine-tuned) to your own organizational needs, no need to start from scratch. On hugging face, there are datasets ready to be used to train Neural network and spaces to allow you to use the models. This is where things start to get interesting."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#enter-the-ai-engieer",
    "href": "posts/future_skills_in_ai/index.html#enter-the-ai-engieer",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "Enter the AI engieer,",
    "text": "Enter the AI engieer,\nIn the rapidly evolving landscapes of Industry 4.0 and 5.0, the role of the AI engineer emerges as a pivotal element. This role is defined by a deep understanding of the problem domain, comprehensive knowledge of AI models and ecosystems, and the ability to apply this expertise to deliver tangible business outcomes. These outcomes can range from gaining new insights and automating tasks, to enhancing the quality of products. The AI engineer leverages the principles of what can be described as “software 3.0” along with software 1.0 and 2.0. Some specific applications of AI in manufacturing include:\n\nPredictive Maintenance: A car manufacturing company uses AI algorithms to analyze data from assembly line robots. The AI predicts when a robot’s components are likely to fail, allowing for maintenance before a breakdown occurs, thereby reducing production downtime.\nSupply Chain Optimization: A consumer electronics manufacturer employs AI to forecast demand for its products, adjusting production schedules and inventory levels accordingly. This AI-driven approach helps the company avoid overproduction and stock shortages.\nPersonalized Manufacturing: A furniture company uses AI to offer customers the ability to customize products online. The AI assists in translating these customizations into manufacturing instructions, enabling mass customization at scale.\nEnergy Efficiency: A steel plant uses AI to optimize its furnaces’ energy consumption. The AI system constantly analyzes production data and adjusts furnace operations to maximize energy efficiency while maintaining quality.\nQuality Control: A pharmaceutical company implements AI for real-time quality control. The AI system continuously analyzes images of drug capsules to identify and sort out defects, ensuring high-quality products.\nWorker Safety and Ergonomics: An AI system in a chemical plant monitors the workplace environment using sensors and cameras. It identifies potential safety hazards and suggests ergonomic improvements to reduce the risk of worker injuries.\nProcess Optimization: In a textile factory, AI algorithms analyze the entire production process, identifying bottlenecks and suggesting improvements to enhance throughput and reduce waste.\nMarket Analysis and Consumer Insights: An AI system helps a toy manufacturer analyze social media trends and consumer feedback to predict which toy designs are likely to be popular, guiding product development and marketing strategies.\nCollaborative Robotics (Cobots): In a packaging facility, cobots equipped with AI work alongside human employees, taking over repetitive tasks and reducing the physical strain on workers.\nCustomized AI Solutions: A bespoke AI solution is developed for a food processing company to monitor and optimize the ripening process of fruits, enhancing product quality.\nTraining and Simulation: AI-driven virtual reality simulations are used in an aerospace company to train assembly workers, reducing the learning curve and improving precision in complex tasks on high value products. This helps to reduce the cost of training and the cost of mistakes.\nEnvironmental Monitoring and Compliance: An AI system in a cement factory monitors emissions in real-time, ensuring compliance with environmental regulations and identifying areas for improvement.\nEnhancing Customer Experiences: An AI chatbot in a manufacturing company’s customer service department provides quick and accurate responses to customer inquiries, improving satisfaction.\nIntegrating IoT with AI: In a smart factory, AI algorithms analyze data from IoT sensors\n\nFoundation models like GPT-3/4, Claude, and Whisper, signify a shift in the AI paradigm. These models are accessible off-the-shelf via APIs, offering a streamlined path to implementation. Understanding the architecture of these models - their layers and structure - is crucial. The ability to employ a pre-trained model bypasses the need for extensive data collection and training, accelerating the deployment process.\nPutting these foundation models into production varies in complexity. It can be as simple as calling an API where no knowledge of infrastrucure is required. Or as intricate as running the models locally, serving high-volume predictions. Running locally and managing demands requires a keen understanding of GPU utilization, batching, and infrastructure expertise.\nThe emergence of AI engineers is reshaping the landscape of development. AI is becoming increasingly accessible to traditional software engineers, blurring the lines between machine learning engineers and AI engineers. The latter focuses more on consuming foundation model APIs and the former more on building models from scratch.\nThis shift has economic implications. The demand for AI far exceeds the supply of machine learning experts. Consequently, AI engineers are evolving from software engineers who acquire these specialized skills.\nThe AI engineering stack is defined by systems of reasoning and retrieval-augmented generation stacks. These connect models to diverse data sources, broadening the scope of application.\nAI UX is expanding beyond chatbots, introducing new modalities and interfaces. Building products with these foundation models requires a focus on delivering unique value, solving customer problems, and building trust.\nHowever, there’s a balance to be struck. While some scepticism towards AI is healthy, there’s also a tendency to unfairly blame AI for shortcomings. The hype generated by media and industry creators sets high expectations. It’s essential for AI engineers to remain grounded in real customer needs.\nThe impact of AI applications is already being felt in various sectors, demonstrating the positive influence of these technologies. For AI engineers, this era represents an unprecedented opportunity to innovate, explore, and shape the future of industries 4.0 and 5.0. Their role is not just about technical proficiency, but also about envisioning and realizing the potential of AI to transform industries and improve lives."
  },
  {
    "objectID": "posts/future_skills_in_ai/index.html#what-makes-manufacturing-different-to-other-sectors",
    "href": "posts/future_skills_in_ai/index.html#what-makes-manufacturing-different-to-other-sectors",
    "title": "AI’s Evolution in Manufacturing: Shaping Jobs and the Workforce of Tomorrow.",
    "section": "what makes manufacturing different to other sectors",
    "text": "what makes manufacturing different to other sectors\nUnderstanding the factory floor and products might not be possible with selecting off the shelf models. There are certain AI modalities that are particularly prominent or uniquely adapted to the industry’s needs. The general purpose models might require fine tuning or if thats not possible, models will be created from scratch.Some examples of unique AI modalities in manufacturing include:\n\nLaser Surface Velocimeters: These are used for non-contact speed and length measurement of objects moving on a production line. They are particularly valuable in industries where contact measurement could damage the product, such as in paper or textile manufacturing.\nMachine Vision Systems with Custom Sensors: While machine vision is used in various industries, manufacturing often requires highly specialized vision systems. These can include sensors for inspecting microelectronics at a microscopic level or for detecting minute defects in automotive parts, where standard vision systems would be inadequate.\nHigh-Precision Displacement Sensors (e.g., LVDTs - Linear Variable Differential Transformers): These are used for ultra-precise measurements in environments like CNC machining centers and for calibrating industrial robots. The precision required in these contexts is often significantly higher than in other sectors.\nSpecialized Chemical Sensors for Process Monitoring: For example, in the pharmaceutical or food manufacturing industries, highly sensitive sensors might be used to detect specific compounds or contamination levels that wouldn’t be relevant in other sectors.\nAcoustic Emission Sensors: These are used for condition monitoring and to detect early signs of machinery fatigue or failure, particularly in heavy-duty machinery. They can pick up ultrasonic frequencies generated by cracks or other structural failures not detectable by other means.\nInductive and Capacitive Sensors in Harsh Environments: In sectors like metal fabrication or mining, sensors need to withstand extreme temperatures, dust, and vibration. The design and deployment of these sensors are often unique to the harsh conditions of manufacturing.\nMagnetic Field Sensors in Electric Motor Manufacturing: These sensors are used for precise measurement and control in the production of electric motors, where understanding the magnetic field distribution is crucial.\nFiber Bragg Grating (FBG) Sensors in Composite Material Manufacturing: FBGs are used to monitor strain and temperature during the manufacturing of advanced composite materials, which is critical for ensuring the integrity and performance of the final product.\n\nIf your working in manfacturing you can use and leverage models using existing models, but theres might be senarios that make sense to either fine tune (transfer learning) or create your a new model from scratch and knowing when to do so is a skill in itself."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html",
    "href": "posts/initialization_neural_networks/index.html",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand the inner workings of each aspect of a neural network. This will allow me to create new/improve existing techniques and enable me to piece together the right neural network for the right task.\nThe primary objective was to encapsulate the LSUV initialization method within a callback structure, honing my skills in this programming paradigm. Mastery of callback-based coding is widely recognized for its benefits, including enhanced code readability and maintainability. Moreover, this approach significantly accelerates the experimentation process, a critical advantage in the field of deep learning.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.\n\n\n\n\nProper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.\n\n\n\n\nEach model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.\n\n\n\nThe aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : A chart showing a colour matrix of inactive neurons. The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are. 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Shows the means and standard devivations of the activations. Means close to zero but ideally should be close to 1 to train optimally.  - and finally the conclusion of the results\n\n\n\nThis code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n\n….click to expand code\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]"
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#why-am-i-writing-about-lsuv",
    "href": "posts/initialization_neural_networks/index.html#why-am-i-writing-about-lsuv",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand the inner workings of each aspect of a neural network. This will allow me to create new/improve existing techniques and enable me to piece together the right neural network for the right task.\nThe primary objective was to encapsulate the LSUV initialization method within a callback structure, honing my skills in this programming paradigm. Mastery of callback-based coding is widely recognized for its benefits, including enhanced code readability and maintainability. Moreover, this approach significantly accelerates the experimentation process, a critical advantage in the field of deep learning.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#why-initialization-model-weights-before-starting-the-optimization",
    "href": "posts/initialization_neural_networks/index.html#why-initialization-model-weights-before-starting-the-optimization",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "Proper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#lsuv-vs-other-weight-optimization-techniques",
    "href": "posts/initialization_neural_networks/index.html#lsuv-vs-other-weight-optimization-techniques",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "Each model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#the-following-sections-guide-you-through-the-code-along-with-comments-and-reflections-on-the-results",
    "href": "posts/initialization_neural_networks/index.html#the-following-sections-guide-you-through-the-code-along-with-comments-and-reflections-on-the-results",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "The aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : A chart showing a colour matrix of inactive neurons. The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are. 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Shows the means and standard devivations of the activations. Means close to zero but ideally should be close to 1 to train optimally.  - and finally the conclusion of the results"
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#setup-environment-loading-the-dataset-transforming-the-data-for-training",
    "href": "posts/initialization_neural_networks/index.html#setup-environment-loading-the-dataset-transforming-the-data-for-training",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "This code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n\n….click to expand code\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]"
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html",
    "href": "posts/the_makers_journey_unimaker/index.html",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "",
    "text": "Here’s a article based on a talk I gave on 8th September 2020 for the Sheffield University. The talk was about my journey as a maker and how I’ve been able to apply my skills and knowledge in the industrial environment at Tinsel Bridge. I’ve also included a video of the talk for those that prefer to watch rather than read."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#why-im-a-maker-at-heart",
    "href": "posts/the_makers_journey_unimaker/index.html#why-im-a-maker-at-heart",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Why I’m a Maker at Heart",
    "text": "Why I’m a Maker at Heart\nMy passion for making is rooted in the philosophy of makerspaces – the essence of learn-by-doing. This approach has always resonated with me, as it aligns with my personal experiences in various creative fields. From my days in comedy improvisation to my involvement in Toastmasters, I’ve always thrived in environments where learning and creativity happen spontaneously and collaboratively."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#the-sheffield-makerspace-experience",
    "href": "posts/the_makers_journey_unimaker/index.html#the-sheffield-makerspace-experience",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "The Sheffield Makerspace Experience",
    "text": "The Sheffield Makerspace Experience\nOver the past three years at the Sheffield Makerspace, I’ve discovered the true value of community learning. Here, knowledge and skills are shared organically, fostering an environment of mutual teaching and learning. Whether it’s soldering, designing circuits, coding, laser cutting, or using a bandsaw, there’s always an opportunity to learn something new or to share your expertise with others."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#turning-passion-into-profession",
    "href": "posts/the_makers_journey_unimaker/index.html#turning-passion-into-profession",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Turning Passion into Profession",
    "text": "Turning Passion into Profession\nMakerspaces aren’t just about hobbies; they’re breeding grounds for innovation and entrepreneurship. I’ve seen firsthand how a simple project can evolve into a business venture. Pimoroni, for instance, started as a project in the hackspace and has now grown into a renowned electronics company. This transformation from maker to entrepreneur is a journey that truly inspires me."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#the-iot-and-lorawan-exploration",
    "href": "posts/the_makers_journey_unimaker/index.html#the-iot-and-lorawan-exploration",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "The IoT and LoRaWAN Exploration",
    "text": "The IoT and LoRaWAN Exploration\nMy foray into IoT and specifically, LoRaWAN, has been a game changer, particularly in the challenging industrial environment at Tinsel Bridge. The ability of LoRaWAN to transmit data effectively, despite the electrical noise, has not only streamlined our processes but also opened up a world of possibilities in terms of industrial IoT applications."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#mentoring-in-the-sela-project",
    "href": "posts/the_makers_journey_unimaker/index.html#mentoring-in-the-sela-project",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Mentoring in the SELA Project",
    "text": "Mentoring in the SELA Project\nBeing involved in the SELA project as a mentor has been another enriching experience. Providing students with real industry problems and guiding them through the solution process has not only been fulfilling but also a learning experience for me. Witnessing their growth and their fresh, innovative approaches to problem-solving has been truly rewarding."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#conclusion",
    "href": "posts/the_makers_journey_unimaker/index.html#conclusion",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Conclusion",
    "text": "Conclusion\nMy journey in the world of makerspaces and IoT has been more than just about learning new technologies or skills. It’s about being part of a community that thrives on collaboration, creativity, and shared knowledge. From fostering friendships to enhancing communication and leadership skills, the experiences I’ve gathered have shaped me both professionally and personally. Whether it’s through making, mentoring, or exploring new technologies, the journey continues to be an exciting and fulfilling one."
  },
  {
    "objectID": "posts/WeightsAndBias/WeightsAndBias.html",
    "href": "posts/WeightsAndBias/WeightsAndBias.html",
    "title": "Weights and bias to keep track of training run",
    "section": "",
    "text": "The blog will run you through a simple training loop and help understand how to intergrate weights and bias into your training loops using the mini ai from Fast AI."
  },
  {
    "objectID": "posts/WeightsAndBias/WeightsAndBias.html#all-the-setup-code-from-part-1",
    "href": "posts/WeightsAndBias/WeightsAndBias.html#all-the-setup-code-from-part-1",
    "title": "Weights and bias to keep track of training run",
    "section": "all the setup code from part 1",
    "text": "all the setup code from part 1\nCreating the dataset\ncode calapsed as we have already covered this in a previous blog post but provided for convience.\n\n\nCode\n# install and import the necessary libraries\n!pip install datasets[vision]\n!pip install torcheval\n\n\nfrom datasets import load_dataset, Image\n\nimport os\nimport sys\nfrom datasets import Dataset, Image\nimport torch\n# creates a DataLoader object that can be used to iterate through the dataset\nfrom torch.utils.data import DataLoader\n# Creates a transform that converts the image to a tensor\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.transforms.functional import to_tensor, normalize\n\n\n# checks if the environment is local or remote\ndef check_if_local():\n    # Checking for common remote environment indicators\n    remote_indicators = ['COLAB_GPU', 'JUPYTERHUB_SERVICE_PREFIX']\n\n    # If any of the indicators are present, it's likely not a local environment\n    if any(indicator in os.environ for indicator in remote_indicators):\n        return False\n    else:\n        # Assuming local environment if none of the remote indicators are found\n        return True\n\n\n\n# checks if the environment is local or remote and sets the path accordingly\nif check_if_local() == False:\n    print('Running in a remote environment, mounting Google Drive...')\n    from google.colab import userdata\n    api_key = userdata.get('wandb_key')\n    from google.colab import drive\n    drive.mount('/content/drive')\n    data_science_folder = '/content/drive/MyDrive/Learning/data_science/'\n    sys.path.append(data_science_folder)\n    dataset_path = data_science_folder  + 'datasets_folder/gaze-points/work-laptop'\nelse :\n    print('Running in a local environment...')\n    # import datasets before sys.path.append to avoid conflict with local datasets\n    data_science_folder = 'G:\\My Drive\\Learning\\data_science'\n    sys.path.append(data_science_folder)\n    dataset_path = data_science_folder + \"\\\\datasets_folder\\gaze-points\\work-laptop\"\n\n# extracts screen coordinates from the filenames and stores in a list of tensors\nlabel_tensors = [torch.tensor([int(f.split('_')[-2]), int(f.split('_')[-1].split('.')[0])]) for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n# Divide the first column by 2560 and the second column by 1440\nlabel_tensors = [tensor.float() / torch.tensor([2560.0, 1440.0]) for tensor in label_tensors]\n\n# get the last 20 elements for testings purposes\n# label_tensors = label_tensors[:200]\n# gets a list of all images in a directory and stores in a list of strings\nimage_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n# image_files = image_files[:200]\n# create the dataset from the image files and labels\ndataset = Dataset.from_dict({\"image\": image_files}).cast_column(\"image\", Image())\n# create a new dictionary with the images and labels\n# i'm not happy with having to add the labels to the dataset after as it takes alot longer\n# but i'm not sure how to do it in the the from_dict method above.\nupdated_dataset_dict = {\"image\": dataset[\"image\"], \"label\": label_tensors}\nupdated_dataset = Dataset.from_dict(updated_dataset_dict)\nto_tensor = transforms.ToTensor()\ndef transform_images_with_stack(batch):\n    if \"image\" in batch:\n        # Convert all images in the batch to tensors and collect them in a list\n        images_tensor = torch.stack([to_tensor(image) for image in batch['image']])\n        images_tensor = normalize(images_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # usual means nad st deviations recomended for images\n        batch['image'] = images_tensor  # Replace the list of images with a stacked tensor\n    if \"label\" in batch:\n        # Convert all labels in the batch to tensors and collect them in a list\n        #labels_tensor = torch.stack([torch.tensor(label) for label in batch['label']])\n        labels_tensor = torch.stack([torch.tensor(label, dtype=torch.float32) for label in batch['label']])\n        batch['label'] = labels_tensor  # Replace the list of labels with a stacked tensor\n    return batch\n# Executes the transform on the dataset, the returning dataset[image] will be a tensor\nupdated_dataset_with_transform = updated_dataset.with_transform(transform_images_with_stack)\n# splits the dataset into a training and test set\n# the test set is 20% of the dataset\n# the training set is 80% of the dataset\nupdated_dataset_split = updated_dataset_with_transform.train_test_split(test_size=0.2)\nupdated_dataset_split[\"train\"][0][\"label\"].type()\nupdated_dataset_split[\"train\"][0][\"image\"].type()\nupdated_dataset_split[\"train\"][0][\"image\"]\nupdated_dataset_split"
  },
  {
    "objectID": "posts/WeightsAndBias/WeightsAndBias.html#data-loaders-following-fast-ai-method",
    "href": "posts/WeightsAndBias/WeightsAndBias.html#data-loaders-following-fast-ai-method",
    "title": "Weights and bias to keep track of training run",
    "section": "data loaders (following Fast ai method)",
    "text": "data loaders (following Fast ai method)\ndataset to dataloaders\ncode calapsed as we have already covered this in a previous blog post but provided for convience.\n\n\nCode\n# sets the batch size for the data loader\nbatch_size = 100\nfrom operator import itemgetter\nfrom torch.utils.data.dataloader import default_collate\n\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls[:2]\n\n    @classmethod\n    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n        f = collate_dict(dd['train'])\n        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f, **kwargs))\nupdated_dataset_split\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n\ndef collate_dict(ds):\n    get = itemgetter(*ds.features)\n    def _f(b): return get(default_collate(b))\n    return _f\ndls = DataLoaders.from_dd(updated_dataset_split, batch_size=batch_size, num_workers=0)\n\ndt = dls.train\n\nxb,yb = next(iter(dt))\nxb.shape\nyb.shape"
  },
  {
    "objectID": "posts/WeightsAndBias/WeightsAndBias.html#load-the-optimizer-and-learner",
    "href": "posts/WeightsAndBias/WeightsAndBias.html#load-the-optimizer-and-learner",
    "title": "Weights and bias to keep track of training run",
    "section": "Load the optimizer and learner",
    "text": "Load the optimizer and learner\ncode calapsed as we have already covered this in a previous blog post but provided for convience.\n\n\nCode\nfrom torcheval.metrics import MulticlassAccuracy\nimport fastcore.all as fc\nfrom functools import partial\n\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.datasets import *"
  },
  {
    "objectID": "posts/WeightsAndBias/WeightsAndBias.html#the-learner",
    "href": "posts/WeightsAndBias/WeightsAndBias.html#the-learner",
    "title": "Weights and bias to keep track of training run",
    "section": "The learner",
    "text": "The learner\ncode calapsed as we have already covered this in a previous blog post but provided for convience.\n\n\nCode\n# maybe i need to chang the loss.  do i need to consider the batch???\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()"
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#multi-attention-headers",
    "href": "posts/transformers_paper/paper_explanation.html#multi-attention-headers",
    "title": "Paper reading group - Attention is All You Need",
    "section": "Multi-attention headers",
    "text": "Multi-attention headers\nBefore we dive deeper, I want to introduce a couple more analogies to help you understand Multi-attention headers. Understanding the intuition behind multi-attention headers through analogies can be a good segway into further learning. They helped me, not only understand why using headers helps the model perform better but also helped retain the information. The multi-attention headers are made of Query, Key and Value and thats what we will dig into first. The overall purpose will make more sense once you go through the components section below.\n\nEasy Analogy for QKV Mechanism\nImagine you’re at a large dinner party, trying to follow the conversations around the table:\nQuery (Q): This is like you trying to understand a comment made by someone. You’re focused on this comment and trying to figure out its context and relevance to the conversation. Key (K): Think of every person at the table as holding a “key” to different pieces of information. Some of what they say will be more relevant to understanding the comment you’re focused on, and some less so. Value (V): The “value” is the actual content or meaning each person (key) can contribute to help you understand the comment in question. After deciding whose input is most relevant, you’ll pay more attention to what those few people are saying.\nThe transformer, like you in this scenario, uses the QKV mechanism to decide which parts of the input (the conversation) to pay attention to when trying to understand each piece (word or comment).\n\n\nEasy Analogy for Multiple Heads\nContinuing with the dinner party analogy, imagine now that you’re not just trying to understand the content of the conversation but also the emotional tone, who’s leading the conversation, and how topics are changing over time.\nHaving multiple heads is like having several versions of you at the party, each with a different focus. One version of you is trying to follow the main topic, another is paying attention to the emotional undercurrents, another is noting how the conversation topics shift, and so on. Each “version” of you can focus on different aspects of the conversation simultaneously, ensuring that you get a fuller understanding of what’s happening at the dinner party than you would if you were just trying to follow the main topic. In essence, the QKV mechanism with multiple heads allows the transformer to “attend” to a complex sequence (like a conversation) from multiple perspectives at once, ensuring it captures the rich, multifaceted nature of the data it’s processing."
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#input-encoder-components",
    "href": "posts/transformers_paper/paper_explanation.html#input-encoder-components",
    "title": "Paper reading group - Attention is All You Need",
    "section": "(input) encoder components",
    "text": "(input) encoder components\n\n\n\nencoder highlighted\n\n\n\nInput Embedding: This can be thought of as a dictionary, each word (or token) is referenced as a single vector. This also captures the semantic information of the words and their relationship to each other. This context information is important for the understanding of the meaning of each word is to each other.\n\nPositional Encodings: This is used to track the position of the word for the model to understand its relevance due to its position. The words are not inherently processed in order so keeping track of the order is important. This is an important part that allows the model to process in parallel.\n\nMulti-headed Attention: This is where the magic of the self-attention is performed. Each head will keep track of it’s relationship to other words and its semantic understanding. In the paper, there are 8 heads meaning there are 8 different perspectives for each word (or token) and their relationship to the other words and the great meaning. Each head is different due to the random weights assigned at the start, these weights are then updated during training to hone in on different perspectives that matter the most in reference to the training data.\n\nThe feed forward layer: before the forward feed, the heads are concatenated and sent to the feed foward layer for further transformations to improve the understanding of all the heads and gain a better conceptual understanding to improve the overall accuracy of the model.\nFinally, this is then passed to the decoder part of the model to aid prediction in its new form."
  },
  {
    "objectID": "posts/transformers_paper/paper_explanation.html#output-decoder-components",
    "href": "posts/transformers_paper/paper_explanation.html#output-decoder-components",
    "title": "Paper reading group - Attention is All You Need",
    "section": "(output) decoder components",
    "text": "(output) decoder components\n\n\n\ndecoder highlighted\n\n\n\nOutput Embeddings: This is the same as the input embeddings, there is no difference on this layer, only when the embeddings reach the masked layers. The embeddings can be thought of as a dictionary, each word (or token) is referenced as a single vector. This also captures the semantic information of the words and their relationship to each other. This context information is important for the understanding of the meaning of each word is to each other.\n\nPositional Encodings: This is used to track the position of the word for the model to understand its relevance due to its position. The words are not inherently processed in order so keeping track of the order is important. This is an important part that allows the model to process in parallel.\n\nMasked multi-head attention: Its aim is similar to the multi-headed attention in the encoder but to iteratively learn the relationship and semantic meanings one token at a time to predict the next token. For example, if the sentence to translate is “you are the best” to French, the first token is “you”, then the 2nd word “you are”, 3rd word “you are the” and 4th “you are the best” each time asking “given what you know so far, whats the next token”. This is how the masking process works but this can be done in parallel.\nMulti-headed Attention: This again is where this gets really interesting. This layer takes the whole semantic meaning from the encoder of the whole input tokens and also the masked multi-head attention (up to the point it’s at e.g. “you Are”) to predict the next token. It applies several heads to pay attention to different aspects of the information passed.\n\nThe feed forward layer: before the forward feed, the heads are concatenated and sent to the feed foward layer for further transformations to improve the understanding of all the heads and gain a better conceptual understanding to improve the overall accuracy of the model.\nLinear Layer: The linear layers gain further understanding and produce the logits to pass to the softmax function\nSoftmax function: The softmax function converts the logits to a probability and the highest probability word is selected."
  }
]