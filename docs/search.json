[
  {
    "objectID": "posts/initialization_neural_networks/index.html",
    "href": "posts/initialization_neural_networks/index.html",
    "title": "The importantance of propper initializtaion",
    "section": "",
    "text": "Why am I writing about LSUV?\nI’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand inner workings of each aspect of a neural network. This will allow me to create new techniques and improve existing techniques and enable me to piece together the right neural network for the right task.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.\n\n\nWhy initialization model weights before starting the optimization\n\nProper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.\n\n\n\nLSUV vs other weight optimization techniques\nEach model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.\n\n\nThe following sections guide you through the code along with comments and reflections on the results\nThe aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are, 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Means close to zero but standard deviations far off expected goal of 1, to far from 1 to train optimally.   - and finally the conclusion of the results\n\n\nSetup environment, loading the dataset, transforming the data for training\nThis code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n\n….click to expand code\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n\n\n\n\nFind the optimal learning rate\nThe Learning Rate Finder is a tool designed to help find a good learning rate for training deep learning models. It increases the learning rate after each mini-batch and records the loss. As the learning rate increases, initially, the loss will decrease (as the model learns). But after a certain point, the learning rate might be too high causing the loss to increase due to overshooting the optimal weights. The usual method is to choose the best learning rate is to choose a figure just before the steep fall.\n\n\nFrom looking at the chart, it looks like the best learning rate is going to be between 10^-2 and 10^-1…. click to expand code\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n\nlrfind = LRFinderCB()\ncbs = [TrainCB(), DeviceCB(), lrfind]\n\n# fits data\ndef fit(model, epochs=1, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.0000001, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n# conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)\n# along with ni (features) input channels and output channels (feature maps)\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n#\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\nmodel = nn.Sequential(*cnn_layers())\n\nfit(model);\n\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\n\n\nLearning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).\n\n\n\n\n\n\nExample of poorly initialized model (No input or weight initialization)\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.   Key technical information for this section :   Conv 1,8,16,32,64 -&gt; 10 : A Convolution neural network showing the number of filters in each layer and ending with 10 output units.  Activation is nn.ReLU : ReLU function introduces non-linearity to the model.  Data normalisation : None  weight normalization : None \n\n\nThe chart loss shows the training was learning over the 1st 50 batches and flatlined, the first 50 batches didnt see much improvement, further tools are needed to improve the learning process …. click to expand code\n# improved function to include labelling for the stats\nclass ActivationStats(HooksCallback):\n    def __init__(self, mod_filter=fc.noop):\n        super().__init__(append_stats, mod_filter)\n\n    def color_dim(self, figsize=(11,5)):\n      fig, axes = get_grid(len(self), figsize=figsize)\n      for ax, h in zip(axes.flat, self):\n          im = ax.imshow(get_hist(h), origin='lower')  # Using imshow directly\n\n          # Add labels, title, and colorbar for clarity\n          ax.set_xlabel(\"Batch Number\")\n          ax.set_ylabel(\"Activation Value\")\n          ax.set_title(\"Layer \" + \"str(self.index(h))\" + \" Activations\")\n          cbar = plt.colorbar(im, ax=ax)\n          cbar.set_label(\"Frequency\")\n      plt.tight_layout()  # Prevent overlap\n\n\n    def dead_chart(self, figsize=(11,5)):\n        fig, axes = get_grid(len(self), figsize=figsize)\n        for ax, h in zip(axes.flatten(), self):\n            ax.plot(get_min(h), linewidth=3)\n            ax.set_ylim(0,1)\n            ax.set_xlabel(\"Batch Number\")\n            ax.set_ylabel(\"Activation Value\")\n            ax.set_title(\"Layer \" + \"str(self.index(h))\" + \" Dead Activations\")\n        plt.tight_layout()  # Prevent overlap\n\n    def plot_stats(self, figsize=(10,4)):\n        fig, axs = plt.subplots(1,2, figsize=figsize)\n        for h in self:\n            for i in 0,1:\n                axs[i].plot(h.stats[i])\n        axs[0].set_title('Means')\n        axs[1].set_title('Stdevs')\n        axs[0].set_xlabel(\"Batch Number\")\n        axs[1].set_xlabel(\"Batch Number\")\n        axs[0].set_ylabel(\"Mean Activation Value\")\n        axs[1].set_ylabel(\"Standard Deviation of Activation Value\")\n        plt.legend(fc.L.range(self))\n        #plt.tight_layout()  # Prevent overlap\n\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n\n# setup model for learning\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n\n# fits dataset\ndef fit(model, epochs=3, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.2, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n# conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)\n# along with ni (features) input channels and output channels (feature maps)\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n#\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\nmodel = nn.Sequential(*cnn_layers())\n\n#astats = ActivationStats(fc.risinstance(GeneralRelu))\nastats = ActivationStats(fc.risinstance(nn.ReLU))\n\nfit(model, xtra_cbs=[astats]);\n\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.188\n2.247\n0\ntrain\n\n\n0.196\n2.225\n0\neval\n\n\n0.189\n2.237\n1\ntrain\n\n\n0.196\n2.225\n1\neval\n\n\n0.189\n2.237\n2\ntrain\n\n\n0.196\n2.224\n2\neval\n\n\n\n\n\n\n\n\n\n\nThe color represents the frequency of activations in a specific range. Using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently for that batch….. click to expand code\nastats.color_dim()\n\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training…. click to expand code\nastats.plot_stats()\n\n\n\n\n\nPlots of means and standard deviations for each layer\n\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result…. click to expand code\nastats.dead_chart()\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nImprove the model by input normalization\nTo give the optimize algorithm every chance converge quicker, normalization the inputs to a mean of zero and standard deviation of 1 will help. This can be done alone or with normalization the weights too. The following section discusses normalization the inputs alone.\nThe key to this is to ensure that each feature contributes equally to the learning process, which is especially important when the features have different units or different scales.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input normalization (also know as feature scaling). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : This denotes a Convolutional Neural Network with varying numbers of filters across different layers, culminating in 10 output units.  Activation is nn.ReLU : The model utilizes the ReLU (Rectified Linear Unit) activation function to introduce non-linearity, aiding in better approximations of complex functions.\n\n\nSetting up the data without input initialization, you can see the mean is not near 0 and the standard deviation is not near 1 so not a optimal setup….. click to expand code\nxl,yl = 'image','label'\n\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\n\n#| echo: false\n#| output: false\n\nxmeans = []\nxstds = []\n\nfor xb, yb in iter(dls.train):\n    xmean, xstd = xb.mean(), xb.std()\n    xmeans.append(xmean.item())\n    xstds.append(xstd.item())\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(xmeans, label='xmean')\nplt.title('X Mean over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Mean')\n\nplt.subplot(1, 2, 2)\nplt.plot(xstds, label='xstd')\nplt.title('X Std Dev over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Std Dev')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nChart showing mean and standard deviation (y axis) over batches (x axis)\n\n\n\n\n\n\nThis time with input initialization, you can see the mean is now near 0 and the standard deviation near 1 so it should help the model optimize quicker….. click to expand code\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\n\n\nimport matplotlib.pyplot as plt\n\nxmeans = []\nxstds = []\n\nfor xb, yb in iter(dls.train):\n    xmean, xstd = xb.mean(), xb.std()\n    xmeans.append(xmean.item())\n    xstds.append(xstd.item())\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(xmeans, label='xmean')\nplt.title('X Mean over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Mean')\n\nplt.subplot(1, 2, 2)\nplt.plot(xstds, label='xstd')\nplt.title('X Std Dev over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Std Dev')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nChart showing mean and standard deviation (y axis) over batches (x axis)\n\n\n\n\n\n\nThe code sets up a deep learning pipeline for training a CNN model on a dataset. It defines transformations for the dataset, specifies data loaders, metrics, and callbacks, and then creates a CNN model with specific convolutional layers before fitting the model using a learner object with a ratee of lr=0.2, and includes activation statistics. it shows the model has flat lined at an accuracy of 0.19 and loss of 2.22, its likley that the model wont improve much with further training so other methods will be required to improve training further….. click to expand code\n# slightly better than last time but definatly not perfect&gt;\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[astats]);\n\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.185\n2.248\n0\ntrain\n\n\n0.196\n2.225\n0\neval\n\n\n0.189\n2.237\n1\ntrain\n\n\n0.196\n2.224\n1\neval\n\n\n0.189\n2.237\n2\ntrain\n\n\n0.196\n2.224\n2\neval\n\n\n\n\n\n\n\n\n\n\nThe color represents the frequency of activations in a specific range. Using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently for that batch, this is still not looking good,as we have more yellow (more activations at the lower end of the scale) so we need to investigate other methods to help keep those neurons contributing to the end results…… click to expand code\nastats.color_dim()\n\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, we’re far off that for the target standard deviations but they have improved very slighly, looks like the graph has raise by about 0.12. These figures would explain why the training did not improve with further epochs…. click to expand code\n# stanard deviations still away from one but mean looks reasonable\nastats.plot_stats()\n\n\n\n\n\nPlots of means and standard deviations for each layer\n\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result, this has improve slightly on the 1st layer but still not great, lots of neurons not contributing to the end prediction…. click to expand code\n# first layer quite bad and last layer is totally dead.\nastats.dead_chart()\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nBatch Normalization with Leaky ReLU activation and Kaiming normalization.\n\nWe are now changing 3 things. First is batch normalisation to control the inputs between each of layers, Leaky ReLU to keep more neurons alive so they contribute to the end result and finally weight initialization.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units.  Activation is Leaky ReLU : We will now use Leaky ReLU (instead of ReLu) as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e. it passes a positve number for each of the activations.  Data normalisation: is BatchNorm : This will help calculate the data after each activation layer???  Learning Rate 0.2 : Initial training was conducted with a learning rate of 0.2.  Best Training So Far: This version of the model has shown the best training results compared to previous iterations.\nWhat to Try Next : The section concludes with open questions and suggestions for future experiments to further enhance model performance. The use of batch normalization and Leaky ReLU has led to improved training dynamics. The next aim is to implement LSUV as call back\n\n\nTransforms data from source import, plot functions and show a plot of Leaky ReLU….. click to expand code\n# transform dataset from source dsd\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n#| Avoiding inactive Neurons: Leaky ReLU helps to mitigate the problem of \"inactive neurons\" that can occur with ReLU units,\n# where neurons get stuck during training and always output a zero value. By allowing a small, non-zero output for negative inputs,\n# Leaky ReLU ensures that gradients can still flow through the neuron, which can help to keep learning progressing.\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x):\n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n#| export\ndef plot_func(f, start=-5., end=5., steps=100):\n    x = torch.linspace(start, end, steps)\n    plt.plot(x, f(x))\n    plt.grid(True, which='both', ls='--')\n    plt.axhline(y=0, color='k', linewidth=0.7)\n    plt.axvline(x=0, color='k', linewidth=0.7)\n# visual representation of the new relu, left values Jeremeys example\nplot_func(GeneralRelu(leak=0.1, sub=0.4))\n\n\n\n\n\nplot of leaky, always passes through a positive value\n\n\n\n\n\n\nSets up the model, initializes weights based on kaiming_normal with batch norm and runs fit. The training shows massive improvement, dropping sharply over the first 50 batches, after that the rate of change decreases buts its still learning. for future experiments, we could just carry on with the training process for more epochs as its still learning or we could change a few things up ….. click to expand code\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n    if bias is None: bias = not isinstance(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))\n    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n    if norm: layers.append(norm(nf))\n    if act: layers.append(act())\n    return nn.Sequential(*layers)\n#| initializes weights based on kaiming_normal_\ndef init_weights(m, leaky=0.):\n    # checks for a instance of layer and module of the neural network\n    # checks for a instance of 1d, 2d, 3d neural network\n    #\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)):\n      # creates the initialization of the weights, for a, anything that is not zero, standard relu is assumed.\n      init.kaiming_normal_(m.weight, a=leaky)\n# Creates a function based on relu with the parameters already applied\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n\n# Creates a function based on leaky being 0.1\niw = partial(init_weights, leaky=0.1)\n# Returns a instance of a model\n#\ndef get_model(act=nn.ReLU, nfs=None, norm=None):\n    # stores convolutions if not passed for later creation\n    if nfs is None: nfs = [1,8,16,32,64]\n    # Creates convolutions based on conv function for each of the layers in nfs\n    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)\n# collects mean and standard deviations of of each layer thats a ReLu\n# astats = ActivationStats(fc.risinstance(nn.ReLU))\nastats = ActivationStats(fc.risinstance(GeneralRelu))\n# addeds all call backs into a list for later use.\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nset_seed(42)\n# Creates instance of the model and then applys kaiming_normal to the weights\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n# Creates a instance of the learner function\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.599\n1.220\n0\ntrain\n\n\n0.799\n0.680\n0\neval\n\n\n0.840\n0.532\n1\ntrain\n\n\n0.835\n0.558\n1\neval\n\n\n0.867\n0.444\n2\ntrain\n\n\n0.830\n0.559\n2\neval\n\n\n\n\n\n\n\n\n\n\nThe color represents the frequency of activations in a specific range. Using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently for that batch, this is looking really good,as we have more yellow (more activations all across the scale) so im happy with the results…… click to expand code\nastats.color_dim()\n\n\n\n\n?????? ….. click to expand code\nastats.plot_stats()\n\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting asthe standard deviations are lower than 1 but the training went really well, would getting these closer to 1 help?????…. click to expand code\n\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result, this this is much better, they are all quite low so roughly 80% are contributing to the result (prediction)…. click to expand code\nastats.dead_chart()\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nimplement LSUV initialization as a class\nThis new model is more basic than the first model. Theres no data normalization but we’re keeping Leaky ReLU and changing the weight initialization to a custom LSUV callback.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units.  Activation is Leaky ReLU : We will now use Leaky ReLU as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e. it passes a positve number for each of the activations.  Data normalisation : none  weight normalization : LSUV  Learning Rate 0.2 : Initial training was conducted with a learning rate of 0.2.  What to Try Next : Weight initilization without input (data) normalization gave good results. It would be a good experiment to add some input (data) normalization like batch normalization. However, the experiment has now finished and im happy with the results here.\n\n\nThe code tranforms the data, Sets up the model with LSUV initilzing the weights and setting up Leaky ReLU. The training shows massive improvement over using no initialization, it starts off 50 batches but doesnt learn as quick as ‘Batch Normalization with Leaky ReLU activation and Kaiming normalization’. However, its still learning so we could just carry on with the training process but judging by previous experiments, its not optimal, or we could try LSUV with a input normalization too….. click to expand code\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=1)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n# This class implements Layer-Sequential Unit-Variance Initialization (LSUV), a technique used to\n# initialize the weights and biases of neural networks. LSUV aims to set these parameters such that\n# during the forward pass, the variance of the activations remains close to 1. This avoids issues\n# commonly associated with poor initialization, such as vanishing or exploding gradients.\n# To achieve this, the class modifies the initial weights and biases in the context of a sample of input\n# data, targeting a specified range for hardware/software-specific floating-point representation. This approach\n# minimizes the risk of exceeding the numerical range, which can lead to unstable training dynamics, or\n# put anotherway reduces the number of neurons contributing (deactivate) and the weight into the final result.\n# Key methods within this class handle the adjustment of weights and biases, based on the calculated\n#  variances and means of the activations. This is typically invoked at the beginning of the training\n# process, prior to the main training loop.\n\n# Note: Initial tests have shown effective results, although chart visualizations may\n# require further refinement.\n\nclass LSUVStatsHook(Callback):\n  # initialize and store all relevent details to object\n  def __init__(self, modules_for_hooks, modules_for_weights, verbose=False, debug=False):\n    self.mean = None\n    self.std = None\n    self.log = fc.noop if not verbose else print\n    self.debug = debug\n    #fc.store_attr()\n    if self.debug : import pdb; pdb.set_trace()\n    self.modules_for_hooks = modules_for_hooks\n    self.modules_for_weights = modules_for_weights\n\n  # update hooks\n  def hook(self, module, input, output):\n    #import pdb;pdb.set_trace()\n    acts = output.detach().cpu()\n    self.mean = acts.mean()\n    self.std = acts.std()\n\n  # apply hooks to relus, update weights and bias to convs\n  def calc_apply_LSUV_weights_bias(self, learn, batch_of_data):\n    # Get all of the modules that will be used for calculating the  lsuv\n    if self.debug : import pdb; pdb.set_trace()\n    self.log(\"self.modules_for_hooks is type\", self.modules_for_hooks)\n    self.log(\"GeneralRelu is type \" , GeneralRelu)\n    modules_to_apply_hooks = [o for o in learn.model.modules() if isinstance(o, self.modules_for_hooks)]\n    self.log(\"modules to apply hooks to: \", modules_to_apply_hooks)\n    module_to_update_weights = [o for o in learn.model.modules() if isinstance(o, self.modules_for_weights)]\n\n    # Update the weights and bias's util desired range is achieved\n    if self.debug : import pdb; pdb.set_trace()\n    no_of_layers = len(modules_to_apply_hooks)\n    for item in range(no_of_layers):\n      self.log(\"entering layer : \", item)\n      handle = modules_to_apply_hooks[item].register_forward_hook(hook_LUSV.hook)\n      with torch.no_grad():\n        while learn.model(batch_of_data) is not None and (abs(hook_LUSV.std-1)&gt;1e-3 or abs(hook_LUSV.mean)&gt;1e-3):\n          self.log(\"update weights to modules: \",  module_to_update_weights[item])\n          module_to_update_weights[item].bias -= hook_LUSV.mean\n          module_to_update_weights[item].weight.data /= hook_LUSV.std\n          self.log(\"standard deviation is :\", hook_LUSV.std)\n          self.log(\"mean is :              \", hook_LUSV.mean)\n    # deregister the hook\n    handle.remove()\n\n  # calls calc_apply_LSUV_weights_bias to update weights and bias's\n  def before_fit(self, learn):\n    if self.debug : import pdb; pdb.set_trace()\n    LSUVStatsHook.calc_apply_LSUV_weights_bias(self, learn, batch_of_data=xb)\n\n# Custom callback with some debugging code commented out\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        #import pdb; pdb.set_trace()\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        #import pdb; pdb.set_trace()\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        #import pdb; pdb.set_trace()\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n        #print(f\"loss : {learn.loss}, weight : {len(x)}\")\n\n# Load the metrics\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\n\n# module for a custom Relu\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x):\n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n\n# setup the LSUV hook to pass to the model\nhook_LUSV = LSUVStatsHook(modules_for_hooks = GeneralRelu, modules_for_weights = nn.Conv2d,verbose=False,debug=False)\n\n# setup the activation statics and module\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nastats = ActivationStats(fc.risinstance((GeneralRelu, nn.ReLU)))\n\n# setup the model and call fit\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, act())\n    return res\n\ndef get_model(act=nn.ReLU, nfs=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None), nn.Flatten()).to(def_device)\n\nmodel = get_model(act_gr)\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, hook_LUSV]\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n\nlearn.fit(3)\n\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.461\n1.575\n0\ntrain\n\n\n0.693\n1.012\n0\neval\n\n\n0.758\n0.783\n1\ntrain\n\n\n0.754\n0.817\n1\neval\n\n\n0.811\n0.622\n2\ntrain\n\n\n0.799\n0.681\n2\neval\n\n\n\n\n\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result, this acceptable they are all quite low so roughly 80% are contributing to the result (prediction) except from at initial batches…. click to expand code\nastats.dead_chart()\n\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nThe color represents the frequency of activations in a specific range. Using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently for that batch, this is looking really good,as we have more yellow (more activations all across the scale) so im happy with the results apart from the start…… click to expand code\nastats.color_dim()\n\n\n\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\n\n?????? ….. click to expand code\nastats.plot_stats()\n\n\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting as this really shows training got to the optimal weights after batch 75, layer 1 jumped up to 1 at batch 0, layer 2 jumped upto 1 at 25, layer 3 jumped up 1 at batch 50 and layer 3 jumped upto 75 so 25 between each of the layers. …. click to expand code"
  },
  {
    "objectID": "useful_info.html",
    "href": "useful_info.html",
    "title": "Commands to run quarto",
    "section": "",
    "text": "Commands to run quarto\nto create a project (run once at the beginning to create the project files) - quarto create project_name –template=blog\nto run a preview locally - quarto preview .\nto Create update the files (run before pushing to github) - quarto render .\n\n\nlinks :\n\nVideo of somone showing how to use quarto https://www.youtube.com/watch?v=nllKcuX7rEc\nilovetensoor coppied a good link here : https://discord.com/channels/689892369998676007/1096981866248147014/1167453441291984936\nexample blog http://vishalbakshi.com/blog/posts/2023-10-09-nn-rf-embeddings/\nexample blog https://ilovetensor.tech/posts/neural-nets-from-scratch/\n\n\n\nquarto reference page :\nhttps://quarto.org/docs/reference/cells/cells-jupyter.html\n\n\nsetup for using colab with quarto\n\nCreate a new notebook in colab and add markup in markup - no need for RAW as documentation says.\ninstall the drive app in windows which will reveal the drive folder in the file explorer, i used streams so no files are stored locally.\nCreate a script to copy file to local installation and run “quarto render .” to create the html file. see file located C:projects_lsuv_from_colab.bat\nEdit the notebook in colab and run the script to render the html file, repeat until happy with the result and publish to github."
  },
  {
    "objectID": "posts/chatGPT_audio_conversation/index.html",
    "href": "posts/chatGPT_audio_conversation/index.html",
    "title": "Maximizing Your Commute: Learning on the Go with ChatGPT Voice",
    "section": "",
    "text": "Overview\nI have 20 minute drive into work and back home everyday where I usually listen to a podcast, audio book or sometimes just stair at the road. I’m always thinking, how can I make better use of my time…. I saw a twitter post from Le Cunn, the legendary Data Scientist from Meta and that i fearously agree with ““books are a user interface to knowledge.”.\nThat’s what AI assistants are poised to become: “AI assistants will be a better user interface to knowledge.”\nThis is a short blog on how im using chatgpt to study and making use of spare time on journes to work.\n\n\n\nWhat is ChatGPT voice\nChatgpt voice is based on GPT 3.5 model and includes features giving that give it the ability to hear and speak.\nFor the voice feature, OpenAI uses Whisper, its speech recognition system, to transcribe a user’s spoken words into text and a new text-to-speech model that can generate human-like audio from text with just a few seconds of speech.\nI have a selection of 5 voices to choose from and have been using “sky” and it sounds really natural. You can still tell its a computer speech which I imagine is on purpose, it gets the pauses and tone, emotion right, im really impressed.\nIt available on the plus and enterprise edition, not the free edition of ChatGPT.\n\n\nWhats wrong with podcast and audiobooks\nI’ve been using ChatGPT voice conversations for over a month now (it’s still in beta) and for me its a game changer for studying and making the spare time useful on the journey to work and back.\nListening to podcasts or audiobooks can be enriching, yet they come with limitations. Sometimes they delve into topics that don’t interest you. At other times, they might present information in a convoluted manner. There’s also the issue of pacing: some podcasts assume advanced knowledge, leaving you lost, while others may cover familiar ground, leading to frustration\n\n\nwhy use ChatGPT voice\nChatGPT Voice offers a versatile learning experience by tapping into a wealth of internet knowledge. It can adapt its conversational style to mimic various tones, such as Shakespearean language or the distinctive styles of well-known educators like Richard Feynman and Jeremy Howard. While I haven’t personally tried these specific modes, they could resonate better with your learning preferences\n\n\nHow to use ChatGPT Voice\nDo you have a favourite educator? Ask GPT voice to give explanations like that educator, don’t understand something, ask it to explain it in a different way. Ask it to explain it to a 5 year old, a 10 year old, say what you understand and what you dont understand and for me, its filled in the gaps.\nAsk it to give you a topic and ask for a questions at the end to make sure you understand.\nWhat are the most important facts, dates, or formulas related to (topic)? Help me create a memorization technique to remember them easily.\nGive it a statement and ask chatgpt to give feedback, any corrections or improvements.\nAsk voice GPT to create models or analogies to help me understand and remember “optimization techinques in deep learning”.\nDo you have a difficult concept to understand, ask Chatgpt to Guide you through a visualization exercise to help me internalize the term optimization techinques and imagine yourself successfully applying it to a real-life situation. This has really helped me on difficult concepts, highly recommend you try it.\nMy favourite is to ask chatgpt voice “I want you to act as a Socrat and use the Socratic method to help me improve my critical thinking, logic, and reasoning skills. Your task is to ask open-ended questions to the statement ‘optimization techinques in deep learning’, give me constructive feedback to each response before you ask the next question.””\nAll conversations are saved in text format when your return to your computer and phone, you can review the conversation and save it to your notes or share it.\n\n\nOther Uses\nDo you have a important conversation coming up, talk it through with ChatGPT first to get your thoughts in order and ask for opionions on how other positions, react and respond and how to frame the conversation. You can have this conversation as many times as you want, it will never get bored, it will never get angry and its always available 24 hours a day 7 days a week, 365 days a year.\nPotential use cases:\n\nNegotiations\nInterviews\nSales\nPresentations\nConflict resolution\nDifficult conversations\nAsking for a raise\nAsking for a promotion\nAsking for a date\n\n\n\nThings that need improving\n\nSometimes Chatgpt voices cuts the converstation off half way throgh me saying something if i pause while im thinking about how to phrase something. I wish it would wait a bit longer before cutting off the conversation.\nSometimes I want to cut off what the chatgpt voice is saying because it/I haven’t fully explained the context. I wish it would stop speaking and go into listening mode or be able to do both.\nI wish it would give me a summary of the conversation at the end, it would be useful to have a summary of the conversation to help me remember what we talked about.\n\n\n\nConclusion\nI will be using this more and more, I think its a game changer for me, I can see myself using it. I’m really excited about the future of AI assistants and how they will help us learn and understand the world together. It will be interesting to know how bigger or better models will improve the experience. I’m sure there will be a lot of research in this area.\nTry ChatGPT, I think it will change the way you think about AI assistants and how you can use them to learn and understand together.\n\nReferences\n(“How to Use ChatGPT to Easily Learn Any Skill You Want” n.d.)\n\n\n\n\n\n\nReferences\n\n“How to Use ChatGPT to Easily Learn Any Skill You Want.” n.d. Accessed October 26, 2023. https://www.youtube.com/watch?v=MnDudvCyWpc&t=365s."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html",
    "href": "posts/Exploring_GTP4V_paper/index.html",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Hi, this is a explority view of GPT4 vision, study hacks and it’s uses in manufacturing and industry 4.0. Its a commentary of microsoft paper TheDawnofLMMs: PreliminaryExplorationswithGPT-4V(ision) picking out the highlights, my take on how GPT4 vision can be used in daily life, how to maximise my studying of the fast.ai course and what GPT4 vision means for the future of manufacturing and industry 4.0.\nThe paper and chatgpt vision was discussed with the Fast.AI study group. The study group is a multi-national group of people who are passionate about deep learning and AI. We meet online on Saturdays to run through the course work provided by fastAI, discuss papers and latest trends and get to know each other.\n\n\n\nGPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA.\n\n\n\n\n\nSomeone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog.\n\n\n\n\n\nIn the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more.\n\n\n\n\nDuring the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing.\n\n\n\n\nI beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks\n\n\n\n\n\nDefect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "GPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Someone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "href": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "In the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "href": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "During the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "href": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "I beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks"
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "href": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Defect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "docs/posts/initialization_neural_networks/index.html",
    "href": "docs/posts/initialization_neural_networks/index.html",
    "title": "The importantance of propper initializtaion",
    "section": "",
    "text": "Why am I writing about LSUV?\nI’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand inner workings of each aspect of a neural network. This will allow me to create new techniques and improve existing techniques and enable me to piece together the right neural network for the right task.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.\n\n\nWhy initialization model weights before starting the optimization\n\nProper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.\n\n\n\nLSUV vs other weight optimization techniques\nEach model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.\n\n\nThe following sections guide you through the code along with comments and reflections on the results\nThe aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are, 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Means close to zero but standard deviations far off expected goal of 1, to far from 1 to train optimally.   - and finally the conclusion of the results\n\n\nSetup environment, loading the dataset, transforming the data for training\nThis code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n\n\nFind the optimal learning rate\nThe Learning Rate Finder is a tool designed to help find a good learning rate for training deep learning models. It increases the learning rate after each mini-batch and records the loss. As the learning rate increases, initially, the loss will decrease (as the model learns). But after a certain point, the learning rate might be too high causing the loss to increase due to overshooting the optimal weights. The usual method is to choose the best learning rate is to choose a figure just before the steep fall.\n\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n\nlrfind = LRFinderCB()\ncbs = [TrainCB(), DeviceCB(), lrfind]\n\n# fits data\ndef fit(model, epochs=1, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.0000001, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n# conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)\n# along with ni (features) input channels and output channels (feature maps)\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n#\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\nmodel = nn.Sequential(*cnn_layers())\n\nfit(model);\n\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\nLearning rate finder graph, the graph shows the relationship between learning rate (x-axis) and loss (y-axis).\n\n\n\n\n\n\nExample of poorly initialized model (No input or weight initialization)\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.   Key technical information for this section :   Conv 1,8,16,32,64 -&gt; 10 : A Convolution neural network showing the number of filters in each layer and ending with 10 output units.  Activation is nn.ReLU : ReLU function introduces non-linearity to the model.  Data normalisation : None  weight normalization : None \n\n# improved function to include labelling for the stats\nclass ActivationStats(HooksCallback):\n    def __init__(self, mod_filter=fc.noop):\n        super().__init__(append_stats, mod_filter)\n\n    def color_dim(self, figsize=(11,5)):\n      fig, axes = get_grid(len(self), figsize=figsize)\n      for ax, h in zip(axes.flat, self):\n          im = ax.imshow(get_hist(h), origin='lower')  # Using imshow directly\n\n          # Add labels, title, and colorbar for clarity\n          ax.set_xlabel(\"Batch Number\")\n          ax.set_ylabel(\"Activation Value\")\n          ax.set_title(\"Layer \" + \"str(self.index(h))\" + \" Activations\")\n          cbar = plt.colorbar(im, ax=ax)\n          cbar.set_label(\"Frequency\")\n      plt.tight_layout()  # Prevent overlap\n\n\n    def dead_chart(self, figsize=(11,5)):\n        fig, axes = get_grid(len(self), figsize=figsize)\n        for ax, h in zip(axes.flatten(), self):\n            ax.plot(get_min(h), linewidth=3)\n            ax.set_ylim(0,1)\n            ax.set_xlabel(\"Batch Number\")\n            ax.set_ylabel(\"Activation Value\")\n            ax.set_title(\"Layer \" + \"str(self.index(h))\" + \" Dead Activations\")\n        plt.tight_layout()  # Prevent overlap\n\n    def plot_stats(self, figsize=(10,4)):\n        fig, axs = plt.subplots(1,2, figsize=figsize)\n        for h in self:\n            for i in 0,1:\n                axs[i].plot(h.stats[i])\n        axs[0].set_title('Means')\n        axs[1].set_title('Stdevs')\n        axs[0].set_xlabel(\"Batch Number\")\n        axs[1].set_xlabel(\"Batch Number\")\n        axs[0].set_ylabel(\"Mean Activation Value\")\n        axs[1].set_ylabel(\"Standard Deviation of Activation Value\")\n        plt.legend(fc.L.range(self))\n        #plt.tight_layout()  # Prevent overlap\n\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n\n# setup model for learning\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n\n# fits dataset\ndef fit(model, epochs=3, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.2, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n# conv function takes in kernal size, stride (how many elements are skipped) and padding (number of zeros added to the edge of the input data)\n# along with ni (features) input channels and output channels (feature maps)\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n#\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\nmodel = nn.Sequential(*cnn_layers())\n\n#astats = ActivationStats(fc.risinstance(GeneralRelu))\nastats = ActivationStats(fc.risinstance(nn.ReLU))\n\nfit(model, xtra_cbs=[astats]);\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.188\n2.247\n0\ntrain\n\n\n0.196\n2.225\n0\neval\n\n\n0.189\n2.237\n1\ntrain\n\n\n0.196\n2.225\n1\neval\n\n\n0.189\n2.237\n2\ntrain\n\n\n0.196\n2.224\n2\neval\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\nPlots of means and standard deviations for each layer\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nImprove the model by input normalization\nTo give the optimize algorithm every chance converge quicker, normalization the inputs to a mean of zero and standard deviation of 1 will help. This can be done alone or with normalization the weights too. The following section discusses normalization the inputs alone.\nThe key to this is to ensure that each feature contributes equally to the learning process, which is especially important when the features have different units or different scales.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input normalization (also know as feature scaling). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : This denotes a Convolutional Neural Network with varying numbers of filters across different layers, culminating in 10 output units.  Activation is nn.ReLU : The model utilizes the ReLU (Rectified Linear Unit) activation function to introduce non-linearity, aiding in better approximations of complex functions.\n\nxl,yl = 'image','label'\n\n# transform dataset and loader\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\n\n#| echo: false\n#| output: false\n\nxmeans = []\nxstds = []\n\nfor xb, yb in iter(dls.train):\n    xmean, xstd = xb.mean(), xb.std()\n    xmeans.append(xmean.item())\n    xstds.append(xstd.item())\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(xmeans, label='xmean')\nplt.title('X Mean over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Mean')\n\nplt.subplot(1, 2, 2)\nplt.plot(xstds, label='xstd')\nplt.title('X Std Dev over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Std Dev')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nChart showing mean and standard deviation (y axis) over batches (x axis)\n\n\n\n\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\n\n\nimport matplotlib.pyplot as plt\n\nxmeans = []\nxstds = []\n\nfor xb, yb in iter(dls.train):\n    xmean, xstd = xb.mean(), xb.std()\n    xmeans.append(xmean.item())\n    xstds.append(xstd.item())\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(xmeans, label='xmean')\nplt.title('X Mean over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Mean')\n\nplt.subplot(1, 2, 2)\nplt.plot(xstds, label='xstd')\nplt.title('X Std Dev over Iterations')\nplt.xlabel('Iteration')\nplt.ylabel('Std Dev')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nChart showing mean and standard deviation (y axis) over batches (x axis)\n\n\n\n\n\n# slightly better than last time but definatly not perfect&gt;\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[astats]);\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.185\n2.248\n0\ntrain\n\n\n0.196\n2.225\n0\neval\n\n\n0.189\n2.237\n1\ntrain\n\n\n0.196\n2.224\n1\neval\n\n\n0.189\n2.237\n2\ntrain\n\n\n0.196\n2.224\n2\neval\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\n# stanard deviations still away from one but mean looks reasonable\nastats.plot_stats()\n\n\n\n\nPlots of means and standard deviations for each layer\n\n\n\n\n\n# first layer quite bad and last layer is totally dead.\nastats.dead_chart()\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nBatch Normalization with Leaky ReLU activation and Kaiming normalization.\n\nWe are now changing 3 things. First is batch normalisation to control the inputs between each of layers, Leaky ReLU to keep more neurons alive so they contribute to the end result and finally weight initialization.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units.  Activation is Leaky ReLU : We will now use Leaky ReLU (instead of ReLu) as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e. it passes a positve number for each of the activations.  Data normalisation: is BatchNorm : This will help calculate the data after each activation layer???  Learning Rate 0.2 : Initial training was conducted with a learning rate of 0.2.  Best Training So Far: This version of the model has shown the best training results compared to previous iterations.\nWhat to Try Next : The section concludes with open questions and suggestions for future experiments to further enhance model performance. The use of batch normalization and Leaky ReLU has led to improved training dynamics. The next aim is to implement LSUV as call back\n\n# transform dataset from source dsd\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\n#| Avoiding inactive Neurons: Leaky ReLU helps to mitigate the problem of \"inactive neurons\" that can occur with ReLU units,\n# where neurons get stuck during training and always output a zero value. By allowing a small, non-zero output for negative inputs,\n# Leaky ReLU ensures that gradients can still flow through the neuron, which can help to keep learning progressing.\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x):\n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n#| export\ndef plot_func(f, start=-5., end=5., steps=100):\n    x = torch.linspace(start, end, steps)\n    plt.plot(x, f(x))\n    plt.grid(True, which='both', ls='--')\n    plt.axhline(y=0, color='k', linewidth=0.7)\n    plt.axvline(x=0, color='k', linewidth=0.7)\n# visual representation of the new relu, left values Jeremeys example\nplot_func(GeneralRelu(leak=0.1, sub=0.4))\n\n\n\n\nplot of leaky, always passes through a positive value\n\n\n\n\n\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n    if bias is None: bias = not isinstance(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))\n    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n    if norm: layers.append(norm(nf))\n    if act: layers.append(act())\n    return nn.Sequential(*layers)\n#| initializes weights based on kaiming_normal_\ndef init_weights(m, leaky=0.):\n    # checks for a instance of layer and module of the neural network\n    # checks for a instance of 1d, 2d, 3d neural network\n    #\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)):\n      # creates the initialization of the weights, for a, anything that is not zero, standard relu is assumed.\n      init.kaiming_normal_(m.weight, a=leaky)\n# Creates a function based on relu with the parameters already applied\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n\n# Creates a function based on leaky being 0.1\niw = partial(init_weights, leaky=0.1)\n# Returns a instance of a model\n#\ndef get_model(act=nn.ReLU, nfs=None, norm=None):\n    # stores convolutions if not passed for later creation\n    if nfs is None: nfs = [1,8,16,32,64]\n    # Creates convolutions based on conv function for each of the layers in nfs\n    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)\n# collects mean and standard deviations of of each layer thats a ReLu\n# astats = ActivationStats(fc.risinstance(nn.ReLU))\nastats = ActivationStats(fc.risinstance(GeneralRelu))\n# addeds all call backs into a list for later use.\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nset_seed(42)\n# Creates instance of the model and then applys kaiming_normal to the weights\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n# Creates a instance of the learner function\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.599\n1.220\n0\ntrain\n\n\n0.799\n0.680\n0\neval\n\n\n0.840\n0.532\n1\ntrain\n\n\n0.835\n0.558\n1\neval\n\n\n0.867\n0.444\n2\ntrain\n\n\n0.830\n0.559\n2\neval\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\nastats.plot_stats()\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting asthe standard deviations are lower than 1 but the training went really well, would getting these closer to 1 help?????…. click to expand code\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\n\nimplement LSUV initialization as a class\nThis new model is more basic than the first model. Theres no data normalization but we’re keeping Leaky ReLU and changing the weight initialization to a custom LSUV callback.\nIt will show training pipeline and corresponding metrics visualized through various charts. Annotations provide insights into the model’s performance during training.\nBy paying attention to these key metrics and visual cues, we can assess the shortcomings in the input initialization and training regimen, specifically issues such as inactive neurons and poorly tuned input standardization (also know as input initializations). We can then compare them against previously un-initialized inputs and see any improvements.\nKey Technical Information for This Section:\nConv 1,8,16,32,64 -&gt; 10 : Similar to the previous model, this convolutional architecture has filter sizes escalating from 1 to 64, ending with 10 output units.  Activation is Leaky ReLU : We will now use Leaky ReLU as the activation function, which allows for a small, non-zero gradient when the unit is not active. i.e. it passes a positve number for each of the activations.  Data normalisation : none  weight normalization : LSUV  Learning Rate 0.2 : Initial training was conducted with a learning rate of 0.2.  What to Try Next : Weight initilization without input (data) normalization gave good results. It would be a good experiment to add some input (data) normalization like batch normalization. However, the experiment has now finished and im happy with the results here.\n\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=1)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n# This class implements Layer-Sequential Unit-Variance Initialization (LSUV), a technique used to\n# initialize the weights and biases of neural networks. LSUV aims to set these parameters such that\n# during the forward pass, the variance of the activations remains close to 1. This avoids issues\n# commonly associated with poor initialization, such as vanishing or exploding gradients.\n# To achieve this, the class modifies the initial weights and biases in the context of a sample of input\n# data, targeting a specified range for hardware/software-specific floating-point representation. This approach\n# minimizes the risk of exceeding the numerical range, which can lead to unstable training dynamics, or\n# put anotherway reduces the number of neurons contributing (deactivate) and the weight into the final result.\n# Key methods within this class handle the adjustment of weights and biases, based on the calculated\n#  variances and means of the activations. This is typically invoked at the beginning of the training\n# process, prior to the main training loop.\n\n# Note: Initial tests have shown effective results, although chart visualizations may\n# require further refinement.\n\nclass LSUVStatsHook(Callback):\n  # initialize and store all relevent details to object\n  def __init__(self, modules_for_hooks, modules_for_weights, verbose=False, debug=False):\n    self.mean = None\n    self.std = None\n    self.log = fc.noop if not verbose else print\n    self.debug = debug\n    #fc.store_attr()\n    if self.debug : import pdb; pdb.set_trace()\n    self.modules_for_hooks = modules_for_hooks\n    self.modules_for_weights = modules_for_weights\n\n  # update hooks\n  def hook(self, module, input, output):\n    #import pdb;pdb.set_trace()\n    acts = output.detach().cpu()\n    self.mean = acts.mean()\n    self.std = acts.std()\n\n  # apply hooks to relus, update weights and bias to convs\n  def calc_apply_LSUV_weights_bias(self, learn, batch_of_data):\n    # Get all of the modules that will be used for calculating the  lsuv\n    if self.debug : import pdb; pdb.set_trace()\n    self.log(\"self.modules_for_hooks is type\", self.modules_for_hooks)\n    self.log(\"GeneralRelu is type \" , GeneralRelu)\n    modules_to_apply_hooks = [o for o in learn.model.modules() if isinstance(o, self.modules_for_hooks)]\n    self.log(\"modules to apply hooks to: \", modules_to_apply_hooks)\n    module_to_update_weights = [o for o in learn.model.modules() if isinstance(o, self.modules_for_weights)]\n\n    # Update the weights and bias's util desired range is achieved\n    if self.debug : import pdb; pdb.set_trace()\n    no_of_layers = len(modules_to_apply_hooks)\n    for item in range(no_of_layers):\n      self.log(\"entering layer : \", item)\n      handle = modules_to_apply_hooks[item].register_forward_hook(hook_LUSV.hook)\n      with torch.no_grad():\n        while learn.model(batch_of_data) is not None and (abs(hook_LUSV.std-1)&gt;1e-3 or abs(hook_LUSV.mean)&gt;1e-3):\n          self.log(\"update weights to modules: \",  module_to_update_weights[item])\n          module_to_update_weights[item].bias -= hook_LUSV.mean\n          module_to_update_weights[item].weight.data /= hook_LUSV.std\n          self.log(\"standard deviation is :\", hook_LUSV.std)\n          self.log(\"mean is :              \", hook_LUSV.mean)\n    # deregister the hook\n    handle.remove()\n\n  # calls calc_apply_LSUV_weights_bias to update weights and bias's\n  def before_fit(self, learn):\n    if self.debug : import pdb; pdb.set_trace()\n    LSUVStatsHook.calc_apply_LSUV_weights_bias(self, learn, batch_of_data=xb)\n\n# Custom callback with some debugging code commented out\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        #import pdb; pdb.set_trace()\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        #import pdb; pdb.set_trace()\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        #import pdb; pdb.set_trace()\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n        #print(f\"loss : {learn.loss}, weight : {len(x)}\")\n\n# Load the metrics\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\n\n# module for a custom Relu\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x):\n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n\n# setup the LSUV hook to pass to the model\nhook_LUSV = LSUVStatsHook(modules_for_hooks = GeneralRelu, modules_for_weights = nn.Conv2d,verbose=False,debug=False)\n\n# setup the activation statics and module\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nastats = ActivationStats(fc.risinstance((GeneralRelu, nn.ReLU)))\n\n# setup the model and call fit\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, act())\n    return res\n\ndef get_model(act=nn.ReLU, nfs=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None), nn.Flatten()).to(def_device)\n\nmodel = get_model(act_gr)\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats, hook_LUSV]\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n\nlearn.fit(3)\n\n\n\nVisualization of training metrics for learner model, batch numbers along the (x-axis) and loss (y-axis). The table shows the accuracy and loss of the model for the epoch and where it is a train and eval\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.461\n1.575\n0\ntrain\n\n\n0.693\n1.012\n0\neval\n\n\n0.758\n0.783\n1\ntrain\n\n\n0.754\n0.817\n1\neval\n\n\n0.811\n0.622\n2\ntrain\n\n\n0.799\n0.681\n2\neval\n\n\n\n\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\nplots of inactive neurons (zero neurons) for each layer of the neural network, the lower the better so all neurons contribute to the result\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\nColour chart to show inactive neurons\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\nPlots of means and standard deviations for each layer activations. Means should be close to zero and Stdevs should be close to 1 for optimal training, this is interesting as this really shows training got to the optimal weights after batch 75, layer 1 jumped up to 1 at batch 0, layer 2 jumped upto 1 at 25, layer 3 jumped up 1 at batch 50 and layer 3 jumped upto 75 so 25 between each of the layers. …. click to expand code"
  }
]