[
  {
    "objectID": "useful_info.html",
    "href": "useful_info.html",
    "title": "Commands to run quarto",
    "section": "",
    "text": "Commands to run quarto\nto create a project (run once at the beginning to create the project files) - quarto create project_name –template=blog\nto run a preview locally - quarto preview .\nto Create update the files (run before pushing to github) - quarto render .\n\n\nlinks :\n\nVideo of somone showing how to use quarto https://www.youtube.com/watch?v=nllKcuX7rEc\nilovetensoor coppied a good link here : https://discord.com/channels/689892369998676007/1096981866248147014/1167453441291984936\nexample blog http://vishalbakshi.com/blog/posts/2023-10-09-nn-rf-embeddings/\nexample blog https://ilovetensor.tech/posts/neural-nets-from-scratch/\n\n\n\nquarto reference page :\nhttps://quarto.org/docs/reference/cells/cells-jupyter.html\n\n\nsetup for using colab with quarto\n\nCreate a new notebook in colab and add markup in markup - no need for RAW as documentation says.\ninstall the drive app in windows which will reveal the drive folder in the file explorer, i used streams so no files are stored locally.\nCreate a script to copy file to local installation and run “quarto render .” to create the html file. see file located C:projects_lsuv_from_colab.bat\nEdit the notebook in colab and run the script to render the html file, repeat until happy with the result and publish to github."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html",
    "href": "posts/iot_in_a_factory/index.html",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "",
    "text": "As someone deeply involved in factory operations, I’ve witnessed firsthand the remarkable transformation brought about by the Internet of Things (IoT). Let me share with you the journey we’ve embarked on to automate and streamline our manufacturing processes, blending traditional methods with cutting-edge technology."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#introduction",
    "href": "posts/iot_in_a_factory/index.html#introduction",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "",
    "text": "As someone deeply involved in factory operations, I’ve witnessed firsthand the remarkable transformation brought about by the Internet of Things (IoT). Let me share with you the journey we’ve embarked on to automate and streamline our manufacturing processes, blending traditional methods with cutting-edge technology."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#the-need-for-automation",
    "href": "posts/iot_in_a_factory/index.html#the-need-for-automation",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "The Need for Automation",
    "text": "The Need for Automation\nMy day typically started with a walkthrough of the factory, assessing everything from energy usage to production progress. This routine made it clear that automating some of these processes was essential. We needed to evolve from traditional ERP systems that required manual logging to a more integrated approach using IoT technologies."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#integrating-real-time-monitoring",
    "href": "posts/iot_in_a_factory/index.html#integrating-real-time-monitoring",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Integrating Real-Time Monitoring",
    "text": "Integrating Real-Time Monitoring\nIn our control room, we combined advanced monitoring systems with traditional tools like whiteboards. The objective was to create a seamless blend of new and old, ensuring that the transition to high-tech did not overshadow the simplicity and effectiveness of conventional methods."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#practical-applications-of-iot-in-our-factory",
    "href": "posts/iot_in_a_factory/index.html#practical-applications-of-iot-in-our-factory",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Practical Applications of IoT in Our Factory",
    "text": "Practical Applications of IoT in Our Factory\n\nAdopting Wireless Technology: Despite initial skepticism due to our factory’s challenging environment, technologies like LoRaWAN proved to be a game-changer, offering robust and reliable wireless communication.\nImplementing Safety and Efficiency Sensors: We strategically placed cost-effective IoT sensors throughout the factory to monitor crucial parameters like temperature, humidity, and sound levels, ensuring a safer and more efficient working environment."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#gaining-a-competitive-edge",
    "href": "posts/iot_in_a_factory/index.html#gaining-a-competitive-edge",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Gaining a Competitive Edge",
    "text": "Gaining a Competitive Edge\nThe economic benefits of integrating IoT have been substantial. Reduced sensor costs and the ease of deploying edge devices have played a critical role in maintaining our competitive edge, especially in the international market. Open-source software has also been a boon, helping us cut operational costs significantly."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#overcoming-challenges",
    "href": "posts/iot_in_a_factory/index.html#overcoming-challenges",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Overcoming Challenges",
    "text": "Overcoming Challenges\nImplementing IoT in a factory setting wasn’t without its challenges. Wireless technologies initially struggled with the electromagnetic interference prevalent in our environment. However, adopting solutions like LoRaWAN helped us overcome these obstacles effectively."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#our-iot-system-architecture",
    "href": "posts/iot_in_a_factory/index.html#our-iot-system-architecture",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Our IoT System Architecture",
    "text": "Our IoT System Architecture\nOur system architecture is a blend of LoRaWAN devices, Industrial IoT devices, and legacy systems like Raspberry Pi. We store our data in an Influx database and use Grafana for real-time alerting and monitoring. This setup exemplifies our commitment to integrating modern technology while respecting the reliability of proven systems."
  },
  {
    "objectID": "posts/iot_in_a_factory/index.html#conclusion",
    "href": "posts/iot_in_a_factory/index.html#conclusion",
    "title": "Embracing IoT in Manufacturing: My Journey to Transforming Factory Operations",
    "section": "Conclusion",
    "text": "Conclusion\nAs we continue to embrace IoT and its myriad applications, we’re not just improving our factory operations. We are actively participating in the fourth industrial revolution, paving the way for a more connected, efficient, and innovative future in manufacturing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html",
    "href": "posts/Exploring_GTP4V_paper/index.html",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Hi, this is a explority view of GPT4 vision, study hacks and it’s uses in manufacturing and industry 4.0. Its a commentary of microsoft paper TheDawnofLMMs: PreliminaryExplorationswithGPT-4V(ision) picking out the highlights, my take on how GPT4 vision can be used in daily life, how to maximise my studying of the fast.ai course and what GPT4 vision means for the future of manufacturing and industry 4.0.\nThe paper and chatgpt vision was discussed with the Fast.AI study group. The study group is a multi-national group of people who are passionate about deep learning and AI. We meet online on Saturdays to run through the course work provided by fastAI, discuss papers and latest trends and get to know each other.\n\n\n\nGPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA.\n\n\n\n\n\nSomeone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog.\n\n\n\n\n\nIn the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more.\n\n\n\n\nDuring the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing.\n\n\n\n\nI beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks\n\n\n\n\n\nDefect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "GPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Someone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "href": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "In the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "href": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "During the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "href": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "I beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks"
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "href": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Defect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/chatGPT_audio_conversation/index.html",
    "href": "posts/chatGPT_audio_conversation/index.html",
    "title": "Maximizing Your Commute: Learning on the Go with ChatGPT Voice",
    "section": "",
    "text": "Overview\nI have 20 minute drive into work and back home everyday where I usually listen to a podcast, audio book or sometimes just stair at the road. I’m always thinking, how can I make better use of my time…. I saw a twitter post from Le Cunn, the legendary Data Scientist from Meta and that i fearously agree with ““books are a user interface to knowledge.”.\nThat’s what AI assistants are poised to become: “AI assistants will be a better user interface to knowledge.”\nThis is a short blog on how im using chatgpt to study and making use of spare time on journes to work.\n\n\n\nWhat is ChatGPT voice\nChatgpt voice is based on GPT 3.5 model and includes features giving that give it the ability to hear and speak.\nFor the voice feature, OpenAI uses Whisper, its speech recognition system, to transcribe a user’s spoken words into text and a new text-to-speech model that can generate human-like audio from text with just a few seconds of speech.\nI have a selection of 5 voices to choose from and have been using “sky” and it sounds really natural. You can still tell its a computer speech which I imagine is on purpose, it gets the pauses and tone, emotion right, im really impressed.\nIt available on the plus and enterprise edition, not the free edition of ChatGPT.\n\n\nWhats wrong with podcast and audiobooks\nI’ve been using ChatGPT voice conversations for over a month now (it’s still in beta) and for me its a game changer for studying and making the spare time useful on the journey to work and back.\nListening to podcasts or audiobooks can be enriching, yet they come with limitations. Sometimes they delve into topics that don’t interest you. At other times, they might present information in a convoluted manner. There’s also the issue of pacing: some podcasts assume advanced knowledge, leaving you lost, while others may cover familiar ground, leading to frustration\n\n\nwhy use ChatGPT voice\nChatGPT Voice offers a versatile learning experience by tapping into a wealth of internet knowledge. It can adapt its conversational style to mimic various tones, such as Shakespearean language or the distinctive styles of well-known educators like Richard Feynman and Jeremy Howard. While I haven’t personally tried these specific modes, they could resonate better with your learning preferences\n\n\nHow to use ChatGPT Voice\nDo you have a favourite educator? Ask GPT voice to give explanations like that educator, don’t understand something, ask it to explain it in a different way. Ask it to explain it to a 5 year old, a 10 year old, say what you understand and what you dont understand and for me, its filled in the gaps.\nAsk it to give you a topic and ask for a questions at the end to make sure you understand.\nWhat are the most important facts, dates, or formulas related to (topic)? Help me create a memorization technique to remember them easily.\nGive it a statement and ask chatgpt to give feedback, any corrections or improvements.\nAsk voice GPT to create models or analogies to help me understand and remember “optimization techinques in deep learning”.\nDo you have a difficult concept to understand, ask Chatgpt to Guide you through a visualization exercise to help me internalize the term optimization techinques and imagine yourself successfully applying it to a real-life situation. This has really helped me on difficult concepts, highly recommend you try it.\nMy favourite is to ask chatgpt voice “I want you to act as a Socrat and use the Socratic method to help me improve my critical thinking, logic, and reasoning skills. Your task is to ask open-ended questions to the statement ‘optimization techinques in deep learning’, give me constructive feedback to each response before you ask the next question.””\nAll conversations are saved in text format when your return to your computer and phone, you can review the conversation and save it to your notes or share it.\n\n\nOther Uses\nDo you have a important conversation coming up, talk it through with ChatGPT first to get your thoughts in order and ask for opionions on how other positions, react and respond and how to frame the conversation. You can have this conversation as many times as you want, it will never get bored, it will never get angry and its always available 24 hours a day 7 days a week, 365 days a year.\nPotential use cases:\n\nNegotiations\nInterviews\nSales\nPresentations\nConflict resolution\nDifficult conversations\nAsking for a raise\nAsking for a promotion\nAsking for a date\n\n\n\nThings that need improving\n\nSometimes Chatgpt voices cuts the converstation off half way throgh me saying something if i pause while im thinking about how to phrase something. I wish it would wait a bit longer before cutting off the conversation.\nSometimes I want to cut off what the chatgpt voice is saying because it/I haven’t fully explained the context. I wish it would stop speaking and go into listening mode or be able to do both.\nI wish it would give me a summary of the conversation at the end, it would be useful to have a summary of the conversation to help me remember what we talked about.\n\n\n\nConclusion\nI will be using this more and more, I think its a game changer for me, I can see myself using it. I’m really excited about the future of AI assistants and how they will help us learn and understand the world together. It will be interesting to know how bigger or better models will improve the experience. I’m sure there will be a lot of research in this area.\nTry ChatGPT, I think it will change the way you think about AI assistants and how you can use them to learn and understand together.\n\nReferences\n(“How to Use ChatGPT to Easily Learn Any Skill You Want” n.d.)\n\n\n\n\n\n\nReferences\n\n“How to Use ChatGPT to Easily Learn Any Skill You Want.” n.d. Accessed October 26, 2023. https://www.youtube.com/watch?v=MnDudvCyWpc&t=365s."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI: The Catalyst of the 4th Industrial transformation",
    "section": "",
    "text": "Stepping into the heart of the 4th industrial revolution, this blog series examines the future of manufacturing, with AI and data analytics positioned at the core of this momentous transformation.\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe Power of Retrieval-Augmented Generation in AI Language Models\n\n\n\n\n\n\n\ndeep learning\n\n\nnlp\n\n\nrag\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe Importance of Proper Initialization.\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nAlex Kelly\n\n\n\n\n\n\n  \n\n\n\n\nMaximizing Your Commute: Learning on the Go with ChatGPT Voice\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nchatbot\n\n\ngpt\n\n\nnlp\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nExploring GPT4 vision, fastai study hacks and what it means for industry 4.0\n\n\n\n\n\n\n\nai\n\n\nmanufacturing\n\n\nfastai\n\n\ngpt4\n\n\nchatgpt\n\n\nrpa\n\n\nautomation\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nEmbracing IoT in Manufacturing: My Journey to Transforming Factory Operations\n\n\n\n\n\n\n\nIoT\n\n\nManufacturing\n\n\nLoRaWAN\n\n\nIndustrial IoT\n\n\nRaspberry Pi\n\n\nInfluxDB\n\n\nGrafana\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nNurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020\n\n\n\n\n\n\n\nLeadership\n\n\nEducation\n\n\nEEUK\n\n\nEvent\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nThe Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces\n\n\n\n\n\n\n\nIoT\n\n\nLoRaWAN\n\n\nMakerspaces\n\n\nSELA\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2020\n\n\nAlex Paul Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Alex Kelly Seasoned Technologist & Leader | Empowering Future Innovations in IT | Advocate of AI & IoT | Transforming Business Processes in Manufacturing Sector\nIn this space, I aim to unravel the intricacies of the 4th Industrial Revolution, shedding light on how AI, IoT, Robotics, and Deep Learning are converging to redefine the manufacturing landscape. Through a series of insightful blogs, I will share my explorations and findings on the synergies between these cutting-edge technologies and their transformative impact on industrial operations and processes. This journey delves into not just the technological advancements steering this revolution, but also the potential challenges and solutions that come along, offering a holistic view on the unfolding industrial epoch.\nJoin me as we navigate through the myriad facets of this transformative era, seeking to understand and harness the immense potential that lies at the intersection of AI, data analytics, and modern industrial practices."
  },
  {
    "objectID": "posts/all_things_RAG_paper_discussion/all_things_rag_paper_discussion.html",
    "href": "posts/all_things_RAG_paper_discussion/all_things_rag_paper_discussion.html",
    "title": "The Power of Retrieval-Augmented Generation in AI Language Models",
    "section": "",
    "text": "I’ve recently joined a discord group where we discuss deep learning papers and this week its about RAG (Retrieval-Augmented Generation). It consisted listening to the author of the paper and questions and answers session for hte majory of the time. Some of questions on the paper were interesting and we got to shape the direction of the discussion by our questions.\nThe paper can be found here\nThe discord group can be found here\n\nabout the paper discussed\nIn the ever-evolving landscape of Artificial Intelligence, some development has emerged from the labs of Facebook AI Research and University College London. Patrick Lewis and his team, brings to light an innovative approach to enhancing language models - the Retrieval-Augmented Generation (RAG) model. This breakthrough addresses a critical challenge in AI language processing: the efficient access and manipulation of knowledge for complex tasks.\n\n\nBridging the Knowledge Gap in Language Models\nTraditional language models, despite their size and sophistication, often stumble when dealing with knowledge-intensive tasks. They struggle to retrieve and accurately use specific information. This is where the RAG model steps in, blending the prowess of a pre-trained seq2seq model with a rich, non-parametric memory bank, primarily sourced from Wikipedia.\n\n\nThe Mechanics of RAG: A Symphony of Parametric and Non-Parametric Memory\nAt the heart of RAG lies a seamless integration of two components: a powerful seq2seq model and a neural retriever accessing a dense vector index of Wikipedia. This dual approach allows the model to dynamically pull in relevant information during the generation process, leading to more precise and informed responses. Pararmetric is deep learning models which are trained and in this case non-parametric are stored values in a vector database and retrieved when appropriate.\n\n\nRAG in Action: A Leap Forward in NLP Tasks\nThe effectiveness of RAG models is evident across a spectrum of natural language processing (NLP) tasks. From open-domain question answering to abstractive summarization, Jeopardy-style question generation, and fact verification. The paper also highlights the potential of RAG models in the field of dialogue systems, where they can be used to generate more contextually relevant responses.\n\n\nBeyond Performance: The Societal Impact of RAG Models\nWhile the technical achievements of RAG models are impressive, the paper also thoughtfully examines their broader impact. The potential societal benefits are significant, particularly in areas where accurate and diverse information generation is crucial. However, the team is also mindful of the downsides, including the ethical considerations and challenges that accompany advanced AI technologies.\n\n\nThe Road Ahead: A New Era in AI Language Processing\nThe introduction of Retrieval-Augmented Generation models marks a significant milestone in the field of AI and NLP. By effectively addressing the knowledge limitations of previous models, RAG paves the way for more nuanced and contextually aware AI systems. As we stand on the brink of this new era, the possibilities seem as boundless as they are exciting.\n\n\nQuestions and answers findings\n\nLarger models are going to play a key role in more accurate and diverse information generation.\nUse rag if you want the model to be more contextually aware - You could think of it being more biased to data supplied to the datastore. To be more specific, if you want know the solutions to know your address, eye color, customer specific information etc. rag is the way to go.\nRag is not a silver bullet, it has its own limitations. e.g. It can’t generate information which is not present in the knowledge base.\nThe future of rag is multimodel, where the knowledge base is not just text but also images, videos, audio etc.\n\nThis is going to be interesting but I imagine it will be computationally expensive.\nHows it going to split the attention between the different modalities.\nHows it going to chunk the information from the different modalities.\nHows it going to generate the information in the different modalities."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html",
    "href": "posts/developing_the_skils_for_smart_futures/index.html",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "",
    "text": "In an era where leadership skills are pivotal for success across various sectors, the EEUK Leadership Event 2020 provided a platform for vibrant discussions and shared learning. The event, conducted virtually, brought academics from Sheffield Hallam, Sheffield University, Liverpool John Moores university together with industry professionals, and students to explore the development of leadership skills in the context of emerging trends and challenges."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#innovative-teaching-methods",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#innovative-teaching-methods",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Innovative Teaching Methods",
    "text": "Innovative Teaching Methods\nThe event highlighted innovative approaches to teaching leadership skills. Lisa McMullan and Matthias Feist praised the use of visual minutes in capturing key learnings, emphasizing the importance of reflective and engaging teaching methods."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#interdisciplinary-learning",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#interdisciplinary-learning",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Interdisciplinary Learning",
    "text": "Interdisciplinary Learning\nGary Wood from Sheffield Hallam University and other panelists discussed the significance of interdisciplinary learning in leadership education. They stressed that integrating different disciplines could enhance students’ understanding and application of leadership skills in diverse contexts."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#collaboration-between-academia-and-industry",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#collaboration-between-academia-and-industry",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Collaboration Between Academia and Industry",
    "text": "Collaboration Between Academia and Industry\nThe dialogue emphasized the importance of academia-industry collaboration. Lesley Lambert from LJMU LCR 4 START discussed accessing funding for projects integrating industry issues into the curriculum, highlighting the benefits of such collaborations for students, industry, and academics."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#experiential-learning-opportunities",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#experiential-learning-opportunities",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Experiential Learning Opportunities",
    "text": "Experiential Learning Opportunities\nThe role of extracurricular and experiential learning was a focal point. Gary Wood spoke about the Sheffield Engineering Leadership Academy (SELA), mentioning its non-credit bearing nature and the creative freedom it offers for exploring new teaching methods."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#enhancing-student-engagement",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#enhancing-student-engagement",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Enhancing Student Engagement",
    "text": "Enhancing Student Engagement\nHayley Jones and Laura Foster raised concerns about effectively communicating opportunities to students. They discussed strategies like leveraging university PR channels and integrating success stories to increase student participation in leadership development programs."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#digital-transformation-and-leadership",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#digital-transformation-and-leadership",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "Digital Transformation and Leadership",
    "text": "Digital Transformation and Leadership\nAlex Kelly emphasized the growing importance of digital skills in leadership, citing the relevance of coding and AI understanding for future leaders. He also highlighted the potential of hackspaces in fostering creativity, communication skills, and leadership."
  },
  {
    "objectID": "posts/developing_the_skils_for_smart_futures/index.html#the-future-of-leadership-education",
    "href": "posts/developing_the_skils_for_smart_futures/index.html#the-future-of-leadership-education",
    "title": "Nurturing Leadership Skills for Smart Futures: Insights from the EEUK Leadership Event 2020",
    "section": "The Future of Leadership Education",
    "text": "The Future of Leadership Education\nThe event concluded with discussions on the future trajectory of leadership education, particularly in light of the pandemic’s impact. Speakers like Daniel Habbershaw and Nick Cooper discussed adapting to current challenges and the importance of reflective and adaptive learning approaches."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html",
    "href": "posts/initialization_neural_networks/index.html",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand the inner workings of each aspect of a neural network. This will allow me to create new/improve existing techniques and enable me to piece together the right neural network for the right task.\nThe primary objective was to encapsulate the LSUV initialization method within a callback structure, honing my skills in this programming paradigm. Mastery of callback-based coding is widely recognized for its benefits, including enhanced code readability and maintainability. Moreover, this approach significantly accelerates the experimentation process, a critical advantage in the field of deep learning.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.\n\n\n\n\nProper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.\n\n\n\n\nEach model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.\n\n\n\nThe aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : A chart showing a colour matrix of inactive neurons. The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are. 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Shows the means and standard devivations of the activations. Means close to zero but ideally should be close to 1 to train optimally.  - and finally the conclusion of the results\n\n\n\nThis code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n\n….click to expand code\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]"
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#why-am-i-writing-about-lsuv",
    "href": "posts/initialization_neural_networks/index.html#why-am-i-writing-about-lsuv",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "I’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand the inner workings of each aspect of a neural network. This will allow me to create new/improve existing techniques and enable me to piece together the right neural network for the right task.\nThe primary objective was to encapsulate the LSUV initialization method within a callback structure, honing my skills in this programming paradigm. Mastery of callback-based coding is widely recognized for its benefits, including enhanced code readability and maintainability. Moreover, this approach significantly accelerates the experimentation process, a critical advantage in the field of deep learning.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#why-initialization-model-weights-before-starting-the-optimization",
    "href": "posts/initialization_neural_networks/index.html#why-initialization-model-weights-before-starting-the-optimization",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "Proper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. It’s not just about the initialization of the 1st layer of weights, its about all of the layers weights from the 1st to the last.\nHere are a few key points on weight initializations:\n\nThe hardware has floating point limitations that mean it processes a limited number of bits and stores in a limited amount of memory. if the weights are too high or too low, then the resulting calculations can exceed the numerical range that can be represented in the specified memory, leading to what is known as exploding or vanishing gradients (i.e.. inactive neurons) at any level in the neural network. This results in information lost, which are called inactive neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#lsuv-vs-other-weight-optimization-techniques",
    "href": "posts/initialization_neural_networks/index.html#lsuv-vs-other-weight-optimization-techniques",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "Each model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best with your data and model. Here are a few examples of initialization techniques and when to use them :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization: Initializes neural network weights in a way that the variance of the outputs of each layer is unitary, aiming to combat the vanishing and exploding gradient problem.\nZero Initialization: Sets all the initial weights of the neural network to zero, often leading to poor performance as all neurons in a layer will learn the same features during training.\nRandom Initialization: Assigns weights to neural network layers randomly, usually from a Gaussian or uniform distribution, to break symmetry and ensure different neurons can learn different functions.\nXavier/Glorot Initialization: Sets the initial weights according to a scaled uniform distribution, which is based on the number of input and output neurons, designed to keep the gradients in a reasonable range.\nHe Initialization: Uses a scaled uniform or normal distribution based on the number of input neurons to initialize weights, especially suitable for layers with ReLU activation to preserve gradient variance.\nLeCun Initialization: Initializes weights with a scaled normal distribution where the scaling is based on the number of input neurons, recommended for layers followed by linear or sigmoid activations.\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#the-following-sections-guide-you-through-the-code-along-with-comments-and-reflections-on-the-results",
    "href": "posts/initialization_neural_networks/index.html#the-following-sections-guide-you-through-the-code-along-with-comments-and-reflections-on-the-results",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "The aim of the notebook is create my own callback using LSUV, along the way we will start with the simplest of models and build up to more complex models, viewing and critiqueing the results along the way. All the code and data is here to recreate the results.\nWe will be covering :\n\nSetting up the environment, loading the data set\nfinding the learning rate\nlearner without LSUV or any other initialization techniques and exploring the results.\nlearner wtih Standardizing inputs with no weights optimization techniques\nlearner with Batch Normalization with Leaky ReLU activation and Kaiming normalization.\nLSUV training method\n\neach of the learner sections where we will be running the model will have the following charts :\n loss and accuracy : learner loss and accuracy for the training and validation data sets  Color_dim : A chart showing a colour matrix of inactive neurons. The color represents the frequency of activations in a specific range. We’re using the Viridis colormap, yellow indicates higher frequencies (many activations in that range), and purple indicates lower frequencies. So, areas with more intense yellow mean those activation values occur more frequently along the (y axis) for that batch (X axis). Ideally want the yellow spread accross the y axis which you will see demostrated across the sections.  Dead_chart : Shows how many inactive neurons there are. 1 being all neurons are inactive and 0 meaning no neurons are inactive. Having more neurons contributing the the results is better.  Plot_stats : Shows the means and standard devivations of the activations. Means close to zero but ideally should be close to 1 to train optimally.  - and finally the conclusion of the results"
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html#setup-environment-loading-the-dataset-transforming-the-data-for-training",
    "href": "posts/initialization_neural_networks/index.html#setup-environment-loading-the-dataset-transforming-the-data-for-training",
    "title": "The Importance of Proper Initialization.",
    "section": "",
    "text": "This code sets up a pipeline to preprocess and load the Street View House Numbers (SVHN) dataset for machine learning with PyTorch. It installs required packages, imports libraries, configures settings, fetches the dataset, converts images to grayscale, applies data transformations, and creates data loaders for training.\n\n\n….click to expand code\n# install required libraries\n!pip install datasets\n!pip install torcheval\n\n# Python Standard Library imports\nimport math\nimport logging\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\nimport random\n\n# Third-party library imports\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset, load_dataset_builder\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\nfrom fastcore.test import test_close\nfrom torch.nn import init\nfrom torch import nn,tensor\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torcheval.metrics import MulticlassAccuracy, Mean\nimport numpy as np\n\n# Custom module imports\nfrom conv import *\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\n\n# Configuration settings\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'viridis'\nlogging.disable(logging.WARNING)\n\n\n# get labels\nx,y = 'image','label'\n\n#  Street View House Numbers dataset name\nname = ('svhn')\n\n# fetch dataset from hugging face\ndsd = load_dataset(name, \"cropped_digits\",)\n\n# remove extra (not required for initial run through)\ndsd.pop(\"extra\")\n\n# convert images to greyscale\ndef convert_to_gray(batch):\n    image = batch['image']\n    if image.mode != 'L':  # Only convert if not already grayscale\n        gray_image = image.convert('L')\n        batch['image'] = gray_image\n    return batch\n\n# Apply to all datasets\nfor key in dsd.keys():\n    dsd[key] = dsd[key].map(convert_to_gray, batched=False)\n\n# transform data\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n# extract data set\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(dd=tds, batch_size=bs, num_workers=1)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]"
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html",
    "href": "posts/the_makers_journey_unimaker/index.html",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "",
    "text": "Here’s a article based on a talk I gave on 8th September 2020 for the Sheffield University. The talk was about my journey as a maker and how I’ve been able to apply my skills and knowledge in the industrial environment at Tinsel Bridge. I’ve also included a video of the talk for those that prefer to watch rather than read."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#why-im-a-maker-at-heart",
    "href": "posts/the_makers_journey_unimaker/index.html#why-im-a-maker-at-heart",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Why I’m a Maker at Heart",
    "text": "Why I’m a Maker at Heart\nMy passion for making is rooted in the philosophy of makerspaces – the essence of learn-by-doing. This approach has always resonated with me, as it aligns with my personal experiences in various creative fields. From my days in comedy improvisation to my involvement in Toastmasters, I’ve always thrived in environments where learning and creativity happen spontaneously and collaboratively."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#the-sheffield-makerspace-experience",
    "href": "posts/the_makers_journey_unimaker/index.html#the-sheffield-makerspace-experience",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "The Sheffield Makerspace Experience",
    "text": "The Sheffield Makerspace Experience\nOver the past three years at the Sheffield Makerspace, I’ve discovered the true value of community learning. Here, knowledge and skills are shared organically, fostering an environment of mutual teaching and learning. Whether it’s soldering, designing circuits, coding, laser cutting, or using a bandsaw, there’s always an opportunity to learn something new or to share your expertise with others."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#turning-passion-into-profession",
    "href": "posts/the_makers_journey_unimaker/index.html#turning-passion-into-profession",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Turning Passion into Profession",
    "text": "Turning Passion into Profession\nMakerspaces aren’t just about hobbies; they’re breeding grounds for innovation and entrepreneurship. I’ve seen firsthand how a simple project can evolve into a business venture. Pimoroni, for instance, started as a project in the hackspace and has now grown into a renowned electronics company. This transformation from maker to entrepreneur is a journey that truly inspires me."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#the-iot-and-lorawan-exploration",
    "href": "posts/the_makers_journey_unimaker/index.html#the-iot-and-lorawan-exploration",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "The IoT and LoRaWAN Exploration",
    "text": "The IoT and LoRaWAN Exploration\nMy foray into IoT and specifically, LoRaWAN, has been a game changer, particularly in the challenging industrial environment at Tinsel Bridge. The ability of LoRaWAN to transmit data effectively, despite the electrical noise, has not only streamlined our processes but also opened up a world of possibilities in terms of industrial IoT applications."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#mentoring-in-the-sela-project",
    "href": "posts/the_makers_journey_unimaker/index.html#mentoring-in-the-sela-project",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Mentoring in the SELA Project",
    "text": "Mentoring in the SELA Project\nBeing involved in the SELA project as a mentor has been another enriching experience. Providing students with real industry problems and guiding them through the solution process has not only been fulfilling but also a learning experience for me. Witnessing their growth and their fresh, innovative approaches to problem-solving has been truly rewarding."
  },
  {
    "objectID": "posts/the_makers_journey_unimaker/index.html#conclusion",
    "href": "posts/the_makers_journey_unimaker/index.html#conclusion",
    "title": "The Maker’s Journey: Discovering Innovation and Collaboration in Makerspaces",
    "section": "Conclusion",
    "text": "Conclusion\nMy journey in the world of makerspaces and IoT has been more than just about learning new technologies or skills. It’s about being part of a community that thrives on collaboration, creativity, and shared knowledge. From fostering friendships to enhancing communication and leadership skills, the experiences I’ve gathered have shaped me both professionally and personally. Whether it’s through making, mentoring, or exploring new technologies, the journey continues to be an exciting and fulfilling one."
  }
]