[
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html",
    "href": "posts/Exploring_GTP4V_paper/index.html",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Hi, this is a explority view of GPT4 vision, study hacks and it’s uses in manufacturing and industry 4.0. Its a commentary of microsoft paper TheDawnofLMMs: PreliminaryExplorationswithGPT-4V(ision) picking out the highlights, my take on how GPT4 vision can be used in daily life, how to maximise my studying of the fast.ai course and what GPT4 vision means for the future of manufacturing and industry 4.0.\nThe paper and chatgpt vision was discussed with the Fast.AI study group. The study group is a multi-national group of people who are passionate about deep learning and AI. We meet online on Saturdays to run through the course work provided by fastAI, discuss papers and latest trends and get to know each other.\n\n\n\nGPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA.\n\n\n\n\n\nSomeone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog.\n\n\n\n\n\nIn the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more.\n\n\n\n\nDuring the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing.\n\n\n\n\nI beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks\n\n\n\n\n\nDefect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "GPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Someone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "href": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "In the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "href": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "During the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "href": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "I beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks"
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "href": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Defect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Everything AI Blog",
    "section": "",
    "text": "setup enviroment\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMaximizing Your Commute: Learning on the Go with ChatGPT Voice\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nchatbot\n\n\ngpt\n\n\nnlp\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nExploring GPT4 vision, fastai study hacks and what it means for industry 4.0\n\n\n\n\n\n\n\nai\n\n\nmanufacturing\n\n\nfastai\n\n\ngpt4\n\n\nchatgpt\n\n\nrpa\n\n\nautomation\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Alex Kelly Seasoned Technologist & Leader | Empowering Future Innovations in IT | Advocate of AI & IoT | Transforming Business Processes in Manufacturing Sector"
  },
  {
    "objectID": "posts/chatGPT_audio_conversation/index.html",
    "href": "posts/chatGPT_audio_conversation/index.html",
    "title": "Maximizing Your Commute: Learning on the Go with ChatGPT Voice",
    "section": "",
    "text": "Overview\nI have 20 minute drive into work and back home everyday where I usually listen to a podcast, audio book or sometimes just stair at the road. I’m always thinking, how can I make better use of my time…. I saw a twitter post from Le Cunn, the legendary Data Scientist from Meta and that i fearously agree with ““books are a user interface to knowledge.”.\nThat’s what AI assistants are poised to become: “AI assistants will be a better user interface to knowledge.”\nThis is a short blog on how im using chatgpt to study and making use of spare time on journes to work.\n\n\n\nWhat is ChatGPT voice\nChatgpt voice is based on GPT 3.5 model and includes features giving that give it the ability to hear and speak.\nFor the voice feature, OpenAI uses Whisper, its speech recognition system, to transcribe a user’s spoken words into text and a new text-to-speech model that can generate human-like audio from text with just a few seconds of speech.\nI have a selection of 5 voices to choose from and have been using “sky” and it sounds really natural. You can still tell its a computer speech which I imagine is on purpose, it gets the pauses and tone, emotion right, im really impressed.\nIt available on the plus and enterprise edition, not the free edition of ChatGPT.\n\n\nWhats wrong with podcast and audiobooks\nI’ve been using ChatGPT voice conversations for over a month now (it’s still in beta) and for me its a game changer for studying and making the spare time useful on the journey to work and back.\nListening to podcasts or audiobooks can be enriching, yet they come with limitations. Sometimes they delve into topics that don’t interest you. At other times, they might present information in a convoluted manner. There’s also the issue of pacing: some podcasts assume advanced knowledge, leaving you lost, while others may cover familiar ground, leading to frustration\n\n\nwhy use ChatGPT voice\nChatGPT Voice offers a versatile learning experience by tapping into a wealth of internet knowledge. It can adapt its conversational style to mimic various tones, such as Shakespearean language or the distinctive styles of well-known educators like Richard Feynman and Jeremy Howard. While I haven’t personally tried these specific modes, they could resonate better with your learning preferences\n\n\nHow to use ChatGPT Voice\nDo you have a favourite educator? Ask GPT voice to give explanations like that educator, don’t understand something, ask it to explain it in a different way. Ask it to explain it to a 5 year old, a 10 year old, say what you understand and what you dont understand and for me, its filled in the gaps.\nAsk it to give you a topic and ask for a questions at the end to make sure you understand.\nWhat are the most important facts, dates, or formulas related to (topic)? Help me create a memorization technique to remember them easily.\nGive it a statement and ask chatgpt to give feedback, any corrections or improvements.\nAsk voice GPT to create models or analogies to help me understand and remember “optimization techinques in deep learning”.\nDo you have a difficult concept to understand, ask Chatgpt to Guide you through a visualization exercise to help me internalize the term optimization techinques and imagine yourself successfully applying it to a real-life situation. This has really helped me on difficult concepts, highly recommend you try it.\nMy favourite is to ask chatgpt voice “I want you to act as a Socrat and use the Socratic method to help me improve my critical thinking, logic, and reasoning skills. Your task is to ask open-ended questions to the statement ‘optimization techinques in deep learning’, give me constructive feedback to each response before you ask the next question.””\nAll conversations are saved in text format when your return to your computer and phone, you can review the conversation and save it to your notes or share it.\n\n\nOther Uses\nDo you have a important conversation coming up, talk it through with ChatGPT first to get your thoughts in order and ask for opionions on how other positions, react and respond and how to frame the conversation. You can have this conversation as many times as you want, it will never get bored, it will never get angry and its always available 24 hours a day 7 days a week, 365 days a year.\nPotential use cases:\n\nNegotiations\nInterviews\nSales\nPresentations\nConflict resolution\nDifficult conversations\nAsking for a raise\nAsking for a promotion\nAsking for a date\n\n\n\nThings that need improving\n\nSometimes Chatgpt voices cuts the converstation off half way throgh me saying something if i pause while im thinking about how to phrase something. I wish it would wait a bit longer before cutting off the conversation.\nSometimes I want to cut off what the chatgpt voice is saying because it/I haven’t fully explained the context. I wish it would stop speaking and go into listening mode or be able to do both.\nI wish it would give me a summary of the conversation at the end, it would be useful to have a summary of the conversation to help me remember what we talked about.\n\n\n\nConclusion\nI will be using this more and more, I think its a game changer for me, I can see myself using it. I’m really excited about the future of AI assistants and how they will help us learn and understand the world together. It will be interesting to know how bigger or better models will improve the experience. I’m sure there will be a lot of research in this area.\nTry ChatGPT, I think it will change the way you think about AI assistants and how you can use them to learn and understand together.\n\nReferences\n(“How to Use ChatGPT to Easily Learn Any Skill You Want” n.d.)\n\n\n\n\n\n\nReferences\n\n“How to Use ChatGPT to Easily Learn Any Skill You Want.” n.d. Accessed October 26, 2023. https://www.youtube.com/watch?v=MnDudvCyWpc&t=365s."
  },
  {
    "objectID": "posts/adam_optimizer/Adam_optimizer.html",
    "href": "posts/adam_optimizer/Adam_optimizer.html",
    "title": "setup enviroment",
    "section": "",
    "text": "---\ntitle: \"Your Title Here\"\ndraft: true\n---\n!pip install datasets\n\nCollecting datasets\n  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 7.6 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\nCollecting dill&lt;0.3.8,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 12.5 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 19.9 MB/s eta 0:00:00\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 15.1 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;2023.9.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\nCollecting huggingface-hub&lt;1.0.0,&gt;=0.14.0 (from datasets)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 26.8 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer&lt;4.0,&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (3.2.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.14.0-&gt;datasets) (3.12.2)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.14.0-&gt;datasets) (4.5.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;datasets) (1.16.0)\nInstalling collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\nSuccessfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 xxhash-3.3.0\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n!pip install torcheval\n\nCollecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 2.5 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\nInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\nimport sys\nsys.path.append('/content/drive/MyDrive/Learning/data_science/')"
  },
  {
    "objectID": "posts/adam_optimizer/Adam_optimizer.html#optimizers",
    "href": "posts/adam_optimizer/Adam_optimizer.html#optimizers",
    "title": "setup enviroment",
    "section": "Optimizers",
    "text": "Optimizers\nfinds the optimum learning rate. the learning rate is used to step in the direction that gradient decent points too far and you could over shoot the optimum setting too little and training takes for ever.\n\nSGD\n\n# SGD estimates the gradient using just a single data point or a small batch of data points at each iteration.\n# This makes SGD faster and more computationally efficient\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        params = list(params)\n        fc.store_attr()\n        self.i = 0\n\n    # Called each training iteration to update the parametersg\n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n\n    def opt_step(self, p): p -= p.grad * self.lr\n    def reg_step(self, p):\n        if self.wd != 0: p *= 1 - self.lr*self.wd\n\n    # Zeros out the gradient for all parameters.\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\n# adam is a comparison between Adam and\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'avg'): p.avg = torch.zeros_like(p.grad.data)\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = torch.zeros_like(p.grad.data)\n        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.796\n0.574\n0\ntrain\n\n\n0.843\n0.433\n0\neval\n\n\n0.868\n0.363\n1\ntrain\n\n\n0.867\n0.373\n1\neval\n\n\n0.884\n0.318\n2\ntrain\n\n\n0.875\n0.349\n2\neval\n\n\n\n\n\n\n\n\nAdam with Automatic Annealer Automatic annealing refers to the automatic adjustment of the learning rate during training. Unlike a constant or step-wise decayed learning rate, automatic annealers adjust the learning rate based on the model’s performance, generally on the validation set. Criteria such as validation loss, training loss, or some other metric can be used for this adjustment.  For instance, if the validation loss plateaus or increases, the annealer may reduce the learning rate, effectively “cooling down” the optimization and allowing the model to escape local minima or saddle points. Conversely, if the model is learning effectively, the annealer might maintain or even slightly increase the learning rate."
  }
]