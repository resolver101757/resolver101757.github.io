[
  {
    "objectID": "useful_info.html",
    "href": "useful_info.html",
    "title": "commands to run quarto",
    "section": "",
    "text": "commands to run quarto\nto create a project (run once at the beginning to create the project files) - quarto create project_name –template=blog\nto run a preview locally - quarto preview .\nto Create update the files (run before pushing to github) - quarto render .\n\n\nlinks :\n\nVideo of somone showing how to use quarto https://www.youtube.com/watch?v=nllKcuX7rEc\nilovetensoor coppied a good link here : https://discord.com/channels/689892369998676007/1096981866248147014/1167453441291984936\n\n\n\nquarto reference page :\nhttps://quarto.org/docs/reference/cells/cells-jupyter.html\n\n\nsetup for using colab with quarto\n\nCreate a new notebook in colab and add markup in markup - no need for RAW as documentation says.\ninstall the drive app in windows which will reveal the drive folder in the file explorer, i used streams so no files are stored locally.\nCreate a script to copy file to local installation and run “quarto render .” to create the html file. see file located C:projects_lsuv_from_colab.bat\nEdit the notebook in colab and run the script to render the html file, repeat until happy with the result and publish to github."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html",
    "href": "posts/Exploring_GTP4V_paper/index.html",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Hi, this is a explority view of GPT4 vision, study hacks and it’s uses in manufacturing and industry 4.0. Its a commentary of microsoft paper TheDawnofLMMs: PreliminaryExplorationswithGPT-4V(ision) picking out the highlights, my take on how GPT4 vision can be used in daily life, how to maximise my studying of the fast.ai course and what GPT4 vision means for the future of manufacturing and industry 4.0.\nThe paper and chatgpt vision was discussed with the Fast.AI study group. The study group is a multi-national group of people who are passionate about deep learning and AI. We meet online on Saturdays to run through the course work provided by fastAI, discuss papers and latest trends and get to know each other.\n\n\n\nGPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA.\n\n\n\n\n\nSomeone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog.\n\n\n\n\n\nIn the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more.\n\n\n\n\nDuring the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing.\n\n\n\n\nI beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks\n\n\n\n\n\nDefect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#what-is-gpt4-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "GPT4 vision is a new model from openAI. GPT4 is a LMM (Large Multi-Modal Model). Multimodal technology refers to systems that can process and integrate multiple types of inputs and outputs, in gpt4 vision case its input can be text and images and its output is text only (as of this writing, i imagine more will follow).\nGPT-4 uses a transformer-style architecture in its neural network. A transformer architecture allows for a better understanding of relationships. It also uses an attention mechanism that allows the neural network to parse out which pieces of data are more relevant than others.\nAs of writing GPT4 vision is only available to chatgpt premium users and no general api availability. a open source competitor only is LLaVA."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "href": "posts/Exploring_GTP4V_paper/index.html#fastai-study-hacks-with-chatgpt-vision",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Someone posted a training loss chart in the fast.ai study group and didnt know what was going on. Why not ask your AI study buddy (chatgpt vision) for help instead or along side of posting in fast ai. Whether your a seasoned deep learning pro or a beginner, it might be something simple (or difficult) that chatgpt vision could give you a few ideas for better training or get you passed this road block so you can continue your study…\nI wonder how many people have stopped fast.ai (or any course) due to a road block that could have been easily solved with chatgpt vision.\n\n\n\n\n(see paper Sec. 4.6 Coding Capability with Vision)\nadd a image of a formula you’ve seen on a paper and get the results back in latex or even python code. This could be used to help you understand the paper better or even help you write your own paper / model.\n\n\n\n\n(see paper Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning)\nadd a image of a paper and ask the model to summarise it for you. Vision will have the benefit over llms (large language models) due to its ability to understand images, charts along with the text.\n\nadd a image of a floor plan or cad drawing a get a detailed description of what the floor plan will produce.\n\n\n\n\n(See paper Sec. 4.6 Coding Capability with Vision)\nlike a chart on a paper, ask chatgpt to reproduce it with in the format required, e.g. python code and and it will be returned and ready to be ran in your jupyter notebook.\n\n\n\n\nTake a screen shot of your website or blog and ask chatgpt vision to improve it. It will offer suggestions. This could be used to improve your website or blog, or even give you ideas for a new website or blog. It could also be used to generate new content for your website or blog. This could be used to improve your SEO (search engine optimisation) and increase your traffic to your website or blog."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "href": "posts/Exploring_GTP4V_paper/index.html#shot-learning",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "In the study group, before studying this particular paper we looked at prompting for LLM’s (large language models) and one short learning were giving good results. It seams Chatgpt vision, gives better results with 2 shots or more."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "href": "posts/Exploring_GTP4V_paper/index.html#spatial-relationship-understanding",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "During the study group, we we’re amazed by what chatgpt vision understood in this picture. It described the dog as jumping up, and the man has thrown the frispy. Theres quite a lot to unpack here. Does it understand real world physics to work out whats going on in the photo, or is it just using the a history of similar photos with stored text. I think its a bit of both, probably more the former, whatever it is, it’s still amazing."
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "href": "posts/Exploring_GTP4V_paper/index.html#rpa-robotic-process-automation",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "I beleive the uses of this model will be transformational in the RPA space. RPA is a technology that allows anyone today to configure computer software, or a “robot” to emulate and integrate the actions of a human interacting within digital systems to execute a business process. RPA robots utilize the user interface to capture data and manipulate applications just like humans do. They interpret, trigger responses and communicate with other systems in order to perform on a vast variety of repetitive tasks. Only substantially better: an RPA software robot never sleeps and makes zero mistakes.\nIn manufacturing alone, there are 100’s of use cases to automate processes and tasks. The problem is that RPA is very brittle and requires a lot of manual work to configure and maintain. GPT4 vision will allow for a more natural way to interact with the RPA system. For example, if you want to automate a process that requires you to look at a screen and click on a button, you can now just take a picture of the screen and ask the RPA system to click on the button and if it has knowledge of the system in its weights, it will also know the subquent steps speeding up the process. This will allow for a more natural way to interact with the RPA system and will make it easier to automate processes. The system could provide a feedback loop by continuous monitoring the screens and taking the appropriate action or highlight any issues ready for a oporator to take action.\nHere of Generative AI from genta using text only model. Imagine what could be done with gpt4 vision.\nIn the example below I have given chatgpt a screen and tabs it has never seen and returned something that could easily be used for, the full prompt is “this is a SAP customer screen, pretend your operating it like a human and update all the details in customer with fictitious details .e.g name, address ect. describe each screen click and key presses and provide a json in a format that uipath would accept\n”\nPersonally I’ve never seen SAP but I know a lot of manufacturers use this system and there exist lots of screen shots online so its likely chatgpt could help automate a pipeline to update customer details in SAP and much more. I’ve also seen a lot of RPA systems that use json to describe the steps to take so this could be used to automate the process.ks"
  },
  {
    "objectID": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "href": "posts/Exploring_GTP4V_paper/index.html#i-will-be-performing-future-research-using-real-life-company-data.",
    "title": "Exploring GPT4 vision, fastai study hacks and what it means for industry 4.0",
    "section": "",
    "text": "Defect detection : have parts been assembled correctly, any missing parts, parts with defects, etc.\nSafety inspection : are all safety features in place, are all safety features working, are people wearing safety equipment (e.g. helmets, gloves, etc.)\nComponent identification : This will be useful to check if the operator has packed all components before shipping to customer, or if the operator has assembled all components before shipping to customer.\nSpot the difference : take a few one shot or 2 shot examples of our products and compare to a fresh product of the assembly line and see how they match."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI: The Catalyst of the 4th Industrial transformation",
    "section": "",
    "text": "Stepping into the heart of the 4th industrial revolution, this blog series examines the future of manufacturing, with AI and data analytics positioned at the core of this momentous transformation.\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe importantance of propper initializtaion\n\n\n\n\n\n\n\n\n\n\n\n\nAlex Kelly\n\n\n\n\n\n\n  \n\n\n\n\nMaximizing Your Commute: Learning on the Go with ChatGPT Voice\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nchatbot\n\n\ngpt\n\n\nnlp\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\n  \n\n\n\n\nExploring GPT4 vision, fastai study hacks and what it means for industry 4.0\n\n\n\n\n\n\n\nai\n\n\nmanufacturing\n\n\nfastai\n\n\ngpt4\n\n\nchatgpt\n\n\nrpa\n\n\nautomation\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAlex Paul Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Alex Kelly Seasoned Technologist & Leader | Empowering Future Innovations in IT | Advocate of AI & IoT | Transforming Business Processes in Manufacturing Sector\nIn this space, I aim to unravel the intricacies of the 4th Industrial Revolution, shedding light on how AI, IoT, Robotics, and Deep Learning are converging to redefine the manufacturing landscape. Through a series of insightful blogs, I will share my explorations and findings on the synergies between these cutting-edge technologies and their transformative impact on industrial operations and processes. This journey delves into not just the technological advancements steering this revolution, but also the potential challenges and solutions that come along, offering a holistic view on the unfolding industrial epoch.\nJoin me as we navigate through the myriad facets of this transformative era, seeking to understand and harness the immense potential that lies at the intersection of AI, data analytics, and modern industrial practices."
  },
  {
    "objectID": "posts/chatGPT_audio_conversation/index.html",
    "href": "posts/chatGPT_audio_conversation/index.html",
    "title": "Maximizing Your Commute: Learning on the Go with ChatGPT Voice",
    "section": "",
    "text": "Overview\nI have 20 minute drive into work and back home everyday where I usually listen to a podcast, audio book or sometimes just stair at the road. I’m always thinking, how can I make better use of my time…. I saw a twitter post from Le Cunn, the legendary Data Scientist from Meta and that i fearously agree with ““books are a user interface to knowledge.”.\nThat’s what AI assistants are poised to become: “AI assistants will be a better user interface to knowledge.”\nThis is a short blog on how im using chatgpt to study and making use of spare time on journes to work.\n\n\n\nWhat is ChatGPT voice\nChatgpt voice is based on GPT 3.5 model and includes features giving that give it the ability to hear and speak.\nFor the voice feature, OpenAI uses Whisper, its speech recognition system, to transcribe a user’s spoken words into text and a new text-to-speech model that can generate human-like audio from text with just a few seconds of speech.\nI have a selection of 5 voices to choose from and have been using “sky” and it sounds really natural. You can still tell its a computer speech which I imagine is on purpose, it gets the pauses and tone, emotion right, im really impressed.\nIt available on the plus and enterprise edition, not the free edition of ChatGPT.\n\n\nWhats wrong with podcast and audiobooks\nI’ve been using ChatGPT voice conversations for over a month now (it’s still in beta) and for me its a game changer for studying and making the spare time useful on the journey to work and back.\nListening to podcasts or audiobooks can be enriching, yet they come with limitations. Sometimes they delve into topics that don’t interest you. At other times, they might present information in a convoluted manner. There’s also the issue of pacing: some podcasts assume advanced knowledge, leaving you lost, while others may cover familiar ground, leading to frustration\n\n\nwhy use ChatGPT voice\nChatGPT Voice offers a versatile learning experience by tapping into a wealth of internet knowledge. It can adapt its conversational style to mimic various tones, such as Shakespearean language or the distinctive styles of well-known educators like Richard Feynman and Jeremy Howard. While I haven’t personally tried these specific modes, they could resonate better with your learning preferences\n\n\nHow to use ChatGPT Voice\nDo you have a favourite educator? Ask GPT voice to give explanations like that educator, don’t understand something, ask it to explain it in a different way. Ask it to explain it to a 5 year old, a 10 year old, say what you understand and what you dont understand and for me, its filled in the gaps.\nAsk it to give you a topic and ask for a questions at the end to make sure you understand.\nWhat are the most important facts, dates, or formulas related to (topic)? Help me create a memorization technique to remember them easily.\nGive it a statement and ask chatgpt to give feedback, any corrections or improvements.\nAsk voice GPT to create models or analogies to help me understand and remember “optimization techinques in deep learning”.\nDo you have a difficult concept to understand, ask Chatgpt to Guide you through a visualization exercise to help me internalize the term optimization techinques and imagine yourself successfully applying it to a real-life situation. This has really helped me on difficult concepts, highly recommend you try it.\nMy favourite is to ask chatgpt voice “I want you to act as a Socrat and use the Socratic method to help me improve my critical thinking, logic, and reasoning skills. Your task is to ask open-ended questions to the statement ‘optimization techinques in deep learning’, give me constructive feedback to each response before you ask the next question.””\nAll conversations are saved in text format when your return to your computer and phone, you can review the conversation and save it to your notes or share it.\n\n\nOther Uses\nDo you have a important conversation coming up, talk it through with ChatGPT first to get your thoughts in order and ask for opionions on how other positions, react and respond and how to frame the conversation. You can have this conversation as many times as you want, it will never get bored, it will never get angry and its always available 24 hours a day 7 days a week, 365 days a year.\nPotential use cases:\n\nNegotiations\nInterviews\nSales\nPresentations\nConflict resolution\nDifficult conversations\nAsking for a raise\nAsking for a promotion\nAsking for a date\n\n\n\nThings that need improving\n\nSometimes Chatgpt voices cuts the converstation off half way throgh me saying something if i pause while im thinking about how to phrase something. I wish it would wait a bit longer before cutting off the conversation.\nSometimes I want to cut off what the chatgpt voice is saying because it/I haven’t fully explained the context. I wish it would stop speaking and go into listening mode or be able to do both.\nI wish it would give me a summary of the conversation at the end, it would be useful to have a summary of the conversation to help me remember what we talked about.\n\n\n\nConclusion\nI will be using this more and more, I think its a game changer for me, I can see myself using it. I’m really excited about the future of AI assistants and how they will help us learn and understand the world together. It will be interesting to know how bigger or better models will improve the experience. I’m sure there will be a lot of research in this area.\nTry ChatGPT, I think it will change the way you think about AI assistants and how you can use them to learn and understand together.\n\nReferences\n(“How to Use ChatGPT to Easily Learn Any Skill You Want” n.d.)\n\n\n\n\n\n\nReferences\n\n“How to Use ChatGPT to Easily Learn Any Skill You Want.” n.d. Accessed October 26, 2023. https://www.youtube.com/watch?v=MnDudvCyWpc&t=365s."
  },
  {
    "objectID": "posts/initialization_neural_networks/index.html",
    "href": "posts/initialization_neural_networks/index.html",
    "title": "The importantance of propper initializtaion",
    "section": "",
    "text": "Why am I writing about LSUV?\nI’m watching online videos series by Fast.ai where we are looking at re-creating neural networks from scratch using Python ensuring we fully understand innerworkings of each aspect of a nerual network. This will enable me to create new techniques and improve existing techniques and enable me to piece together the right neural network for the right task.\nOn top of that we meet up online for a study group and share/discuss what we have been studying. This blog post is why we need a better way to initialize weights than just choosing random values.\n\n\nWhy initialization model weights before starting the optimization\n\nProper initialization can affect how quickly the model converges to a minimum loss, or even whether it converges at all. Its not just about the initialization of the 1st layer of weights, its about all the weights from layer 1 to the last to the outputs.\nHere are a few key points on weight initialations :\n\nThe hardware has floating point limitations that mean it processes limited number of bits and stores in a limited amount of memory. If the weights are too high or too low then it ends up calculating the results with too high or too low to store into memory specified which are called exploding or vanishing neurons (i.e.. dead neurons) at anypoint level in the nereual network. This results in information lost, which are called dead neurons or neurons that dont contribute to the end result in a optimal way.\n\nThe mean should be close to zero, if the number is far away from zero, you will more likely end up with exploding or vanishing neurons (i.e.. dead neurons) that don’t contribute to the end prediction or classification. Enforcing a mean of zero is a way to optimize the weights so when calculated against the inputs they give a optimal result in the floating point range that the hardware can handle.\nThe standard deviation should be near 1 so that the values don’t vary too far from the mean (i.e. 0 mentioned in point 2). A standard deviation that’s too high or too low could lead to weights that are too disparate or too similar, affecting the learning dynamics.\n\n\n\nLSUV vs other methods\nEach model comes with its own issues and choosing the right initialization model is key to success. Some initializations work better with large models, some with small and some depend on the activation functions, sometimes you have to experiment to see which ones work best. Here are a few examples of initialization techniques :\n\nLSUV (Layer-Sequential Unit-Variance) Initialization\nZero Initialization\nRandom Initialization\nXavier/Glorot Initialization\nHe Initialization\nLeCun Initialization\n\nLSUV is a valuable weight initialization technique, especially for deeper architectures where traditional techniques might not be as effective. However, the choice of weight initialization should be based on the network architecture, activation function, and specific challenges of the problem at hand.\n\n\nCode\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close"
  }
]